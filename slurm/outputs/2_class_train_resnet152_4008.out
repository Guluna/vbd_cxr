cuda
Params to learn, when feature extract = True:
	 fc.weight
	 fc.bias
Program started
Epoch 0/29
----------
Train Iteration #0:: 1.5665013790130615 Acc: 0.375
Train Iteration #50:: 5.2699666023254395 Acc: 0.25
Train Iteration #100:: 4.255545616149902 Acc: 0.25
Train Iteration #150:: 0.8090698719024658 Acc: 0.5
Train Iteration #200:: 1.9161111116409302 Acc: 0.5
Train Iteration #250:: 1.5861538648605347 Acc: 0.5
Train Iteration #300:: 1.4860962629318237 Acc: 0.625
Train Iteration #350:: 0.6470253467559814 Acc: 0.75
Train Iteration #400:: 1.5982155799865723 Acc: 0.5
Train Iteration #450:: 1.7941077947616577 Acc: 0.75
Train Iteration #500:: 1.0107977390289307 Acc: 0.75
Train Iteration #550:: 1.0288335084915161 Acc: 0.5
Train Iteration #600:: 0.9367839097976685 Acc: 0.625
Train Iteration #650:: 0.7474710941314697 Acc: 0.75
Train Iteration #700:: 0.7512609958648682 Acc: 0.625
Train Iteration #750:: 0.10406984388828278 Acc: 1.0
Train Iteration #800:: 2.15762996673584 Acc: 0.25
Train Iteration #850:: 0.5653089284896851 Acc: 0.75
Train Iteration #900:: 0.15961408615112305 Acc: 1.0
Train Iteration #950:: 0.21447855234146118 Acc: 1.0
Train Iteration #1000:: 1.0711627006530762 Acc: 0.625
Train Iteration #1050:: 2.3922290802001953 Acc: 0.625
Train Iteration #1100:: 2.4366111755371094 Acc: 0.75
Train Iteration #1150:: 0.762439489364624 Acc: 0.75
Train Iteration #1200:: 2.3342301845550537 Acc: 0.25
Train Iteration #1250:: 0.37229254841804504 Acc: 0.875
Train Iteration #1300:: 0.5978233218193054 Acc: 0.875
Train Iteration #1350:: 2.4140477180480957 Acc: 0.5
Train Iteration #1400:: 0.6257953643798828 Acc: 0.75
Train Iteration #1450:: 0.8796312808990479 Acc: 0.75
Train Iteration #1500:: 0.5591368675231934 Acc: 0.75
Epoch Training:: Loss: 1.2134 Acc: 0.6681
Validation Iteration #0:: 1.4319332838058472 Acc: 0.375
Validation Iteration #50:: 1.1178488731384277 Acc: 0.625
Validation Iteration #100:: 0.4346156120300293 Acc: 0.75
Validation Iteration #150:: 1.4203414916992188 Acc: 0.375
Epoch Validation:: Loss: 1.1450 Acc: 0.5938
Epoch 1/29
----------
Train Iteration #1550:: 0.4329947531223297 Acc: 0.875
Train Iteration #1600:: 0.045609962195158005 Acc: 1.0
Train Iteration #1650:: 0.13164150714874268 Acc: 1.0
Train Iteration #1700:: 1.36696457862854 Acc: 0.625
Train Iteration #1750:: 0.8392975926399231 Acc: 0.625
Train Iteration #1800:: 1.2810893058776855 Acc: 0.875
Train Iteration #1850:: 1.1723947525024414 Acc: 0.625
Train Iteration #1900:: 0.21236354112625122 Acc: 0.875
Train Iteration #1950:: 3.282968521118164 Acc: 0.25
Train Iteration #2000:: 0.6203720569610596 Acc: 0.875
Train Iteration #2050:: 0.766260027885437 Acc: 0.75
Train Iteration #2100:: 0.6269369125366211 Acc: 0.875
Train Iteration #2150:: 0.31735116243362427 Acc: 1.0
Train Iteration #2200:: 0.15066125988960266 Acc: 0.875
Train Iteration #2250:: 2.0204715728759766 Acc: 0.625
Train Iteration #2300:: 0.4456809461116791 Acc: 0.875
Train Iteration #2350:: 2.4252986907958984 Acc: 0.625
Train Iteration #2400:: 0.37119537591934204 Acc: 0.875
Train Iteration #2450:: 0.1890692114830017 Acc: 1.0
Train Iteration #2500:: 1.1021728515625 Acc: 0.625
Train Iteration #2550:: 0.29663753509521484 Acc: 0.875
Train Iteration #2600:: 1.3795537948608398 Acc: 0.625
Train Iteration #2650:: 0.5375743508338928 Acc: 0.875
Train Iteration #2700:: 1.2366001605987549 Acc: 0.625
Train Iteration #2750:: 0.4937376081943512 Acc: 0.875
Train Iteration #2800:: 1.5723247528076172 Acc: 0.625
Train Iteration #2850:: 0.21713685989379883 Acc: 0.875
Train Iteration #2900:: 1.5759961605072021 Acc: 0.375
Train Iteration #2950:: 1.7881710529327393 Acc: 0.75
Train Iteration #3000:: 0.015455584973096848 Acc: 1.0
Epoch Training:: Loss: 1.2225 Acc: 0.7200
Validation Iteration #200:: 1.2809903621673584 Acc: 0.75
Validation Iteration #250:: 0.12250470370054245 Acc: 1.0
Validation Iteration #300:: 0.7712216377258301 Acc: 0.75
Epoch Validation:: Loss: 0.6769 Acc: 0.8777
Epoch 2/29
----------
Train Iteration #3050:: 0.342743456363678 Acc: 1.0
Train Iteration #3100:: 0.3768889307975769 Acc: 0.875
Train Iteration #3150:: 0.8986436128616333 Acc: 0.625
Train Iteration #3200:: 2.604069471359253 Acc: 0.625
Train Iteration #3250:: 1.0607500076293945 Acc: 0.625
Train Iteration #3300:: 0.26828089356422424 Acc: 0.875
Train Iteration #3350:: 3.2307851314544678 Acc: 0.625
Train Iteration #3400:: 1.2436416149139404 Acc: 0.625
Train Iteration #3450:: 0.2635861933231354 Acc: 0.75
Train Iteration #3500:: 0.1729820966720581 Acc: 1.0
Train Iteration #3550:: 0.10964513570070267 Acc: 1.0
Train Iteration #3600:: 0.2731066644191742 Acc: 1.0
Train Iteration #3650:: 0.09668926894664764 Acc: 1.0
Train Iteration #3700:: 0.4956507980823517 Acc: 0.875
Train Iteration #3750:: 3.732252597808838 Acc: 0.25
Train Iteration #3800:: 0.6454330682754517 Acc: 0.75
Train Iteration #3850:: 0.9919846653938293 Acc: 0.75
Train Iteration #3900:: 1.7016569375991821 Acc: 0.625
Train Iteration #3950:: 0.2603633403778076 Acc: 0.875
Train Iteration #4000:: 0.03854574263095856 Acc: 1.0
Train Iteration #4050:: 0.04410606622695923 Acc: 1.0
Train Iteration #4100:: 1.5257282257080078 Acc: 0.625
Train Iteration #4150:: 0.5419793128967285 Acc: 0.875
Train Iteration #4200:: 1.9535164833068848 Acc: 0.5
Train Iteration #4250:: 0.26680558919906616 Acc: 0.875
Train Iteration #4300:: 2.955536365509033 Acc: 0.375
Train Iteration #4350:: 0.2549263536930084 Acc: 0.75
Train Iteration #4400:: 2.3870604038238525 Acc: 0.5
Train Iteration #4450:: 1.1038236618041992 Acc: 0.625
Train Iteration #4500:: 0.16037099063396454 Acc: 0.875
Train Iteration #4550:: 2.2612733840942383 Acc: 0.625
Epoch Training:: Loss: 1.1535 Acc: 0.7382
Validation Iteration #350:: 0.3923560082912445 Acc: 0.75
Validation Iteration #400:: 1.525289535522461 Acc: 0.75
Validation Iteration #450:: 0.45733967423439026 Acc: 0.75
Validation Iteration #500:: 2.1412506103515625 Acc: 0.5
Epoch Validation:: Loss: 1.7759 Acc: 0.5649
Epoch 3/29
----------
Train Iteration #4600:: 0.6548134088516235 Acc: 0.625
Train Iteration #4650:: 0.206415057182312 Acc: 0.875
Train Iteration #4700:: 0.09821894019842148 Acc: 1.0
Train Iteration #4750:: 0.486097127199173 Acc: 0.875
Train Iteration #4800:: 1.6686933040618896 Acc: 0.75
Train Iteration #4850:: 1.351593017578125 Acc: 0.75
Train Iteration #4900:: 0.6515190601348877 Acc: 0.75
Train Iteration #4950:: 0.1767577975988388 Acc: 1.0
Train Iteration #5000:: 0.8941675424575806 Acc: 0.75
Train Iteration #5050:: 0.6648979187011719 Acc: 0.875
Train Iteration #5100:: 0.776627779006958 Acc: 0.625
Train Iteration #5150:: 0.7373985052108765 Acc: 0.625
Train Iteration #5200:: 0.8119231462478638 Acc: 0.75
Train Iteration #5250:: 0.8608238101005554 Acc: 0.75
Train Iteration #5300:: 0.14460483193397522 Acc: 1.0
Train Iteration #5350:: 0.4858589172363281 Acc: 0.875
Train Iteration #5400:: 0.6862119436264038 Acc: 0.75
Train Iteration #5450:: 3.3264145851135254 Acc: 0.25
Train Iteration #5500:: 0.48365721106529236 Acc: 0.875
Train Iteration #5550:: 0.10014104843139648 Acc: 1.0
Train Iteration #5600:: 0.254144549369812 Acc: 0.875
Train Iteration #5650:: 0.03592750430107117 Acc: 1.0
Train Iteration #5700:: 0.16910940408706665 Acc: 0.875
Train Iteration #5750:: 0.06194893643260002 Acc: 1.0
Train Iteration #5800:: 1.451087236404419 Acc: 0.75
Train Iteration #5850:: 0.9171560406684875 Acc: 0.625
Train Iteration #5900:: 0.6505112648010254 Acc: 0.875
Train Iteration #5950:: 0.9568734169006348 Acc: 0.75
Train Iteration #6000:: 0.5262961387634277 Acc: 0.875
Train Iteration #6050:: 1.8689937591552734 Acc: 0.625
Epoch Training:: Loss: 1.0801 Acc: 0.7414
Validation Iteration #550:: 1.1090896129608154 Acc: 0.625
Validation Iteration #600:: 2.5247514247894287 Acc: 0.5
Validation Iteration #650:: 2.5114567279815674 Acc: 0.5
Epoch Validation:: Loss: 1.4569 Acc: 0.5997
Epoch 4/29
----------
Train Iteration #6100:: 1.4041428565979004 Acc: 0.625
Train Iteration #6150:: 0.2865535020828247 Acc: 1.0
Train Iteration #6200:: 0.23305462300777435 Acc: 1.0
Train Iteration #6250:: 1.028317928314209 Acc: 0.75
Train Iteration #6300:: 0.7230862975120544 Acc: 0.75
Train Iteration #6350:: 1.4420092105865479 Acc: 0.625
Train Iteration #6400:: 0.3891667127609253 Acc: 0.875
Train Iteration #6450:: 0.4084894061088562 Acc: 0.875
Train Iteration #6500:: 0.15059109032154083 Acc: 1.0
Train Iteration #6550:: 0.34113478660583496 Acc: 0.875
Train Iteration #6600:: 0.6528569459915161 Acc: 0.625
Train Iteration #6650:: 0.5817136168479919 Acc: 0.875
Train Iteration #6700:: 0.555618166923523 Acc: 0.875
Train Iteration #6750:: 1.2969226837158203 Acc: 0.625
Train Iteration #6800:: 1.5569509267807007 Acc: 0.625
Train Iteration #6850:: 0.2201201170682907 Acc: 1.0
Train Iteration #6900:: 1.7948012351989746 Acc: 0.5
Train Iteration #6950:: 0.7435598969459534 Acc: 0.875
Train Iteration #7000:: 2.516561985015869 Acc: 0.75
Train Iteration #7050:: 1.2257009744644165 Acc: 0.5
Train Iteration #7100:: 0.24486520886421204 Acc: 0.875
Train Iteration #7150:: 0.042286958545446396 Acc: 1.0
Train Iteration #7200:: 2.1420159339904785 Acc: 0.625
Train Iteration #7250:: 0.42474281787872314 Acc: 0.875
Train Iteration #7300:: 0.8813930153846741 Acc: 0.625
Train Iteration #7350:: 0.41164159774780273 Acc: 0.875
Train Iteration #7400:: 0.03791917860507965 Acc: 1.0
Train Iteration #7450:: 2.258168935775757 Acc: 0.375
Train Iteration #7500:: 0.11920695006847382 Acc: 1.0
Train Iteration #7550:: 3.0027711391448975 Acc: 0.5
Epoch Training:: Loss: 1.0265 Acc: 0.7591
Validation Iteration #700:: 0.7423975467681885 Acc: 0.75
Validation Iteration #750:: 0.26362961530685425 Acc: 0.875
Validation Iteration #800:: 1.2283248901367188 Acc: 0.625
Epoch Validation:: Loss: 0.8540 Acc: 0.7576
Epoch 5/29
----------
Params to learn, when feature extract = False:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer2.3.conv1.weight
	 layer2.3.bn1.weight
	 layer2.3.bn1.bias
	 layer2.3.conv2.weight
	 layer2.3.bn2.weight
	 layer2.3.bn2.bias
	 layer2.3.conv3.weight
	 layer2.3.bn3.weight
	 layer2.3.bn3.bias
	 layer2.4.conv1.weight
	 layer2.4.bn1.weight
	 layer2.4.bn1.bias
	 layer2.4.conv2.weight
	 layer2.4.bn2.weight
	 layer2.4.bn2.bias
	 layer2.4.conv3.weight
	 layer2.4.bn3.weight
	 layer2.4.bn3.bias
	 layer2.5.conv1.weight
	 layer2.5.bn1.weight
	 layer2.5.bn1.bias
	 layer2.5.conv2.weight
	 layer2.5.bn2.weight
	 layer2.5.bn2.bias
	 layer2.5.conv3.weight
	 layer2.5.bn3.weight
	 layer2.5.bn3.bias
	 layer2.6.conv1.weight
	 layer2.6.bn1.weight
	 layer2.6.bn1.bias
	 layer2.6.conv2.weight
	 layer2.6.bn2.weight
	 layer2.6.bn2.bias
	 layer2.6.conv3.weight
	 layer2.6.bn3.weight
	 layer2.6.bn3.bias
	 layer2.7.conv1.weight
	 layer2.7.bn1.weight
	 layer2.7.bn1.bias
	 layer2.7.conv2.weight
	 layer2.7.bn2.weight
	 layer2.7.bn2.bias
	 layer2.7.conv3.weight
	 layer2.7.bn3.weight
	 layer2.7.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer3.3.conv1.weight
	 layer3.3.bn1.weight
	 layer3.3.bn1.bias
	 layer3.3.conv2.weight
	 layer3.3.bn2.weight
	 layer3.3.bn2.bias
	 layer3.3.conv3.weight
	 layer3.3.bn3.weight
	 layer3.3.bn3.bias
	 layer3.4.conv1.weight
	 layer3.4.bn1.weight
	 layer3.4.bn1.bias
	 layer3.4.conv2.weight
	 layer3.4.bn2.weight
	 layer3.4.bn2.bias
	 layer3.4.conv3.weight
	 layer3.4.bn3.weight
	 layer3.4.bn3.bias
	 layer3.5.conv1.weight
	 layer3.5.bn1.weight
	 layer3.5.bn1.bias
	 layer3.5.conv2.weight
	 layer3.5.bn2.weight
	 layer3.5.bn2.bias
	 layer3.5.conv3.weight
	 layer3.5.bn3.weight
	 layer3.5.bn3.bias
	 layer3.6.conv1.weight
	 layer3.6.bn1.weight
	 layer3.6.bn1.bias
	 layer3.6.conv2.weight
	 layer3.6.bn2.weight
	 layer3.6.bn2.bias
	 layer3.6.conv3.weight
	 layer3.6.bn3.weight
	 layer3.6.bn3.bias
	 layer3.7.conv1.weight
	 layer3.7.bn1.weight
	 layer3.7.bn1.bias
	 layer3.7.conv2.weight
	 layer3.7.bn2.weight
	 layer3.7.bn2.bias
	 layer3.7.conv3.weight
	 layer3.7.bn3.weight
	 layer3.7.bn3.bias
	 layer3.8.conv1.weight
	 layer3.8.bn1.weight
	 layer3.8.bn1.bias
	 layer3.8.conv2.weight
	 layer3.8.bn2.weight
	 layer3.8.bn2.bias
	 layer3.8.conv3.weight
	 layer3.8.bn3.weight
	 layer3.8.bn3.bias
	 layer3.9.conv1.weight
	 layer3.9.bn1.weight
	 layer3.9.bn1.bias
	 layer3.9.conv2.weight
	 layer3.9.bn2.weight
	 layer3.9.bn2.bias
	 layer3.9.conv3.weight
	 layer3.9.bn3.weight
	 layer3.9.bn3.bias
	 layer3.10.conv1.weight
	 layer3.10.bn1.weight
	 layer3.10.bn1.bias
	 layer3.10.conv2.weight
	 layer3.10.bn2.weight
	 layer3.10.bn2.bias
	 layer3.10.conv3.weight
	 layer3.10.bn3.weight
	 layer3.10.bn3.bias
	 layer3.11.conv1.weight
	 layer3.11.bn1.weight
	 layer3.11.bn1.bias
	 layer3.11.conv2.weight
	 layer3.11.bn2.weight
	 layer3.11.bn2.bias
	 layer3.11.conv3.weight
	 layer3.11.bn3.weight
	 layer3.11.bn3.bias
	 layer3.12.conv1.weight
	 layer3.12.bn1.weight
	 layer3.12.bn1.bias
	 layer3.12.conv2.weight
	 layer3.12.bn2.weight
	 layer3.12.bn2.bias
	 layer3.12.conv3.weight
	 layer3.12.bn3.weight
	 layer3.12.bn3.bias
	 layer3.13.conv1.weight
	 layer3.13.bn1.weight
	 layer3.13.bn1.bias
	 layer3.13.conv2.weight
	 layer3.13.bn2.weight
	 layer3.13.bn2.bias
	 layer3.13.conv3.weight
	 layer3.13.bn3.weight
	 layer3.13.bn3.bias
	 layer3.14.conv1.weight
	 layer3.14.bn1.weight
	 layer3.14.bn1.bias
	 layer3.14.conv2.weight
	 layer3.14.bn2.weight
	 layer3.14.bn2.bias
	 layer3.14.conv3.weight
	 layer3.14.bn3.weight
	 layer3.14.bn3.bias
	 layer3.15.conv1.weight
	 layer3.15.bn1.weight
	 layer3.15.bn1.bias
	 layer3.15.conv2.weight
	 layer3.15.bn2.weight
	 layer3.15.bn2.bias
	 layer3.15.conv3.weight
	 layer3.15.bn3.weight
	 layer3.15.bn3.bias
	 layer3.16.conv1.weight
	 layer3.16.bn1.weight
	 layer3.16.bn1.bias
	 layer3.16.conv2.weight
	 layer3.16.bn2.weight
	 layer3.16.bn2.bias
	 layer3.16.conv3.weight
	 layer3.16.bn3.weight
	 layer3.16.bn3.bias
	 layer3.17.conv1.weight
	 layer3.17.bn1.weight
	 layer3.17.bn1.bias
	 layer3.17.conv2.weight
	 layer3.17.bn2.weight
	 layer3.17.bn2.bias
	 layer3.17.conv3.weight
	 layer3.17.bn3.weight
	 layer3.17.bn3.bias
	 layer3.18.conv1.weight
	 layer3.18.bn1.weight
	 layer3.18.bn1.bias
	 layer3.18.conv2.weight
	 layer3.18.bn2.weight
	 layer3.18.bn2.bias
	 layer3.18.conv3.weight
	 layer3.18.bn3.weight
	 layer3.18.bn3.bias
	 layer3.19.conv1.weight
	 layer3.19.bn1.weight
	 layer3.19.bn1.bias
	 layer3.19.conv2.weight
	 layer3.19.bn2.weight
	 layer3.19.bn2.bias
	 layer3.19.conv3.weight
	 layer3.19.bn3.weight
	 layer3.19.bn3.bias
	 layer3.20.conv1.weight
	 layer3.20.bn1.weight
	 layer3.20.bn1.bias
	 layer3.20.conv2.weight
	 layer3.20.bn2.weight
	 layer3.20.bn2.bias
	 layer3.20.conv3.weight
	 layer3.20.bn3.weight
	 layer3.20.bn3.bias
	 layer3.21.conv1.weight
	 layer3.21.bn1.weight
	 layer3.21.bn1.bias
	 layer3.21.conv2.weight
	 layer3.21.bn2.weight
	 layer3.21.bn2.bias
	 layer3.21.conv3.weight
	 layer3.21.bn3.weight
	 layer3.21.bn3.bias
	 layer3.22.conv1.weight
	 layer3.22.bn1.weight
	 layer3.22.bn1.bias
	 layer3.22.conv2.weight
	 layer3.22.bn2.weight
	 layer3.22.bn2.bias
	 layer3.22.conv3.weight
	 layer3.22.bn3.weight
	 layer3.22.bn3.bias
	 layer3.23.conv1.weight
	 layer3.23.bn1.weight
	 layer3.23.bn1.bias
	 layer3.23.conv2.weight
	 layer3.23.bn2.weight
	 layer3.23.bn2.bias
	 layer3.23.conv3.weight
	 layer3.23.bn3.weight
	 layer3.23.bn3.bias
	 layer3.24.conv1.weight
	 layer3.24.bn1.weight
	 layer3.24.bn1.bias
	 layer3.24.conv2.weight
	 layer3.24.bn2.weight
	 layer3.24.bn2.bias
	 layer3.24.conv3.weight
	 layer3.24.bn3.weight
	 layer3.24.bn3.bias
	 layer3.25.conv1.weight
	 layer3.25.bn1.weight
	 layer3.25.bn1.bias
	 layer3.25.conv2.weight
	 layer3.25.bn2.weight
	 layer3.25.bn2.bias
	 layer3.25.conv3.weight
	 layer3.25.bn3.weight
	 layer3.25.bn3.bias
	 layer3.26.conv1.weight
	 layer3.26.bn1.weight
	 layer3.26.bn1.bias
	 layer3.26.conv2.weight
	 layer3.26.bn2.weight
	 layer3.26.bn2.bias
	 layer3.26.conv3.weight
	 layer3.26.bn3.weight
	 layer3.26.bn3.bias
	 layer3.27.conv1.weight
	 layer3.27.bn1.weight
	 layer3.27.bn1.bias
	 layer3.27.conv2.weight
	 layer3.27.bn2.weight
	 layer3.27.bn2.bias
	 layer3.27.conv3.weight
	 layer3.27.bn3.weight
	 layer3.27.bn3.bias
	 layer3.28.conv1.weight
	 layer3.28.bn1.weight
	 layer3.28.bn1.bias
	 layer3.28.conv2.weight
	 layer3.28.bn2.weight
	 layer3.28.bn2.bias
	 layer3.28.conv3.weight
	 layer3.28.bn3.weight
	 layer3.28.bn3.bias
	 layer3.29.conv1.weight
	 layer3.29.bn1.weight
	 layer3.29.bn1.bias
	 layer3.29.conv2.weight
	 layer3.29.bn2.weight
	 layer3.29.bn2.bias
	 layer3.29.conv3.weight
	 layer3.29.bn3.weight
	 layer3.29.bn3.bias
	 layer3.30.conv1.weight
	 layer3.30.bn1.weight
	 layer3.30.bn1.bias
	 layer3.30.conv2.weight
	 layer3.30.bn2.weight
	 layer3.30.bn2.bias
	 layer3.30.conv3.weight
	 layer3.30.bn3.weight
	 layer3.30.bn3.bias
	 layer3.31.conv1.weight
	 layer3.31.bn1.weight
	 layer3.31.bn1.bias
	 layer3.31.conv2.weight
	 layer3.31.bn2.weight
	 layer3.31.bn2.bias
	 layer3.31.conv3.weight
	 layer3.31.bn3.weight
	 layer3.31.bn3.bias
	 layer3.32.conv1.weight
	 layer3.32.bn1.weight
	 layer3.32.bn1.bias
	 layer3.32.conv2.weight
	 layer3.32.bn2.weight
	 layer3.32.bn2.bias
	 layer3.32.conv3.weight
	 layer3.32.bn3.weight
	 layer3.32.bn3.bias
	 layer3.33.conv1.weight
	 layer3.33.bn1.weight
	 layer3.33.bn1.bias
	 layer3.33.conv2.weight
	 layer3.33.bn2.weight
	 layer3.33.bn2.bias
	 layer3.33.conv3.weight
	 layer3.33.bn3.weight
	 layer3.33.bn3.bias
	 layer3.34.conv1.weight
	 layer3.34.bn1.weight
	 layer3.34.bn1.bias
	 layer3.34.conv2.weight
	 layer3.34.bn2.weight
	 layer3.34.bn2.bias
	 layer3.34.conv3.weight
	 layer3.34.bn3.weight
	 layer3.34.bn3.bias
	 layer3.35.conv1.weight
	 layer3.35.bn1.weight
	 layer3.35.bn1.bias
	 layer3.35.conv2.weight
	 layer3.35.bn2.weight
	 layer3.35.bn2.bias
	 layer3.35.conv3.weight
	 layer3.35.bn3.weight
	 layer3.35.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fc.weight
	 fc.bias
Train Iteration #7600:: 0.8359273076057434 Acc: 0.625
Train Iteration #7650:: 0.5462907552719116 Acc: 0.875
Train Iteration #7700:: 2.1221518516540527 Acc: 0.5
Train Iteration #7750:: 0.0274682454764843 Acc: 1.0
Train Iteration #7800:: 2.896556854248047 Acc: 0.625
Train Iteration #7850:: 1.0373033285140991 Acc: 0.75
Train Iteration #7900:: 1.1929296255111694 Acc: 0.625
Train Iteration #7950:: 0.08773687481880188 Acc: 1.0
Train Iteration #8000:: 1.3673067092895508 Acc: 0.75
Train Iteration #8050:: 0.6033899784088135 Acc: 0.75
Train Iteration #8100:: 0.1054423600435257 Acc: 0.875
Train Iteration #8150:: 0.31792882084846497 Acc: 0.875
Train Iteration #8200:: 0.18155622482299805 Acc: 0.875
Train Iteration #8250:: 1.1177196502685547 Acc: 0.5
Train Iteration #8300:: 0.030330689623951912 Acc: 1.0
Train Iteration #8350:: 1.57619047164917 Acc: 0.625
Train Iteration #8400:: 1.6234334707260132 Acc: 0.625
Train Iteration #8450:: 1.5796713829040527 Acc: 0.75
Train Iteration #8500:: 1.3852543830871582 Acc: 0.75
Train Iteration #8550:: 1.824224829673767 Acc: 0.5
Train Iteration #8600:: 0.5607619285583496 Acc: 0.875
Train Iteration #8650:: 1.1126914024353027 Acc: 0.625
Train Iteration #8700:: 1.5946974754333496 Acc: 0.75
Train Iteration #8750:: 1.4544315338134766 Acc: 0.75
Train Iteration #8800:: 0.14220213890075684 Acc: 1.0
Train Iteration #8850:: 0.0464974045753479 Acc: 1.0
Train Iteration #8900:: 0.18851272761821747 Acc: 1.0
Train Iteration #8950:: 0.05802667886018753 Acc: 1.0
Train Iteration #9000:: 0.019341301172971725 Acc: 1.0
Train Iteration #9050:: 1.1710233688354492 Acc: 0.75
Train Iteration #9100:: 1.798920750617981 Acc: 0.5
Epoch Training:: Loss: 0.6448 Acc: 0.8330
Validation Iteration #850:: 0.01097065955400467 Acc: 1.0
Validation Iteration #900:: 0.036995209753513336 Acc: 1.0
Validation Iteration #950:: 0.5325480699539185 Acc: 0.875
Validation Iteration #1000:: 0.5230375528335571 Acc: 0.875
Epoch Validation:: Loss: 0.2842 Acc: 0.9110
Epoch 6/29
----------
Train Iteration #9150:: 0.16880233585834503 Acc: 1.0
Train Iteration #9200:: 0.05698523297905922 Acc: 1.0
Train Iteration #9250:: 0.0811232179403305 Acc: 1.0
Train Iteration #9300:: 0.023996558040380478 Acc: 1.0
Train Iteration #9350:: 0.5562816262245178 Acc: 0.875
Train Iteration #9400:: 0.025016073137521744 Acc: 1.0
Train Iteration #9450:: 0.11174248158931732 Acc: 1.0
Train Iteration #9500:: 0.1788482666015625 Acc: 1.0
Train Iteration #9550:: 0.1494525521993637 Acc: 1.0
Train Iteration #9600:: 0.07583561539649963 Acc: 1.0
Train Iteration #9650:: 0.013844356872141361 Acc: 1.0
Train Iteration #9700:: 0.36860647797584534 Acc: 0.875
Train Iteration #9750:: 0.03257184848189354 Acc: 1.0
Train Iteration #9800:: 0.6650131940841675 Acc: 0.75
Train Iteration #9850:: 0.007061529904603958 Acc: 1.0
Train Iteration #9900:: 0.21572034060955048 Acc: 1.0
Train Iteration #9950:: 0.4377420246601105 Acc: 0.75
Train Iteration #10000:: 0.1122264564037323 Acc: 1.0
Train Iteration #10050:: 0.19165202975273132 Acc: 1.0
Train Iteration #10100:: 0.21535331010818481 Acc: 0.875
Train Iteration #10150:: 0.11090712249279022 Acc: 1.0
Train Iteration #10200:: 0.0638456642627716 Acc: 1.0
Train Iteration #10250:: 0.20789924263954163 Acc: 0.875
Train Iteration #10300:: 0.2979249060153961 Acc: 0.875
Train Iteration #10350:: 0.16322290897369385 Acc: 1.0
Train Iteration #10400:: 0.03871631622314453 Acc: 1.0
Train Iteration #10450:: 0.1401156187057495 Acc: 1.0
Train Iteration #10500:: 0.02866128273308277 Acc: 1.0
Train Iteration #10550:: 0.19893045723438263 Acc: 1.0
Train Iteration #10600:: 0.18169023096561432 Acc: 1.0
Epoch Training:: Loss: 0.3509 Acc: 0.8975
Validation Iteration #1050:: 1.042219877243042 Acc: 0.875
Validation Iteration #1100:: 0.23821879923343658 Acc: 0.75
Validation Iteration #1150:: 0.5694356560707092 Acc: 0.875
Epoch Validation:: Loss: 0.2665 Acc: 0.9118
Epoch 7/29
----------
Train Iteration #10650:: 0.21867850422859192 Acc: 1.0
Train Iteration #10700:: 0.2864190340042114 Acc: 0.875
Train Iteration #10750:: 0.2444760948419571 Acc: 0.875
Train Iteration #10800:: 1.0060778856277466 Acc: 0.75
Train Iteration #10850:: 0.34084030985832214 Acc: 0.75
Train Iteration #10900:: 0.4777165353298187 Acc: 0.875
Train Iteration #10950:: 0.38646283745765686 Acc: 0.875
Train Iteration #11000:: 0.20187048614025116 Acc: 0.875
Train Iteration #11050:: 0.2498372495174408 Acc: 0.875
Train Iteration #11100:: 0.09189175069332123 Acc: 1.0
Train Iteration #11150:: 0.4583228528499603 Acc: 0.875
Train Iteration #11200:: 0.30886223912239075 Acc: 0.875
Train Iteration #11250:: 0.5194180607795715 Acc: 0.75
Train Iteration #11300:: 0.09347984939813614 Acc: 1.0
Train Iteration #11350:: 0.10793168842792511 Acc: 1.0
Train Iteration #11400:: 0.024082809686660767 Acc: 1.0
Train Iteration #11450:: 0.48789066076278687 Acc: 0.875
Train Iteration #11500:: 0.37805843353271484 Acc: 0.75
Train Iteration #11550:: 0.16500523686408997 Acc: 1.0
Train Iteration #11600:: 0.08033327013254166 Acc: 1.0
Train Iteration #11650:: 0.6333007216453552 Acc: 0.875
Train Iteration #11700:: 0.014512772671878338 Acc: 1.0
Train Iteration #11750:: 0.0074449991807341576 Acc: 1.0
Train Iteration #11800:: 0.27459222078323364 Acc: 0.875
Train Iteration #11850:: 0.3286578953266144 Acc: 0.875
Train Iteration #11900:: 0.06029178202152252 Acc: 1.0
Train Iteration #11950:: 0.2959728538990021 Acc: 0.875
Train Iteration #12000:: 0.12880152463912964 Acc: 1.0
Train Iteration #12050:: 0.03055032528936863 Acc: 1.0
Train Iteration #12100:: 0.22419582307338715 Acc: 0.875
Epoch Training:: Loss: 0.3228 Acc: 0.9045
Validation Iteration #1200:: 0.13309279084205627 Acc: 1.0
Validation Iteration #1250:: 0.8667579889297485 Acc: 0.75
Validation Iteration #1300:: 0.594677209854126 Acc: 0.875
Validation Iteration #1350:: 0.34519726037979126 Acc: 0.875
Epoch Validation:: Loss: 0.2618 Acc: 0.9185
Epoch 8/29
----------
Train Iteration #12150:: 0.16550931334495544 Acc: 1.0
Train Iteration #12200:: 0.0614217072725296 Acc: 1.0
Train Iteration #12250:: 0.02849639765918255 Acc: 1.0
Train Iteration #12300:: 0.017030326649546623 Acc: 1.0
Train Iteration #12350:: 0.07317018508911133 Acc: 1.0
Train Iteration #12400:: 0.05848776549100876 Acc: 1.0
Train Iteration #12450:: 0.008057327941060066 Acc: 1.0
Train Iteration #12500:: 0.21670383214950562 Acc: 0.875
Train Iteration #12550:: 0.22642160952091217 Acc: 0.875
Train Iteration #12600:: 0.5305743217468262 Acc: 0.75
Train Iteration #12650:: 0.08832710981369019 Acc: 1.0
Train Iteration #12700:: 0.04530194029211998 Acc: 1.0
Train Iteration #12750:: 0.022021936252713203 Acc: 1.0
Train Iteration #12800:: 0.020548047497868538 Acc: 1.0
Train Iteration #12850:: 0.037151090800762177 Acc: 1.0
Train Iteration #12900:: 0.8368809223175049 Acc: 0.75
Train Iteration #12950:: 0.18710917234420776 Acc: 1.0
Train Iteration #13000:: 0.3595529794692993 Acc: 0.75
Train Iteration #13050:: 0.1578441560268402 Acc: 1.0
Train Iteration #13100:: 0.6576704978942871 Acc: 0.75
Train Iteration #13150:: 0.31093358993530273 Acc: 0.875
Train Iteration #13200:: 0.04027196019887924 Acc: 1.0
Train Iteration #13250:: 0.272672176361084 Acc: 0.875
Train Iteration #13300:: 0.26164510846138 Acc: 0.875
Train Iteration #13350:: 1.268107533454895 Acc: 0.875
Train Iteration #13400:: 0.03323758393526077 Acc: 1.0
Train Iteration #13450:: 1.8949267864227295 Acc: 0.75
Train Iteration #13500:: 0.00921536237001419 Acc: 1.0
Train Iteration #13550:: 0.32783493399620056 Acc: 0.875
Train Iteration #13600:: 0.9341321587562561 Acc: 0.75
Train Iteration #13650:: 0.029921595007181168 Acc: 1.0
Epoch Training:: Loss: 0.2888 Acc: 0.9123
Validation Iteration #1400:: 0.44311344623565674 Acc: 0.875
Validation Iteration #1450:: 0.048443689942359924 Acc: 1.0
Validation Iteration #1500:: 0.46483996510505676 Acc: 0.875
Epoch Validation:: Loss: 0.2534 Acc: 0.9303
Epoch 9/29
----------
Train Iteration #13700:: 0.08884936571121216 Acc: 1.0
Train Iteration #13750:: 0.44013404846191406 Acc: 0.75
Train Iteration #13800:: 0.10429862141609192 Acc: 1.0
Train Iteration #13850:: 0.06111522391438484 Acc: 1.0
Train Iteration #13900:: 0.2763079106807709 Acc: 0.875
Train Iteration #13950:: 0.133011132478714 Acc: 0.875
Train Iteration #14000:: 0.1116781085729599 Acc: 1.0
Train Iteration #14050:: 0.18138878047466278 Acc: 1.0
Train Iteration #14100:: 0.06718793511390686 Acc: 1.0
Train Iteration #14150:: 0.7002456784248352 Acc: 0.875
Train Iteration #14200:: 0.11878232657909393 Acc: 1.0
Train Iteration #14250:: 0.19392544031143188 Acc: 0.875
Train Iteration #14300:: 0.5259620547294617 Acc: 0.75
Train Iteration #14350:: 0.34776541590690613 Acc: 0.875
Train Iteration #14400:: 0.2233397364616394 Acc: 0.875
Train Iteration #14450:: 0.03580944985151291 Acc: 1.0
Train Iteration #14500:: 0.5210620164871216 Acc: 0.875
Train Iteration #14550:: 0.01953750103712082 Acc: 1.0
Train Iteration #14600:: 0.14150726795196533 Acc: 0.875
Train Iteration #14650:: 0.07234340906143188 Acc: 1.0
Train Iteration #14700:: 0.27853482961654663 Acc: 1.0
Train Iteration #14750:: 0.0997040867805481 Acc: 1.0
Train Iteration #14800:: 0.941999077796936 Acc: 0.75
Train Iteration #14850:: 0.12604185938835144 Acc: 1.0
Train Iteration #14900:: 0.14495466649532318 Acc: 0.875
Train Iteration #14950:: 0.12027961015701294 Acc: 1.0
Train Iteration #15000:: 0.041187722235918045 Acc: 1.0
Train Iteration #15050:: 0.3955717384815216 Acc: 0.625
Train Iteration #15100:: 0.2863079309463501 Acc: 0.875
Train Iteration #15150:: 0.09095492213964462 Acc: 1.0
Epoch Training:: Loss: 0.2685 Acc: 0.9195
Validation Iteration #1550:: 0.00011974586232099682 Acc: 1.0
Validation Iteration #1600:: 0.6438719034194946 Acc: 0.75
Validation Iteration #1650:: 0.42128878831863403 Acc: 0.875
Epoch Validation:: Loss: 0.2529 Acc: 0.9059
Epoch 10/29
----------
Train Iteration #15200:: 0.8441341519355774 Acc: 0.875
Train Iteration #15250:: 0.09893619269132614 Acc: 1.0
Train Iteration #15300:: 0.028014861047267914 Acc: 1.0
Train Iteration #15350:: 0.38227981328964233 Acc: 0.75
Train Iteration #15400:: 0.3794158697128296 Acc: 0.75
Train Iteration #15450:: 0.013923793099820614 Acc: 1.0
Train Iteration #15500:: 0.4035194516181946 Acc: 0.875
Train Iteration #15550:: 0.4963047504425049 Acc: 0.875
Train Iteration #15600:: 0.9981327652931213 Acc: 0.875
Train Iteration #15650:: 0.1673862487077713 Acc: 0.875
Train Iteration #15700:: 0.04790547490119934 Acc: 1.0
Train Iteration #15750:: 0.31075647473335266 Acc: 0.875
Train Iteration #15800:: 0.055264804512262344 Acc: 1.0
Train Iteration #15850:: 0.3037793040275574 Acc: 1.0
Train Iteration #15900:: 0.11063647270202637 Acc: 0.875
Train Iteration #15950:: 0.0664990171790123 Acc: 1.0
Train Iteration #16000:: 0.028136281296610832 Acc: 1.0
Train Iteration #16050:: 0.027317851781845093 Acc: 1.0
Train Iteration #16100:: 0.00526589248329401 Acc: 1.0
Train Iteration #16150:: 0.23828208446502686 Acc: 0.875
Train Iteration #16200:: 0.08187940716743469 Acc: 1.0
Train Iteration #16250:: 0.32930120825767517 Acc: 0.875
Train Iteration #16300:: 0.1585293859243393 Acc: 0.875
Train Iteration #16350:: 0.031373050063848495 Acc: 1.0
Train Iteration #16400:: 0.05346754938364029 Acc: 1.0
Train Iteration #16450:: 0.4636383354663849 Acc: 0.75
Train Iteration #16500:: 0.1709037870168686 Acc: 1.0
Train Iteration #16550:: 0.04227318614721298 Acc: 1.0
Train Iteration #16600:: 0.15033003687858582 Acc: 1.0
Train Iteration #16650:: 0.162700355052948 Acc: 0.875
Epoch Training:: Loss: 0.2339 Acc: 0.9299
Validation Iteration #1700:: 0.5488437414169312 Acc: 0.75
Validation Iteration #1750:: 0.3343161344528198 Acc: 0.875
Validation Iteration #1800:: 0.0431249774992466 Acc: 1.0
Validation Iteration #1850:: 0.304075688123703 Acc: 0.875
Epoch Validation:: Loss: 0.2916 Acc: 0.8918
Epoch 11/29
----------
Train Iteration #16700:: 0.17732447385787964 Acc: 1.0
Train Iteration #16750:: 0.04764409363269806 Acc: 1.0
Train Iteration #16800:: 0.007921566255390644 Acc: 1.0
Train Iteration #16850:: 0.05161580443382263 Acc: 1.0
Train Iteration #16900:: 0.5625922679901123 Acc: 0.875
Train Iteration #16950:: 0.051267292350530624 Acc: 1.0
Train Iteration #17000:: 0.6598998308181763 Acc: 0.625
Train Iteration #17050:: 0.2816653251647949 Acc: 0.875
Train Iteration #17100:: 0.046575941145420074 Acc: 1.0
Train Iteration #17150:: 0.13481804728507996 Acc: 0.875
Train Iteration #17200:: 0.021389558911323547 Acc: 1.0
Train Iteration #17250:: 0.2809058427810669 Acc: 0.75
Train Iteration #17300:: 0.004236039239913225 Acc: 1.0
Train Iteration #17350:: 0.030122030526399612 Acc: 1.0
Train Iteration #17400:: 0.14992769062519073 Acc: 1.0
Train Iteration #17450:: 0.040850892663002014 Acc: 1.0
Train Iteration #17500:: 0.1366715431213379 Acc: 1.0
Train Iteration #17550:: 0.024622831493616104 Acc: 1.0
Train Iteration #17600:: 0.02046714909374714 Acc: 1.0
Train Iteration #17650:: 0.14926157891750336 Acc: 0.875
Train Iteration #17700:: 0.17112687230110168 Acc: 0.875
Train Iteration #17750:: 0.025288928300142288 Acc: 1.0
Train Iteration #17800:: 0.24754387140274048 Acc: 0.875
Train Iteration #17850:: 0.005012808833271265 Acc: 1.0
Train Iteration #17900:: 0.014523348771035671 Acc: 1.0
Train Iteration #17950:: 0.7831568717956543 Acc: 0.75
Train Iteration #18000:: 0.05779695883393288 Acc: 1.0
Train Iteration #18050:: 0.07633418589830399 Acc: 1.0
Train Iteration #18100:: 0.08614499866962433 Acc: 1.0
Train Iteration #18150:: 0.5684441328048706 Acc: 0.875
Train Iteration #18200:: 0.046257853507995605 Acc: 1.0
Epoch Training:: Loss: 0.2202 Acc: 0.9327
Validation Iteration #1900:: 0.5168496370315552 Acc: 0.875
Validation Iteration #1950:: 1.859169363975525 Acc: 0.625
Validation Iteration #2000:: 0.7058225870132446 Acc: 0.875
Epoch Validation:: Loss: 0.6118 Acc: 0.8199
Epoch 12/29
----------
Train Iteration #18250:: 0.12205784022808075 Acc: 1.0
Train Iteration #18300:: 0.13843953609466553 Acc: 1.0
Train Iteration #18350:: 0.5827636122703552 Acc: 0.875
Train Iteration #18400:: 0.11652690917253494 Acc: 1.0
Train Iteration #18450:: 0.04439667984843254 Acc: 1.0
Train Iteration #18500:: 1.1072876453399658 Acc: 0.75
Train Iteration #18550:: 0.10589677095413208 Acc: 1.0
Train Iteration #18600:: 0.389363169670105 Acc: 0.875
Train Iteration #18650:: 0.10690178722143173 Acc: 1.0
Train Iteration #18700:: 0.13110333681106567 Acc: 1.0
Train Iteration #18750:: 0.27062976360321045 Acc: 0.75
Train Iteration #18800:: 0.14030247926712036 Acc: 0.875
Train Iteration #18850:: 0.19379566609859467 Acc: 1.0
Train Iteration #18900:: 0.2517783045768738 Acc: 0.875
Train Iteration #18950:: 0.16083969175815582 Acc: 0.875
Train Iteration #19000:: 0.03803522139787674 Acc: 1.0
Train Iteration #19050:: 0.011938042007386684 Acc: 1.0
Train Iteration #19100:: 0.028031766414642334 Acc: 1.0
Train Iteration #19150:: 0.09139746427536011 Acc: 1.0
Train Iteration #19200:: 0.4050745666027069 Acc: 1.0
Train Iteration #19250:: 0.2829521894454956 Acc: 0.875
Train Iteration #19300:: 0.008854185231029987 Acc: 1.0
Train Iteration #19350:: 0.0019133738242089748 Acc: 1.0
Train Iteration #19400:: 0.01866104081273079 Acc: 1.0
Train Iteration #19450:: 0.05301818251609802 Acc: 1.0
Train Iteration #19500:: 0.4008351266384125 Acc: 0.875
Train Iteration #19550:: 0.0874864012002945 Acc: 1.0
Train Iteration #19600:: 0.6890214681625366 Acc: 0.75
Train Iteration #19650:: 0.14910796284675598 Acc: 1.0
Train Iteration #19700:: 0.2947996258735657 Acc: 1.0
Epoch Training:: Loss: 0.2148 Acc: 0.9376
Validation Iteration #2050:: 1.1327670812606812 Acc: 0.875
Validation Iteration #2100:: 0.6453307867050171 Acc: 0.875
Validation Iteration #2150:: 2.0546410723909503e-06 Acc: 1.0
Epoch Validation:: Loss: 0.2179 Acc: 0.9437
Epoch 13/29
----------
Train Iteration #19750:: 0.11690996587276459 Acc: 1.0
Train Iteration #19800:: 0.003280356526374817 Acc: 1.0
Train Iteration #19850:: 0.17649850249290466 Acc: 0.875
Train Iteration #19900:: 0.21433134377002716 Acc: 0.875
Train Iteration #19950:: 0.23472929000854492 Acc: 1.0
Train Iteration #20000:: 0.05026823282241821 Acc: 1.0
Train Iteration #20050:: 0.0094236945733428 Acc: 1.0
Train Iteration #20100:: 0.051203660666942596 Acc: 1.0
Train Iteration #20150:: 0.16083678603172302 Acc: 1.0
Train Iteration #20200:: 0.010848055593669415 Acc: 1.0
Train Iteration #20250:: 0.6053287386894226 Acc: 0.875
Train Iteration #20300:: 0.020140159875154495 Acc: 1.0
Train Iteration #20350:: 0.028010347858071327 Acc: 1.0
Train Iteration #20400:: 0.0032280944287776947 Acc: 1.0
Train Iteration #20450:: 0.0006885211914777756 Acc: 1.0
Train Iteration #20500:: 1.7576693296432495 Acc: 0.5
Train Iteration #20550:: 0.09675806760787964 Acc: 1.0
Train Iteration #20600:: 0.001361281261779368 Acc: 1.0
Train Iteration #20650:: 0.0038138492964208126 Acc: 1.0
Train Iteration #20700:: 0.02184128388762474 Acc: 1.0
Train Iteration #20750:: 0.008699367754161358 Acc: 1.0
Train Iteration #20800:: 0.08724891394376755 Acc: 1.0
Train Iteration #20850:: 0.0026386005338281393 Acc: 1.0
Train Iteration #20900:: 0.11117532849311829 Acc: 0.875
Train Iteration #20950:: 0.002636255230754614 Acc: 1.0
Train Iteration #21000:: 0.6614391803741455 Acc: 0.875
Train Iteration #21050:: 0.004454744979739189 Acc: 1.0
Train Iteration #21100:: 0.2190341204404831 Acc: 0.875
Train Iteration #21150:: 0.18704941868782043 Acc: 0.875
Train Iteration #21200:: 0.12286054342985153 Acc: 1.0
Train Iteration #21250:: 0.1584160476922989 Acc: 1.0
Epoch Training:: Loss: 0.1889 Acc: 0.9435
Validation Iteration #2200:: 0.09026604890823364 Acc: 1.0
Validation Iteration #2250:: 0.11573881655931473 Acc: 0.875
Validation Iteration #2300:: 0.20998823642730713 Acc: 0.75
Validation Iteration #2350:: 0.03183473274111748 Acc: 1.0
Epoch Validation:: Loss: 0.2274 Acc: 0.9385
Epoch 14/29
----------
Train Iteration #21300:: 0.03186153620481491 Acc: 1.0
Train Iteration #21350:: 0.5508929491043091 Acc: 0.75
Train Iteration #21400:: 0.12021598219871521 Acc: 0.875
Train Iteration #21450:: 0.40332648158073425 Acc: 0.875
Train Iteration #21500:: 0.75905442237854 Acc: 0.875
Train Iteration #21550:: 0.11110028624534607 Acc: 1.0
Train Iteration #21600:: 0.04229825735092163 Acc: 1.0
Train Iteration #21650:: 0.4864426553249359 Acc: 0.875
Train Iteration #21700:: 0.21903136372566223 Acc: 0.875
Train Iteration #21750:: 0.02417660318315029 Acc: 1.0
Train Iteration #21800:: 0.06663219630718231 Acc: 1.0
Train Iteration #21850:: 0.08207298070192337 Acc: 1.0
Train Iteration #21900:: 0.06250596046447754 Acc: 1.0
Train Iteration #21950:: 0.29206371307373047 Acc: 0.875
Train Iteration #22000:: 0.0010795290581882 Acc: 1.0
Train Iteration #22050:: 1.0860910415649414 Acc: 0.625
Train Iteration #22100:: 0.4338644742965698 Acc: 0.875
Train Iteration #22150:: 0.017804931849241257 Acc: 1.0
Train Iteration #22200:: 0.0051171439699828625 Acc: 1.0
Train Iteration #22250:: 0.19954770803451538 Acc: 1.0
Train Iteration #22300:: 0.18339622020721436 Acc: 0.875
Train Iteration #22350:: 0.016970181837677956 Acc: 1.0
Train Iteration #22400:: 0.0270205307751894 Acc: 1.0
Train Iteration #22450:: 0.24854233860969543 Acc: 0.875
Train Iteration #22500:: 0.05014032498002052 Acc: 1.0
Train Iteration #22550:: 1.1692394018173218 Acc: 0.875
Train Iteration #22600:: 0.17678283154964447 Acc: 0.875
Train Iteration #22650:: 0.18282561004161835 Acc: 0.875
Train Iteration #22700:: 1.0398037433624268 Acc: 0.875
Train Iteration #22750:: 0.005362926982343197 Acc: 1.0
Epoch Training:: Loss: 0.1635 Acc: 0.9537
Validation Iteration #2400:: 1.3463921546936035 Acc: 0.75
Validation Iteration #2450:: 0.12276790291070938 Acc: 1.0
Validation Iteration #2500:: 0.20566794276237488 Acc: 0.875
Epoch Validation:: Loss: 0.2794 Acc: 0.9333
Epoch 15/29
----------
Train Iteration #22800:: 1.108282208442688 Acc: 0.75
Train Iteration #22850:: 0.6650960445404053 Acc: 0.75
Train Iteration #22900:: 0.00644319225102663 Acc: 1.0
Train Iteration #22950:: 0.07462982088327408 Acc: 1.0
Train Iteration #23000:: 0.2199339121580124 Acc: 1.0
Train Iteration #23050:: 0.009735566563904285 Acc: 1.0
Train Iteration #23100:: 0.06668397039175034 Acc: 1.0
Train Iteration #23150:: 0.07415644824504852 Acc: 1.0
Train Iteration #23200:: 0.022969534620642662 Acc: 1.0
Train Iteration #23250:: 0.48773691058158875 Acc: 0.625
Train Iteration #23300:: 0.012299268506467342 Acc: 1.0
Train Iteration #23350:: 0.34198638796806335 Acc: 0.875
Train Iteration #23400:: 0.014894486404955387 Acc: 1.0
Train Iteration #23450:: 0.027703585103154182 Acc: 1.0
Train Iteration #23500:: 0.0010004835203289986 Acc: 1.0
Train Iteration #23550:: 0.28174179792404175 Acc: 0.875
Train Iteration #23600:: 0.002381080761551857 Acc: 1.0
Train Iteration #23650:: 0.007959604263305664 Acc: 1.0
Train Iteration #23700:: 0.0180734321475029 Acc: 1.0
Train Iteration #23750:: 0.2187901884317398 Acc: 0.875
Train Iteration #23800:: 0.0003565589722711593 Acc: 1.0
Train Iteration #23850:: 0.11372815072536469 Acc: 1.0
Train Iteration #23900:: 0.08657270669937134 Acc: 1.0
Train Iteration #23950:: 0.7380845546722412 Acc: 0.875
Train Iteration #24000:: 0.08170604705810547 Acc: 1.0
Train Iteration #24050:: 0.13281427323818207 Acc: 0.875
Train Iteration #24100:: 0.007812989875674248 Acc: 1.0
Train Iteration #24150:: 0.10137756913900375 Acc: 1.0
Train Iteration #24200:: 0.04344354197382927 Acc: 1.0
Train Iteration #24250:: 0.006403912790119648 Acc: 1.0
Epoch Training:: Loss: 0.1668 Acc: 0.9535
Validation Iteration #2550:: 0.9097290635108948 Acc: 0.875
Validation Iteration #2600:: 0.11222561448812485 Acc: 1.0
Validation Iteration #2650:: 1.4066696166992188e-05 Acc: 1.0
Validation Iteration #2700:: 0.4055391550064087 Acc: 0.875
Epoch Validation:: Loss: 0.3539 Acc: 0.9429
Epoch 16/29
----------
Train Iteration #24300:: 0.006564030889421701 Acc: 1.0
Train Iteration #24350:: 0.08006051182746887 Acc: 1.0
Train Iteration #24400:: 0.5068673491477966 Acc: 0.75
Train Iteration #24450:: 0.03776169568300247 Acc: 1.0
Train Iteration #24500:: 0.05622716248035431 Acc: 1.0
Train Iteration #24550:: 0.019269909709692 Acc: 1.0
Train Iteration #24600:: 0.037805382162332535 Acc: 1.0
Train Iteration #24650:: 0.10435058176517487 Acc: 1.0
Train Iteration #24700:: 0.24136528372764587 Acc: 1.0
Train Iteration #24750:: 0.023207738995552063 Acc: 1.0
Train Iteration #24800:: 0.09022945910692215 Acc: 1.0
Train Iteration #24850:: 0.04334500432014465 Acc: 1.0
Train Iteration #24900:: 0.18756641447544098 Acc: 0.875
Train Iteration #24950:: 0.17887164652347565 Acc: 0.875
Train Iteration #25000:: 0.2124745398759842 Acc: 1.0
Train Iteration #25050:: 0.05848810449242592 Acc: 1.0
Train Iteration #25100:: 0.2721887528896332 Acc: 0.875
Train Iteration #25150:: 0.003655202453956008 Acc: 1.0
Train Iteration #25200:: 0.15724170207977295 Acc: 0.875
Train Iteration #25250:: 0.035659309476614 Acc: 1.0
Train Iteration #25300:: 0.06891632080078125 Acc: 1.0
Train Iteration #25350:: 0.0043853819370269775 Acc: 1.0
Train Iteration #25400:: 0.004699775017797947 Acc: 1.0
Train Iteration #25450:: 0.3451482355594635 Acc: 0.875
Train Iteration #25500:: 0.09299442172050476 Acc: 1.0
Train Iteration #25550:: 0.5182313323020935 Acc: 0.75
Train Iteration #25600:: 0.08885635435581207 Acc: 1.0
Train Iteration #25650:: 0.3690166771411896 Acc: 0.75
Train Iteration #25700:: 0.0018788231536746025 Acc: 1.0
Train Iteration #25750:: 0.04357762262225151 Acc: 1.0
Train Iteration #25800:: 0.015590306371450424 Acc: 1.0
Epoch Training:: Loss: 0.1595 Acc: 0.9539
Validation Iteration #2750:: 0.15390804409980774 Acc: 1.0
Validation Iteration #2800:: 0.002692567650228739 Acc: 1.0
Validation Iteration #2850:: 0.27228087186813354 Acc: 0.875
Epoch Validation:: Loss: 0.2184 Acc: 0.9474
Epoch 17/29
----------
Train Iteration #25850:: 0.112972691655159 Acc: 1.0
Train Iteration #25900:: 0.06827223300933838 Acc: 1.0
Train Iteration #25950:: 0.29631829261779785 Acc: 1.0
Train Iteration #26000:: 0.005902084521949291 Acc: 1.0
Train Iteration #26050:: 0.07273705303668976 Acc: 1.0
Train Iteration #26100:: 0.40472081303596497 Acc: 0.875
Train Iteration #26150:: 0.44240593910217285 Acc: 0.75
Train Iteration #26200:: 0.16334542632102966 Acc: 0.875
Train Iteration #26250:: 0.07012461870908737 Acc: 1.0
Train Iteration #26300:: 0.07066794484853745 Acc: 1.0
Train Iteration #26350:: 0.03968558833003044 Acc: 1.0
Train Iteration #26400:: 0.3102826476097107 Acc: 0.75
Train Iteration #26450:: 0.10992307960987091 Acc: 1.0
Train Iteration #26500:: 0.23557601869106293 Acc: 0.875
Train Iteration #26550:: 0.051553137600421906 Acc: 1.0
Train Iteration #26600:: 0.3745751678943634 Acc: 0.875
Train Iteration #26650:: 0.09767944365739822 Acc: 1.0
Train Iteration #26700:: 0.12675738334655762 Acc: 0.875
Train Iteration #26750:: 0.0056673879735171795 Acc: 1.0
Train Iteration #26800:: 0.028101567178964615 Acc: 1.0
Train Iteration #26850:: 0.02023015357553959 Acc: 1.0
Train Iteration #26900:: 0.1604379117488861 Acc: 1.0
Train Iteration #26950:: 0.263123095035553 Acc: 0.875
Train Iteration #27000:: 0.05191370099782944 Acc: 1.0
Train Iteration #27050:: 0.21278183162212372 Acc: 0.875
Train Iteration #27100:: 0.012388993054628372 Acc: 1.0
Train Iteration #27150:: 0.2123301476240158 Acc: 0.875
Train Iteration #27200:: 0.008517199195921421 Acc: 1.0
Train Iteration #27250:: 0.01818358711898327 Acc: 1.0
Train Iteration #27300:: 0.027256624773144722 Acc: 1.0
Epoch Training:: Loss: 0.1441 Acc: 0.9592
Validation Iteration #2900:: 0.02488519437611103 Acc: 1.0
Validation Iteration #2950:: 0.02983871102333069 Acc: 1.0
Validation Iteration #3000:: 0.06267237663269043 Acc: 1.0
Epoch Validation:: Loss: 0.2693 Acc: 0.9451
Epoch 18/29
----------
Train Iteration #27350:: 0.05475705862045288 Acc: 1.0
Train Iteration #27400:: 0.005373578518629074 Acc: 1.0
Train Iteration #27450:: 0.06420961022377014 Acc: 1.0
Train Iteration #27500:: 0.00641985610127449 Acc: 1.0
Train Iteration #27550:: 0.010099121369421482 Acc: 1.0
Train Iteration #27600:: 0.012338579632341862 Acc: 1.0
Train Iteration #27650:: 0.016023356467485428 Acc: 1.0
Train Iteration #27700:: 0.3847516179084778 Acc: 0.875
Train Iteration #27750:: 0.07575930655002594 Acc: 1.0
Train Iteration #27800:: 0.20997370779514313 Acc: 0.875
Train Iteration #27850:: 0.2754366099834442 Acc: 0.875
Train Iteration #27900:: 1.3876972198486328 Acc: 0.875
Train Iteration #27950:: 0.6579685807228088 Acc: 0.75
Train Iteration #28000:: 0.0242488794028759 Acc: 1.0
Train Iteration #28050:: 0.06559403985738754 Acc: 1.0
Train Iteration #28100:: 0.004613572731614113 Acc: 1.0
Train Iteration #28150:: 0.01165103167295456 Acc: 1.0
Train Iteration #28200:: 0.010745681822299957 Acc: 1.0
Train Iteration #28250:: 0.017644871026277542 Acc: 1.0
Train Iteration #28300:: 0.033874161541461945 Acc: 1.0
Train Iteration #28350:: 0.002035328187048435 Acc: 1.0
Train Iteration #28400:: 0.10535407811403275 Acc: 0.875
Train Iteration #28450:: 0.8317219614982605 Acc: 0.875
Train Iteration #28500:: 0.03243003413081169 Acc: 1.0
Train Iteration #28550:: 0.005149751901626587 Acc: 1.0
Train Iteration #28600:: 0.013360992074012756 Acc: 1.0
Train Iteration #28650:: 0.04706937074661255 Acc: 1.0
Train Iteration #28700:: 0.006222528871148825 Acc: 1.0
Train Iteration #28750:: 0.006777135655283928 Acc: 1.0
Train Iteration #28800:: 0.008996878750622272 Acc: 1.0
Epoch Training:: Loss: 0.1278 Acc: 0.9652
Validation Iteration #3050:: 0.39958512783050537 Acc: 0.875
Validation Iteration #3100:: 1.447983980178833 Acc: 0.75
Validation Iteration #3150:: 0.11054199934005737 Acc: 0.875
Validation Iteration #3200:: 0.002536909654736519 Acc: 1.0
Epoch Validation:: Loss: 0.4029 Acc: 0.8940
Epoch 19/29
----------
Train Iteration #28850:: 0.06135503202676773 Acc: 1.0
Train Iteration #28900:: 0.2960059344768524 Acc: 0.875
Train Iteration #28950:: 0.35826030373573303 Acc: 0.875
Train Iteration #29000:: 0.027143269777297974 Acc: 1.0
Train Iteration #29050:: 0.375866174697876 Acc: 0.875
Train Iteration #29100:: 0.11311318725347519 Acc: 0.875
Train Iteration #29150:: 0.09928284585475922 Acc: 1.0
Train Iteration #29200:: 0.014554150402545929 Acc: 1.0
Train Iteration #29250:: 0.002584219677373767 Acc: 1.0
Train Iteration #29300:: 0.03697396069765091 Acc: 1.0
Train Iteration #29350:: 0.06946157664060593 Acc: 1.0
Train Iteration #29400:: 0.07897039502859116 Acc: 1.0
Train Iteration #29450:: 0.00676106009632349 Acc: 1.0
Train Iteration #29500:: 0.10365884751081467 Acc: 0.875
Train Iteration #29550:: 0.0017943396233022213 Acc: 1.0
Train Iteration #29600:: 0.062457866966724396 Acc: 1.0
Train Iteration #29650:: 0.04841998964548111 Acc: 1.0
Train Iteration #29700:: 0.008766730315983295 Acc: 1.0
Train Iteration #29750:: 0.11345337331295013 Acc: 1.0
Train Iteration #29800:: 0.31376391649246216 Acc: 0.875
Train Iteration #29850:: 0.03952229768037796 Acc: 1.0
Train Iteration #29900:: 0.012147386558353901 Acc: 1.0
Train Iteration #29950:: 0.011248691007494926 Acc: 1.0
Train Iteration #30000:: 0.024193525314331055 Acc: 1.0
Train Iteration #30050:: 0.09261012077331543 Acc: 1.0
Train Iteration #30100:: 0.06294972449541092 Acc: 1.0
Train Iteration #30150:: 0.06830310821533203 Acc: 1.0
Train Iteration #30200:: 0.07753945142030716 Acc: 1.0
Train Iteration #30250:: 0.4354288876056671 Acc: 0.875
Train Iteration #30300:: 0.006575081963092089 Acc: 1.0
Train Iteration #30350:: 0.02021152526140213 Acc: 1.0
Epoch Training:: Loss: 0.1335 Acc: 0.9631
Validation Iteration #3250:: 0.09914014488458633 Acc: 1.0
Validation Iteration #3300:: 0.07117141038179398 Acc: 1.0
Validation Iteration #3350:: 0.09172224253416061 Acc: 1.0
Epoch Validation:: Loss: 0.2813 Acc: 0.9392
Epoch 20/29
----------
Train Iteration #30400:: 0.0869726613163948 Acc: 1.0
Train Iteration #30450:: 0.020628895610570908 Acc: 1.0
Train Iteration #30500:: 0.002843051217496395 Acc: 1.0
Train Iteration #30550:: 0.002833525650203228 Acc: 1.0
Train Iteration #30600:: 0.00133330631069839 Acc: 1.0
Train Iteration #30650:: 0.031745996326208115 Acc: 1.0
Train Iteration #30700:: 0.04377509281039238 Acc: 1.0
Train Iteration #30750:: 0.014949619770050049 Acc: 1.0
Train Iteration #30800:: 0.2855086326599121 Acc: 0.875
Train Iteration #30850:: 0.012088518589735031 Acc: 1.0
Train Iteration #30900:: 0.27307558059692383 Acc: 0.875
Train Iteration #30950:: 0.017346711829304695 Acc: 1.0
Train Iteration #31000:: 0.39654549956321716 Acc: 0.75
Train Iteration #31050:: 0.051342207938432693 Acc: 1.0
Train Iteration #31100:: 0.007036774419248104 Acc: 1.0
Train Iteration #31150:: 0.11827003955841064 Acc: 0.875
Train Iteration #31200:: 0.042061395943164825 Acc: 1.0
Train Iteration #31250:: 0.003830227768048644 Acc: 1.0
Train Iteration #31300:: 0.006321942899376154 Acc: 1.0
Train Iteration #31350:: 0.01499144732952118 Acc: 1.0
Train Iteration #31400:: 0.00029019295470789075 Acc: 1.0
Train Iteration #31450:: 0.07913124561309814 Acc: 1.0
Train Iteration #31500:: 0.002687386004254222 Acc: 1.0
Train Iteration #31550:: 0.004807494580745697 Acc: 1.0
Train Iteration #31600:: 0.08867548406124115 Acc: 0.875
Train Iteration #31650:: 0.012951274402439594 Acc: 1.0
Train Iteration #31700:: 0.09357954561710358 Acc: 0.875
Train Iteration #31750:: 0.0006627014372497797 Acc: 1.0
Train Iteration #31800:: 0.029998695477843285 Acc: 1.0
Train Iteration #31850:: 0.19573991000652313 Acc: 1.0
Epoch Training:: Loss: 0.1123 Acc: 0.9681
Validation Iteration #3400:: 0.002638756763190031 Acc: 1.0
Validation Iteration #3450:: 0.11547151207923889 Acc: 1.0
Validation Iteration #3500:: 0.023835137486457825 Acc: 1.0
Epoch Validation:: Loss: 0.2640 Acc: 0.9444
Epoch 21/29
----------
Train Iteration #31900:: 0.48524922132492065 Acc: 0.875
Train Iteration #31950:: 0.1206224262714386 Acc: 1.0
Train Iteration #32000:: 0.0072674877010285854 Acc: 1.0
Train Iteration #32050:: 0.042331479489803314 Acc: 1.0
Train Iteration #32100:: 0.012132957577705383 Acc: 1.0
Train Iteration #32150:: 0.22864167392253876 Acc: 0.875
Train Iteration #32200:: 0.43389540910720825 Acc: 0.75
Train Iteration #32250:: 0.00550220999866724 Acc: 1.0
Train Iteration #32300:: 0.07996328920125961 Acc: 1.0
Train Iteration #32350:: 0.21205787360668182 Acc: 0.875
Train Iteration #32400:: 0.002291915938258171 Acc: 1.0
Train Iteration #32450:: 0.003170629031956196 Acc: 1.0
Train Iteration #32500:: 0.12167704105377197 Acc: 1.0
Train Iteration #32550:: 0.008063645102083683 Acc: 1.0
Train Iteration #32600:: 0.02309468388557434 Acc: 1.0
Train Iteration #32650:: 0.022704850882291794 Acc: 1.0
Train Iteration #32700:: 0.8102340698242188 Acc: 0.75
Train Iteration #32750:: 0.0018969851080328226 Acc: 1.0
Train Iteration #32800:: 0.0021801581606268883 Acc: 1.0
Train Iteration #32850:: 0.006308274809271097 Acc: 1.0
Train Iteration #32900:: 0.005057735834270716 Acc: 1.0
Train Iteration #32950:: 0.07830393314361572 Acc: 1.0
Train Iteration #33000:: 0.015757668763399124 Acc: 1.0
Train Iteration #33050:: 0.060160741209983826 Acc: 1.0
Train Iteration #33100:: 0.020491253584623337 Acc: 1.0
Train Iteration #33150:: 0.0029624225571751595 Acc: 1.0
Train Iteration #33200:: 0.14623555541038513 Acc: 0.875
Train Iteration #33250:: 0.012319957837462425 Acc: 1.0
Train Iteration #33300:: 0.13014854490756989 Acc: 1.0
Train Iteration #33350:: 0.011794336140155792 Acc: 1.0
Epoch Training:: Loss: 0.1179 Acc: 0.9673
Validation Iteration #3550:: 0.012136346660554409 Acc: 1.0
Validation Iteration #3600:: 0.0416535958647728 Acc: 1.0
Validation Iteration #3650:: 0.00012791468179784715 Acc: 1.0
Validation Iteration #3700:: 0.014444584026932716 Acc: 1.0
Epoch Validation:: Loss: 0.3203 Acc: 0.9125
Epoch 22/29
----------
Train Iteration #33400:: 0.1681627631187439 Acc: 0.875
Train Iteration #33450:: 0.026780594140291214 Acc: 1.0
Train Iteration #33500:: 0.01605420932173729 Acc: 1.0
Train Iteration #33550:: 0.2782021462917328 Acc: 0.875
Train Iteration #33600:: 0.1749678999185562 Acc: 0.875
Train Iteration #33650:: 0.0016952544683590531 Acc: 1.0
Train Iteration #33700:: 0.3907620906829834 Acc: 0.875
Train Iteration #33750:: 0.014194914139807224 Acc: 1.0
Train Iteration #33800:: 0.7713382840156555 Acc: 0.75
Train Iteration #33850:: 0.4180392622947693 Acc: 0.75
Train Iteration #33900:: 0.05720982328057289 Acc: 1.0
Train Iteration #33950:: 0.08963915705680847 Acc: 1.0
Train Iteration #34000:: 0.044182877987623215 Acc: 1.0
Train Iteration #34050:: 0.12254852056503296 Acc: 1.0
Train Iteration #34100:: 0.006511339917778969 Acc: 1.0
Train Iteration #34150:: 0.001972295343875885 Acc: 1.0
Train Iteration #34200:: 0.10162506997585297 Acc: 0.875
Train Iteration #34250:: 0.004965510684996843 Acc: 1.0
Train Iteration #34300:: 0.0735895112156868 Acc: 1.0
Train Iteration #34350:: 0.006481054238975048 Acc: 1.0
Train Iteration #34400:: 0.0018702156376093626 Acc: 1.0
Train Iteration #34450:: 0.08578523993492126 Acc: 1.0
Train Iteration #34500:: 0.1849897801876068 Acc: 0.875
Train Iteration #34550:: 0.0054205022752285 Acc: 1.0
Train Iteration #34600:: 0.03450159728527069 Acc: 1.0
Train Iteration #34650:: 0.25142350792884827 Acc: 0.875
Train Iteration #34700:: 0.009865552186965942 Acc: 1.0
Train Iteration #34750:: 0.007617564406245947 Acc: 1.0
Train Iteration #34800:: 0.00013211939949542284 Acc: 1.0
Train Iteration #34850:: 0.018641671165823936 Acc: 1.0
Train Iteration #34900:: 0.01365486066788435 Acc: 1.0
Epoch Training:: Loss: 0.1070 Acc: 0.9700
Validation Iteration #3750:: 0.034293726086616516 Acc: 1.0
Validation Iteration #3800:: 0.3424120545387268 Acc: 0.875
Validation Iteration #3850:: 0.015843302011489868 Acc: 1.0
Epoch Validation:: Loss: 0.2626 Acc: 0.9496
Epoch 23/29
----------
Train Iteration #34950:: 0.002591439289972186 Acc: 1.0
Train Iteration #35000:: 0.05343921482563019 Acc: 1.0
Train Iteration #35050:: 0.0008337773033417761 Acc: 1.0
Train Iteration #35100:: 0.014571605250239372 Acc: 1.0
Train Iteration #35150:: 0.002633033785969019 Acc: 1.0
Train Iteration #35200:: 0.004121117293834686 Acc: 1.0
Train Iteration #35250:: 0.0023900181986391544 Acc: 1.0
Train Iteration #35300:: 0.032146524637937546 Acc: 1.0
Train Iteration #35350:: 0.00046297270455397666 Acc: 1.0
Train Iteration #35400:: 0.0395100936293602 Acc: 1.0
Train Iteration #35450:: 0.433786004781723 Acc: 0.875
Train Iteration #35500:: 0.10098998993635178 Acc: 1.0
Train Iteration #35550:: 0.4773906469345093 Acc: 0.875
Train Iteration #35600:: 0.011501546949148178 Acc: 1.0
Train Iteration #35650:: 0.06328094005584717 Acc: 1.0
Train Iteration #35700:: 0.010719029232859612 Acc: 1.0
Train Iteration #35750:: 0.006887560710310936 Acc: 1.0
Train Iteration #35800:: 0.01776178553700447 Acc: 1.0
Train Iteration #35850:: 0.006570377387106419 Acc: 1.0
Train Iteration #35900:: 0.4293653964996338 Acc: 0.875
Train Iteration #35950:: 0.03770417720079422 Acc: 1.0
Train Iteration #36000:: 0.07178697735071182 Acc: 1.0
Train Iteration #36050:: 0.0010435925796627998 Acc: 1.0
Train Iteration #36100:: 0.0798378437757492 Acc: 1.0
Train Iteration #36150:: 0.00040673409239389 Acc: 1.0
Train Iteration #36200:: 0.06049501150846481 Acc: 1.0
Train Iteration #36250:: 0.00032550489413551986 Acc: 1.0
Train Iteration #36300:: 0.3926694691181183 Acc: 0.875
Train Iteration #36350:: 0.0032571814954280853 Acc: 1.0
Train Iteration #36400:: 0.4009409546852112 Acc: 0.875
Epoch Training:: Loss: 0.0992 Acc: 0.9715
Validation Iteration #3900:: 0.029238486662507057 Acc: 1.0
Validation Iteration #3950:: 0.10389377921819687 Acc: 0.875
Validation Iteration #4000:: 0.6547223925590515 Acc: 0.875
Validation Iteration #4050:: 0.011476624757051468 Acc: 1.0
Epoch Validation:: Loss: 0.3077 Acc: 0.9451
Epoch 24/29
----------
Train Iteration #36450:: 0.08364465832710266 Acc: 1.0
Train Iteration #36500:: 0.01132965087890625 Acc: 1.0
Train Iteration #36550:: 0.0380871407687664 Acc: 1.0
Train Iteration #36600:: 0.0017585561145097017 Acc: 1.0
Train Iteration #36650:: 0.010156212374567986 Acc: 1.0
Train Iteration #36700:: 0.013318587094545364 Acc: 1.0
Train Iteration #36750:: 0.06425792723894119 Acc: 1.0
Train Iteration #36800:: 0.029321737587451935 Acc: 1.0
Train Iteration #36850:: 0.010231075808405876 Acc: 1.0
Train Iteration #36900:: 0.010393483564257622 Acc: 1.0
Train Iteration #36950:: 0.29619932174682617 Acc: 0.875
Train Iteration #37000:: 0.006935658864676952 Acc: 1.0
Train Iteration #37050:: 0.004734179005026817 Acc: 1.0
Train Iteration #37100:: 0.3226694166660309 Acc: 0.875
Train Iteration #37150:: 0.0966804176568985 Acc: 1.0
Train Iteration #37200:: 0.14799709618091583 Acc: 1.0
Train Iteration #37250:: 0.055918965488672256 Acc: 1.0
Train Iteration #37300:: 0.25791099667549133 Acc: 0.875
Train Iteration #37350:: 0.0102476691827178 Acc: 1.0
Train Iteration #37400:: 0.007635587360709906 Acc: 1.0
Train Iteration #37450:: 0.016390956938266754 Acc: 1.0
Train Iteration #37500:: 0.7088497281074524 Acc: 0.875
Train Iteration #37550:: 0.05581307038664818 Acc: 1.0
Train Iteration #37600:: 0.01825515739619732 Acc: 1.0
Train Iteration #37650:: 0.000338571349857375 Acc: 1.0
Train Iteration #37700:: 0.0275571346282959 Acc: 1.0
Train Iteration #37750:: 0.003776343073695898 Acc: 1.0
Train Iteration #37800:: 0.006469123065471649 Acc: 1.0
Train Iteration #37850:: 0.18887294828891754 Acc: 0.875
Train Iteration #37900:: 0.003433148842304945 Acc: 1.0
Epoch Training:: Loss: 0.1060 Acc: 0.9713
Validation Iteration #4100:: 0.030098501592874527 Acc: 1.0
Validation Iteration #4150:: 0.9889006614685059 Acc: 0.625
Validation Iteration #4200:: 0.05951381474733353 Acc: 1.0
Epoch Validation:: Loss: 0.3024 Acc: 0.9296
Epoch 25/29
----------
Train Iteration #37950:: 0.002318057930096984 Acc: 1.0
Train Iteration #38000:: 0.11146021634340286 Acc: 0.875
Train Iteration #38050:: 0.021272584795951843 Acc: 1.0
Train Iteration #38100:: 0.0038636960089206696 Acc: 1.0
Train Iteration #38150:: 0.0026939562521874905 Acc: 1.0
Train Iteration #38200:: 0.16020816564559937 Acc: 0.875
Train Iteration #38250:: 0.014224491082131863 Acc: 1.0
Train Iteration #38300:: 0.0370611734688282 Acc: 1.0
Train Iteration #38350:: 0.4698082506656647 Acc: 0.875
Train Iteration #38400:: 0.034024227410554886 Acc: 1.0
Train Iteration #38450:: 0.09534366428852081 Acc: 1.0
Train Iteration #38500:: 0.001531605259515345 Acc: 1.0
Train Iteration #38550:: 0.010019542649388313 Acc: 1.0
Train Iteration #38600:: 0.020511658862233162 Acc: 1.0
Train Iteration #38650:: 0.006764596328139305 Acc: 1.0
Train Iteration #38700:: 0.009421476162970066 Acc: 1.0
Train Iteration #38750:: 0.08748088777065277 Acc: 1.0
Train Iteration #38800:: 0.008334946818649769 Acc: 1.0
Train Iteration #38850:: 0.000452014384791255 Acc: 1.0
Train Iteration #38900:: 0.23511339724063873 Acc: 0.875
Train Iteration #38950:: 0.0025598418433219194 Acc: 1.0
Train Iteration #39000:: 0.06745851039886475 Acc: 1.0
Train Iteration #39050:: 0.028483796864748 Acc: 1.0
Train Iteration #39100:: 0.0002860509848687798 Acc: 1.0
Train Iteration #39150:: 0.07145315408706665 Acc: 1.0
Train Iteration #39200:: 0.6976229548454285 Acc: 0.875
Train Iteration #39250:: 0.044104333966970444 Acc: 1.0
Train Iteration #39300:: 0.0019539196509867907 Acc: 1.0
Train Iteration #39350:: 0.052282899618148804 Acc: 1.0
Train Iteration #39400:: 0.012299049645662308 Acc: 1.0
Train Iteration #39450:: 0.026091555133461952 Acc: 1.0
Epoch Training:: Loss: 0.0864 Acc: 0.9758
Validation Iteration #4250:: 0.002826239448040724 Acc: 1.0
Validation Iteration #4300:: 0.0027667414397001266 Acc: 1.0
Validation Iteration #4350:: 0.013204849325120449 Acc: 1.0
Epoch Validation:: Loss: 0.2823 Acc: 0.9385
Epoch 26/29
----------
Train Iteration #39500:: 0.06898026168346405 Acc: 1.0
Train Iteration #39550:: 0.1642601490020752 Acc: 0.875
Train Iteration #39600:: 0.13363957405090332 Acc: 0.875
Train Iteration #39650:: 0.03505580872297287 Acc: 1.0
Train Iteration #39700:: 0.06704054027795792 Acc: 1.0
Train Iteration #39750:: 0.004528546240180731 Acc: 1.0
Train Iteration #39800:: 0.0014901063404977322 Acc: 1.0
Train Iteration #39850:: 0.006288532167673111 Acc: 1.0
Train Iteration #39900:: 0.004134833812713623 Acc: 1.0
Train Iteration #39950:: 0.016922565177083015 Acc: 1.0
Train Iteration #40000:: 0.0403839647769928 Acc: 1.0
Train Iteration #40050:: 0.016792431473731995 Acc: 1.0
Train Iteration #40100:: 0.04258166253566742 Acc: 1.0
Train Iteration #40150:: 0.4865161180496216 Acc: 0.875
Train Iteration #40200:: 0.006566646974533796 Acc: 1.0
Train Iteration #40250:: 0.06349558383226395 Acc: 1.0
Train Iteration #40300:: 0.29184895753860474 Acc: 0.875
Train Iteration #40350:: 0.007457807660102844 Acc: 1.0
Train Iteration #40400:: 0.02270706184208393 Acc: 1.0
Train Iteration #40450:: 0.032080527395009995 Acc: 1.0
Train Iteration #40500:: 0.04131684452295303 Acc: 1.0
Train Iteration #40550:: 0.0008256348082795739 Acc: 1.0
Train Iteration #40600:: 0.0025353501550853252 Acc: 1.0
Train Iteration #40650:: 0.014131316915154457 Acc: 1.0
Train Iteration #40700:: 0.2912139296531677 Acc: 0.875
Train Iteration #40750:: 0.00032587957684881985 Acc: 1.0
Train Iteration #40800:: 0.04766895994544029 Acc: 1.0
Train Iteration #40850:: 0.0008163265883922577 Acc: 1.0
Train Iteration #40900:: 0.09251539409160614 Acc: 1.0
Train Iteration #40950:: 0.3929382264614105 Acc: 0.875
Epoch Training:: Loss: 0.0958 Acc: 0.9727
Validation Iteration #4400:: 0.7875906229019165 Acc: 0.875
Validation Iteration #4450:: 0.39530014991760254 Acc: 0.75
Validation Iteration #4500:: 0.5252036452293396 Acc: 0.875
Validation Iteration #4550:: 0.535903811454773 Acc: 0.875
Epoch Validation:: Loss: 0.5208 Acc: 0.8814
Epoch 27/29
----------
Train Iteration #41000:: 0.0015015207463875413 Acc: 1.0
Train Iteration #41050:: 0.010503891855478287 Acc: 1.0
Train Iteration #41100:: 0.045754313468933105 Acc: 1.0
Train Iteration #41150:: 0.028780464082956314 Acc: 1.0
Train Iteration #41200:: 0.10773496329784393 Acc: 0.875
Train Iteration #41250:: 0.0035580373369157314 Acc: 1.0
Train Iteration #41300:: 0.0036073578521609306 Acc: 1.0
Train Iteration #41350:: 0.03832237422466278 Acc: 1.0
Train Iteration #41400:: 0.007146545220166445 Acc: 1.0
Train Iteration #41450:: 0.000141773751238361 Acc: 1.0
Train Iteration #41500:: 0.011709650047123432 Acc: 1.0
Train Iteration #41550:: 0.09770771116018295 Acc: 1.0
Train Iteration #41600:: 0.011108906008303165 Acc: 1.0
Train Iteration #41650:: 0.19573672115802765 Acc: 0.875
Train Iteration #41700:: 0.027601538226008415 Acc: 1.0
Train Iteration #41750:: 0.009165517054498196 Acc: 1.0
Train Iteration #41800:: 0.003347644116729498 Acc: 1.0
Train Iteration #41850:: 0.0006399046396836638 Acc: 1.0
Train Iteration #41900:: 0.010983459651470184 Acc: 1.0
Train Iteration #41950:: 0.0002509277546778321 Acc: 1.0
Train Iteration #42000:: 0.0021591971162706614 Acc: 1.0
Train Iteration #42050:: 0.009752772748470306 Acc: 1.0
Train Iteration #42100:: 0.005090011283755302 Acc: 1.0
Train Iteration #42150:: 0.04514670744538307 Acc: 1.0
Train Iteration #42200:: 0.0015227808617055416 Acc: 1.0
Train Iteration #42250:: 0.03544372320175171 Acc: 1.0
Train Iteration #42300:: 0.0004851519479416311 Acc: 1.0
Train Iteration #42350:: 0.027906861156225204 Acc: 1.0
Train Iteration #42400:: 0.0024080274160951376 Acc: 1.0
Train Iteration #42450:: 0.013916067779064178 Acc: 1.0
Train Iteration #42500:: 0.02038455754518509 Acc: 1.0
Epoch Training:: Loss: 0.0836 Acc: 0.9778
Validation Iteration #4600:: 0.0006650291034020483 Acc: 1.0
Validation Iteration #4650:: 0.0005929237231612206 Acc: 1.0
Validation Iteration #4700:: 0.1710890829563141 Acc: 1.0
Epoch Validation:: Loss: 0.2946 Acc: 0.9437
Epoch 28/29
----------
Train Iteration #42550:: 0.0012587583623826504 Acc: 1.0
Train Iteration #42600:: 0.005291972309350967 Acc: 1.0
Train Iteration #42650:: 0.06703546643257141 Acc: 1.0
Train Iteration #42700:: 0.06873123347759247 Acc: 1.0
Train Iteration #42750:: 0.007408516947180033 Acc: 1.0
Train Iteration #42800:: 0.03310514986515045 Acc: 1.0
Train Iteration #42850:: 0.028670554980635643 Acc: 1.0
Train Iteration #42900:: 0.0009349549654871225 Acc: 1.0
Train Iteration #42950:: 0.059404004365205765 Acc: 1.0
Train Iteration #43000:: 0.004916607402265072 Acc: 1.0
Train Iteration #43050:: 0.017615457996726036 Acc: 1.0
Train Iteration #43100:: 8.276708831544966e-05 Acc: 1.0
Train Iteration #43150:: 0.009433001279830933 Acc: 1.0
Train Iteration #43200:: 0.00974295660853386 Acc: 1.0
Train Iteration #43250:: 0.39740580320358276 Acc: 0.875
Train Iteration #43300:: 0.021437611430883408 Acc: 1.0
Train Iteration #43350:: 0.003958229441195726 Acc: 1.0
Train Iteration #43400:: 0.03374175727367401 Acc: 1.0
Train Iteration #43450:: 0.009610582143068314 Acc: 1.0
Train Iteration #43500:: 0.015150712803006172 Acc: 1.0
Train Iteration #43550:: 0.0011742271017283201 Acc: 1.0
Train Iteration #43600:: 0.0012223685625940561 Acc: 1.0
Train Iteration #43650:: 0.003678438952192664 Acc: 1.0
Train Iteration #43700:: 0.001245275605469942 Acc: 1.0
Train Iteration #43750:: 0.37928855419158936 Acc: 0.875
Train Iteration #43800:: 0.010151386260986328 Acc: 1.0
Train Iteration #43850:: 0.030670685693621635 Acc: 1.0
Train Iteration #43900:: 0.3260648250579834 Acc: 0.875
Train Iteration #43950:: 0.2052420824766159 Acc: 0.875
Train Iteration #44000:: 0.05518525093793869 Acc: 1.0
Epoch Training:: Loss: 0.0908 Acc: 0.9755
Validation Iteration #4750:: 0.017874835059046745 Acc: 1.0
Validation Iteration #4800:: 3.907391146640293e-05 Acc: 1.0
Validation Iteration #4850:: 0.15519534051418304 Acc: 1.0
Validation Iteration #4900:: 0.09428160637617111 Acc: 1.0
Epoch Validation:: Loss: 0.2619 Acc: 0.9400
Epoch 29/29
----------
Train Iteration #44050:: 0.004439771641045809 Acc: 1.0
Train Iteration #44100:: 0.48580917716026306 Acc: 0.875
Train Iteration #44150:: 0.05868889391422272 Acc: 1.0
Train Iteration #44200:: 0.09304602444171906 Acc: 1.0
Train Iteration #44250:: 0.036665596067905426 Acc: 1.0
Train Iteration #44300:: 0.004127427004277706 Acc: 1.0
Train Iteration #44350:: 0.0002480244147591293 Acc: 1.0
Train Iteration #44400:: 0.001766757108271122 Acc: 1.0
Train Iteration #44450:: 0.021262437105178833 Acc: 1.0
Train Iteration #44500:: 0.3922654986381531 Acc: 0.75
Train Iteration #44550:: 0.13600735366344452 Acc: 1.0
Train Iteration #44600:: 0.010126493871212006 Acc: 1.0
Train Iteration #44650:: 0.0025045776274055243 Acc: 1.0
Train Iteration #44700:: 0.0015760869719088078 Acc: 1.0
Train Iteration #44750:: 0.17980463802814484 Acc: 0.875
Train Iteration #44800:: 0.6275065541267395 Acc: 0.75
Train Iteration #44850:: 0.011694681830704212 Acc: 1.0
Train Iteration #44900:: 0.0009246138506568968 Acc: 1.0
Train Iteration #44950:: 0.06014084815979004 Acc: 1.0
Train Iteration #45000:: 0.0001226858439622447 Acc: 1.0
Train Iteration #45050:: 0.2753024399280548 Acc: 0.875
Train Iteration #45100:: 0.02866024523973465 Acc: 1.0
Train Iteration #45150:: 0.0006181919598020613 Acc: 1.0
Train Iteration #45200:: 0.03915625438094139 Acc: 1.0
Train Iteration #45250:: 0.0028339028358459473 Acc: 1.0
Train Iteration #45300:: 0.022596530616283417 Acc: 1.0
Train Iteration #45350:: 0.022129524499177933 Acc: 1.0
Train Iteration #45400:: 0.00044233622611500323 Acc: 1.0
Train Iteration #45450:: 0.0004205210425425321 Acc: 1.0
Train Iteration #45500:: 0.0006822708528488874 Acc: 1.0
Epoch Training:: Loss: 0.0769 Acc: 0.9784
Validation Iteration #4950:: 0.011459631845355034 Acc: 1.0
Validation Iteration #5000:: 8.74860561452806e-05 Acc: 1.0
Validation Iteration #5050:: 0.018911391496658325 Acc: 1.0
Epoch Validation:: Loss: 0.2726 Acc: 0.9444
Lowest Validation Acc: 0.217873 at epoch:12
End time:7:14:36.136079
Program Complete
Average Train Loss:0.3433619183581619
Average Validation Loss:0.4490187043719953
Average Train Accuracy:0.9104878785382865
Average Validation Accuracy:0.8831974301952064
