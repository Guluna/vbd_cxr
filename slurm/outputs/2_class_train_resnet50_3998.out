cuda
Params to learn, when feature extract = True:
	 fc.weight
	 fc.bias
Program started
Epoch 0/49
----------
Train Iteration #0:: 1.3472938537597656 Acc: 0.6666666666666666
Train Iteration #50:: 0.9675344228744507 Acc: 0.7083333333333334
Train Iteration #100:: 1.1967082023620605 Acc: 0.7083333333333334
Train Iteration #150:: 0.49548131227493286 Acc: 0.7916666666666666
Train Iteration #200:: 0.5846490859985352 Acc: 0.8333333333333334
Train Iteration #250:: 0.8745526075363159 Acc: 0.75
Train Iteration #300:: 0.7295117378234863 Acc: 0.75
Train Iteration #350:: 0.6720289587974548 Acc: 0.7083333333333334
Train Iteration #400:: 0.8106891512870789 Acc: 0.7916666666666666
Train Iteration #450:: 0.5032002925872803 Acc: 0.7083333333333334
Train Iteration #500:: 2.2626404762268066 Acc: 0.625
Epoch Training:: Loss: 0.9059 Acc: 0.6943
Validation Iteration #0:: 1.964069128036499 Acc: 0.3333333333333333
Validation Iteration #50:: 1.9866894483566284 Acc: 0.4583333333333333
Epoch Validation:: Loss: 2.5288 Acc: 0.3395
Epoch 1/49
----------
Train Iteration #550:: 0.8291254043579102 Acc: 0.8333333333333334
Train Iteration #600:: 1.2280001640319824 Acc: 0.5833333333333334
Train Iteration #650:: 0.9046787023544312 Acc: 0.625
Train Iteration #700:: 1.9425535202026367 Acc: 0.6666666666666666
Train Iteration #750:: 0.9750266075134277 Acc: 0.5833333333333334
Train Iteration #800:: 0.5478218793869019 Acc: 0.9166666666666666
Train Iteration #850:: 1.7134603261947632 Acc: 0.5
Train Iteration #900:: 0.8267040252685547 Acc: 0.625
Train Iteration #950:: 0.6232380270957947 Acc: 0.875
Train Iteration #1000:: 0.8479921221733093 Acc: 0.6666666666666666
Epoch Training:: Loss: 0.9142 Acc: 0.7422
Validation Iteration #100:: 1.3129498958587646 Acc: 0.625
Epoch Validation:: Loss: 0.9725 Acc: 0.6286
Epoch 2/49
----------
Train Iteration #1050:: 2.3220815658569336 Acc: 0.4166666666666667
Train Iteration #1100:: 1.2333563566207886 Acc: 0.625
Train Iteration #1150:: 0.6811751127243042 Acc: 0.6666666666666666
Train Iteration #1200:: 0.30259206891059875 Acc: 0.8333333333333334
Train Iteration #1250:: 0.6299893856048584 Acc: 0.875
Train Iteration #1300:: 0.8955899477005005 Acc: 0.7083333333333334
Train Iteration #1350:: 0.30923932790756226 Acc: 0.9583333333333334
Train Iteration #1400:: 0.5761591792106628 Acc: 0.75
Train Iteration #1450:: 0.36462804675102234 Acc: 0.875
Train Iteration #1500:: 0.6076955795288086 Acc: 0.7916666666666666
Epoch Training:: Loss: 0.7731 Acc: 0.7766
Validation Iteration #150:: 0.7070621252059937 Acc: 0.8333333333333334
Epoch Validation:: Loss: 0.4823 Acc: 0.8340
Epoch 3/49
----------
Train Iteration #1550:: 0.9066017866134644 Acc: 0.7083333333333334
Train Iteration #1600:: 0.6744551062583923 Acc: 0.7083333333333334
Train Iteration #1650:: 0.5257933139801025 Acc: 0.7916666666666666
Train Iteration #1700:: 0.4853336811065674 Acc: 0.7916666666666666
Train Iteration #1750:: 0.43192291259765625 Acc: 0.9166666666666666
Train Iteration #1800:: 0.5035110116004944 Acc: 0.75
Train Iteration #1850:: 0.6523323059082031 Acc: 0.7083333333333334
Train Iteration #1900:: 0.3491933047771454 Acc: 0.9166666666666666
Train Iteration #1950:: 0.40495944023132324 Acc: 0.9166666666666666
Train Iteration #2000:: 1.2126973867416382 Acc: 0.5
Epoch Training:: Loss: 0.7542 Acc: 0.7859
Validation Iteration #200:: 0.18807344138622284 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.4740 Acc: 0.8658
Epoch 4/49
----------
Train Iteration #2050:: 0.572795569896698 Acc: 0.8333333333333334
Train Iteration #2100:: 0.5287809371948242 Acc: 0.8333333333333334
Train Iteration #2150:: 0.3318135440349579 Acc: 0.875
Train Iteration #2200:: 0.2620397210121155 Acc: 1.0
Train Iteration #2250:: 0.16411148011684418 Acc: 0.9583333333333334
Train Iteration #2300:: 2.147496223449707 Acc: 0.375
Train Iteration #2350:: 0.6178208589553833 Acc: 0.7916666666666666
Train Iteration #2400:: 0.5702670812606812 Acc: 0.7916666666666666
Train Iteration #2450:: 0.8162226676940918 Acc: 0.75
Train Iteration #2500:: 0.7292066812515259 Acc: 0.625
Epoch Training:: Loss: 0.6892 Acc: 0.7928
Validation Iteration #250:: 0.47686684131622314 Acc: 0.9166666666666666
Epoch Validation:: Loss: 0.5579 Acc: 0.8836
Epoch 5/49
----------
Params to learn, when feature extract = False:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer2.3.conv1.weight
	 layer2.3.bn1.weight
	 layer2.3.bn1.bias
	 layer2.3.conv2.weight
	 layer2.3.bn2.weight
	 layer2.3.bn2.bias
	 layer2.3.conv3.weight
	 layer2.3.bn3.weight
	 layer2.3.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer3.3.conv1.weight
	 layer3.3.bn1.weight
	 layer3.3.bn1.bias
	 layer3.3.conv2.weight
	 layer3.3.bn2.weight
	 layer3.3.bn2.bias
	 layer3.3.conv3.weight
	 layer3.3.bn3.weight
	 layer3.3.bn3.bias
	 layer3.4.conv1.weight
	 layer3.4.bn1.weight
	 layer3.4.bn1.bias
	 layer3.4.conv2.weight
	 layer3.4.bn2.weight
	 layer3.4.bn2.bias
	 layer3.4.conv3.weight
	 layer3.4.bn3.weight
	 layer3.4.bn3.bias
	 layer3.5.conv1.weight
	 layer3.5.bn1.weight
	 layer3.5.bn1.bias
	 layer3.5.conv2.weight
	 layer3.5.bn2.weight
	 layer3.5.bn2.bias
	 layer3.5.conv3.weight
	 layer3.5.bn3.weight
	 layer3.5.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fc.weight
	 fc.bias
Train Iteration #2550:: 0.3002265393733978 Acc: 0.875
Train Iteration #2600:: 0.5044798851013184 Acc: 0.8333333333333334
Train Iteration #2650:: 0.7051143050193787 Acc: 0.75
Train Iteration #2700:: 0.7179834246635437 Acc: 0.7083333333333334
Train Iteration #2750:: 0.34310635924339294 Acc: 0.8333333333333334
Train Iteration #2800:: 0.2900218665599823 Acc: 0.9166666666666666
Train Iteration #2850:: 0.17963336408138275 Acc: 0.9166666666666666
Train Iteration #2900:: 0.1272347867488861 Acc: 1.0
Train Iteration #2950:: 0.47828423976898193 Acc: 0.7916666666666666
Train Iteration #3000:: 0.3696001470088959 Acc: 0.875
Epoch Training:: Loss: 0.4594 Acc: 0.8695
Validation Iteration #300:: 0.14574840664863586 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.5105 Acc: 0.8347
Epoch 6/49
----------
Train Iteration #3050:: 0.1393412947654724 Acc: 1.0
Train Iteration #3100:: 0.09135985374450684 Acc: 0.9583333333333334
Train Iteration #3150:: 0.26458126306533813 Acc: 0.9166666666666666
Train Iteration #3200:: 0.17888349294662476 Acc: 0.9583333333333334
Train Iteration #3250:: 0.2238537073135376 Acc: 0.9166666666666666
Train Iteration #3300:: 0.5113315582275391 Acc: 0.7916666666666666
Train Iteration #3350:: 0.1993914097547531 Acc: 0.9166666666666666
Train Iteration #3400:: 0.2882309556007385 Acc: 0.9166666666666666
Train Iteration #3450:: 0.16834506392478943 Acc: 0.9583333333333334
Train Iteration #3500:: 0.18450602889060974 Acc: 0.9583333333333334
Epoch Training:: Loss: 0.2932 Acc: 0.9130
Validation Iteration #350:: 0.4778263568878174 Acc: 0.7916666666666666
Epoch Validation:: Loss: 0.3143 Acc: 0.8858
Epoch 7/49
----------
Train Iteration #3550:: 0.3881811797618866 Acc: 0.8333333333333334
Train Iteration #3600:: 0.2888464331626892 Acc: 0.9583333333333334
Train Iteration #3650:: 0.14810697734355927 Acc: 0.9166666666666666
Train Iteration #3700:: 0.7167178988456726 Acc: 0.7916666666666666
Train Iteration #3750:: 0.09815939515829086 Acc: 0.9583333333333334
Train Iteration #3800:: 0.7191309332847595 Acc: 0.8333333333333334
Train Iteration #3850:: 0.1611335724592209 Acc: 0.9583333333333334
Train Iteration #3900:: 0.3311018943786621 Acc: 0.875
Train Iteration #3950:: 0.16549496352672577 Acc: 1.0
Train Iteration #4000:: 0.3227255940437317 Acc: 0.875
Epoch Training:: Loss: 0.2375 Acc: 0.9300
Validation Iteration #400:: 0.7366821765899658 Acc: 0.7916666666666666
Validation Iteration #450:: 0.0997130274772644 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.3904 Acc: 0.8910
Epoch 8/49
----------
Train Iteration #4050:: 0.22509413957595825 Acc: 0.9583333333333334
Train Iteration #4100:: 0.1223769411444664 Acc: 0.9583333333333334
Train Iteration #4150:: 0.4968714118003845 Acc: 0.9166666666666666
Train Iteration #4200:: 0.43717047572135925 Acc: 0.875
Train Iteration #4250:: 0.24329841136932373 Acc: 0.9166666666666666
Train Iteration #4300:: 0.14752405881881714 Acc: 0.9583333333333334
Train Iteration #4350:: 0.1744326800107956 Acc: 0.9583333333333334
Train Iteration #4400:: 0.24506251513957977 Acc: 0.9166666666666666
Train Iteration #4450:: 0.04320458695292473 Acc: 1.0
Train Iteration #4500:: 0.02560780569911003 Acc: 1.0
Train Iteration #4550:: 0.041561417281627655 Acc: 1.0
Epoch Training:: Loss: 0.2079 Acc: 0.9386
Validation Iteration #500:: 0.037436433136463165 Acc: 1.0
Epoch Validation:: Loss: 0.2997 Acc: 0.9096
Epoch 9/49
----------
Train Iteration #4600:: 0.09458273649215698 Acc: 0.9583333333333334
Train Iteration #4650:: 0.37567493319511414 Acc: 0.8333333333333334
Train Iteration #4700:: 0.36900055408477783 Acc: 0.9166666666666666
Train Iteration #4750:: 0.15001870691776276 Acc: 0.9583333333333334
Train Iteration #4800:: 0.2930176258087158 Acc: 0.9166666666666666
Train Iteration #4850:: 0.21566066145896912 Acc: 0.9583333333333334
Train Iteration #4900:: 0.013825732283294201 Acc: 1.0
Train Iteration #4950:: 0.06952120363712311 Acc: 1.0
Train Iteration #5000:: 0.2022777795791626 Acc: 1.0
Train Iteration #5050:: 0.20235256850719452 Acc: 0.9583333333333334
Epoch Training:: Loss: 0.2016 Acc: 0.9428
Validation Iteration #550:: 0.10105141997337341 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.2516 Acc: 0.9251
Epoch 10/49
----------
Train Iteration #5100:: 0.012832841835916042 Acc: 1.0
Train Iteration #5150:: 0.20795589685440063 Acc: 0.9583333333333334
Train Iteration #5200:: 0.30065757036209106 Acc: 0.9583333333333334
Train Iteration #5250:: 0.19450746476650238 Acc: 0.9583333333333334
Train Iteration #5300:: 0.14986425638198853 Acc: 0.9583333333333334
Train Iteration #5350:: 0.2838720679283142 Acc: 0.875
Train Iteration #5400:: 0.10655437409877777 Acc: 0.9583333333333334
Train Iteration #5450:: 0.09751778095960617 Acc: 0.9583333333333334
Train Iteration #5500:: 0.21460120379924774 Acc: 0.9166666666666666
Train Iteration #5550:: 0.03725177049636841 Acc: 1.0
Epoch Training:: Loss: 0.1652 Acc: 0.9530
Validation Iteration #600:: 0.08636821806430817 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.2581 Acc: 0.9437
Epoch 11/49
----------
Train Iteration #5600:: 0.29928475618362427 Acc: 0.9583333333333334
Train Iteration #5650:: 0.15050455927848816 Acc: 0.9583333333333334
Train Iteration #5700:: 0.11510857194662094 Acc: 1.0
Train Iteration #5750:: 0.07055400311946869 Acc: 1.0
Train Iteration #5800:: 0.2118431180715561 Acc: 0.9583333333333334
Train Iteration #5850:: 0.1810922473669052 Acc: 0.9583333333333334
Train Iteration #5900:: 0.03718908131122589 Acc: 1.0
Train Iteration #5950:: 0.024103384464979172 Acc: 1.0
Train Iteration #6000:: 0.17582188546657562 Acc: 0.9583333333333334
Train Iteration #6050:: 0.10966943204402924 Acc: 0.9583333333333334
Epoch Training:: Loss: 0.1443 Acc: 0.9596
Validation Iteration #650:: 0.316760778427124 Acc: 0.9166666666666666
Epoch Validation:: Loss: 0.2971 Acc: 0.9096
Epoch 12/49
----------
Train Iteration #6100:: 0.15289101004600525 Acc: 0.9583333333333334
Train Iteration #6150:: 0.1824512481689453 Acc: 0.9583333333333334
Train Iteration #6200:: 0.005819501355290413 Acc: 1.0
Train Iteration #6250:: 0.522790789604187 Acc: 0.75
Train Iteration #6300:: 0.13195055723190308 Acc: 0.9166666666666666
Train Iteration #6350:: 0.20986971259117126 Acc: 0.9166666666666666
Train Iteration #6400:: 0.06867671012878418 Acc: 1.0
Train Iteration #6450:: 0.256653368473053 Acc: 0.9166666666666666
Train Iteration #6500:: 0.21150116622447968 Acc: 0.875
Train Iteration #6550:: 0.010083518922328949 Acc: 1.0
Epoch Training:: Loss: 0.1360 Acc: 0.9610
Validation Iteration #700:: 0.08717229962348938 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.2539 Acc: 0.9333
Epoch 13/49
----------
Train Iteration #6600:: 1.0971883535385132 Acc: 0.8333333333333334
Train Iteration #6650:: 0.2926599979400635 Acc: 0.9166666666666666
Train Iteration #6700:: 0.026302814483642578 Acc: 1.0
Train Iteration #6750:: 0.014224263839423656 Acc: 1.0
Train Iteration #6800:: 0.03553155064582825 Acc: 1.0
Train Iteration #6850:: 0.14653712511062622 Acc: 0.9166666666666666
Train Iteration #6900:: 0.023740243166685104 Acc: 1.0
Train Iteration #6950:: 0.031365253031253815 Acc: 1.0
Train Iteration #7000:: 0.03588243946433067 Acc: 1.0
Train Iteration #7050:: 0.17438369989395142 Acc: 0.875
Epoch Training:: Loss: 0.1416 Acc: 0.9610
Validation Iteration #750:: 0.24117445945739746 Acc: 0.9166666666666666
Epoch Validation:: Loss: 0.3811 Acc: 0.8910
Epoch 14/49
----------
Train Iteration #7100:: 0.015871945768594742 Acc: 1.0
Train Iteration #7150:: 0.17360559105873108 Acc: 0.9166666666666666
Train Iteration #7200:: 0.05764440819621086 Acc: 1.0
Train Iteration #7250:: 0.13229158520698547 Acc: 0.9583333333333334
Train Iteration #7300:: 0.07315792888402939 Acc: 1.0
Train Iteration #7350:: 0.05193310230970383 Acc: 1.0
Train Iteration #7400:: 0.03526776656508446 Acc: 1.0
Train Iteration #7450:: 0.23013821244239807 Acc: 0.9583333333333334
Train Iteration #7500:: 0.07408352941274643 Acc: 0.9583333333333334
Train Iteration #7550:: 0.2730521261692047 Acc: 0.875
Epoch Training:: Loss: 0.1319 Acc: 0.9634
Validation Iteration #800:: 0.2555074989795685 Acc: 0.9166666666666666
Validation Iteration #850:: 0.043716639280319214 Acc: 1.0
Epoch Validation:: Loss: 0.2631 Acc: 0.9244
Epoch 15/49
----------
Train Iteration #7600:: 0.16301901638507843 Acc: 0.9583333333333334
Train Iteration #7650:: 0.12660564482212067 Acc: 0.9583333333333334
Train Iteration #7700:: 0.010275285691022873 Acc: 1.0
Train Iteration #7750:: 0.027777772396802902 Acc: 1.0
Train Iteration #7800:: 0.34985217452049255 Acc: 0.9166666666666666
Train Iteration #7850:: 0.07133053243160248 Acc: 1.0
Train Iteration #7900:: 0.0075541529804468155 Acc: 1.0
Train Iteration #7950:: 0.022404087707400322 Acc: 1.0
Train Iteration #8000:: 0.23640000820159912 Acc: 0.9583333333333334
Train Iteration #8050:: 0.17379382252693176 Acc: 0.9166666666666666
Epoch Training:: Loss: 0.1285 Acc: 0.9657
Validation Iteration #900:: 0.027131691575050354 Acc: 1.0
Epoch Validation:: Loss: 0.2650 Acc: 0.9311
Epoch 16/49
----------
Train Iteration #8100:: 0.045924730598926544 Acc: 1.0
Train Iteration #8150:: 0.18810495734214783 Acc: 0.9166666666666666
Train Iteration #8200:: 0.21300306916236877 Acc: 0.9583333333333334
Train Iteration #8250:: 0.3589409291744232 Acc: 0.9166666666666666
Train Iteration #8300:: 0.04511868208646774 Acc: 1.0
Train Iteration #8350:: 0.010941090062260628 Acc: 1.0
Train Iteration #8400:: 0.020623445510864258 Acc: 1.0
Train Iteration #8450:: 0.011742930859327316 Acc: 1.0
Train Iteration #8500:: 0.06774367392063141 Acc: 1.0
Train Iteration #8550:: 0.09771066159009933 Acc: 0.9583333333333334
Train Iteration #8600:: 0.022810157388448715 Acc: 1.0
Epoch Training:: Loss: 0.1149 Acc: 0.9680
Validation Iteration #950:: 0.2026958465576172 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.3337 Acc: 0.9066
Epoch 17/49
----------
Train Iteration #8650:: 0.0759735256433487 Acc: 1.0
Train Iteration #8700:: 0.27888888120651245 Acc: 0.9166666666666666
Train Iteration #8750:: 0.025472966954112053 Acc: 1.0
Train Iteration #8800:: 0.007712703198194504 Acc: 1.0
Train Iteration #8850:: 0.09160417318344116 Acc: 0.9583333333333334
Train Iteration #8900:: 0.005839229561388493 Acc: 1.0
Train Iteration #8950:: 0.7258274555206299 Acc: 0.9166666666666666
Train Iteration #9000:: 0.18108731508255005 Acc: 0.9583333333333334
Train Iteration #9050:: 0.16552302241325378 Acc: 0.9583333333333334
Train Iteration #9100:: 0.009038150310516357 Acc: 1.0
Epoch Training:: Loss: 0.1001 Acc: 0.9736
Validation Iteration #1000:: 0.6415780186653137 Acc: 0.8333333333333334
Epoch Validation:: Loss: 0.4166 Acc: 0.8829
Epoch 18/49
----------
Train Iteration #9150:: 0.08955904096364975 Acc: 0.9583333333333334
Train Iteration #9200:: 0.023133961483836174 Acc: 1.0
Train Iteration #9250:: 0.2441704273223877 Acc: 0.9583333333333334
Train Iteration #9300:: 0.0458686463534832 Acc: 1.0
Train Iteration #9350:: 0.033401817083358765 Acc: 1.0
Train Iteration #9400:: 0.15226279199123383 Acc: 0.9583333333333334
Train Iteration #9450:: 0.13651926815509796 Acc: 0.9583333333333334
Train Iteration #9500:: 0.32523101568222046 Acc: 0.875
Train Iteration #9550:: 0.16413173079490662 Acc: 0.9166666666666666
Train Iteration #9600:: 0.037830155342817307 Acc: 1.0
Epoch Training:: Loss: 0.1090 Acc: 0.9703
Validation Iteration #1050:: 0.1352044641971588 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.3522 Acc: 0.9466
Epoch 19/49
----------
Train Iteration #9650:: 0.05501024052500725 Acc: 0.9583333333333334
Train Iteration #9700:: 0.0050134202465415 Acc: 1.0
Train Iteration #9750:: 0.2640010714530945 Acc: 0.9166666666666666
Train Iteration #9800:: 0.02769964560866356 Acc: 1.0
Train Iteration #9850:: 0.11662587523460388 Acc: 0.9583333333333334
Train Iteration #9900:: 0.0665598139166832 Acc: 0.9583333333333334
Train Iteration #9950:: 0.12702976167201996 Acc: 0.9583333333333334
Train Iteration #10000:: 0.128586545586586 Acc: 0.9583333333333334
Train Iteration #10050:: 0.1804705709218979 Acc: 0.875
Train Iteration #10100:: 0.08406943082809448 Acc: 0.9583333333333334
Epoch Training:: Loss: 0.0948 Acc: 0.9727
Validation Iteration #1100:: 0.45036137104034424 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.3447 Acc: 0.9459
Epoch 20/49
----------
Train Iteration #10150:: 0.001378802815452218 Acc: 1.0
Train Iteration #10200:: 0.08060168474912643 Acc: 0.9583333333333334
Train Iteration #10250:: 0.1618289202451706 Acc: 0.9166666666666666
Train Iteration #10300:: 0.007310710847377777 Acc: 1.0
Train Iteration #10350:: 0.051166728138923645 Acc: 1.0
Train Iteration #10400:: 0.10818406194448471 Acc: 0.9583333333333334
Train Iteration #10450:: 0.1006685197353363 Acc: 0.9583333333333334
Train Iteration #10500:: 0.042701348662376404 Acc: 1.0
Train Iteration #10550:: 0.01848020777106285 Acc: 1.0
Train Iteration #10600:: 0.06789220869541168 Acc: 0.9583333333333334
Epoch Training:: Loss: 0.0977 Acc: 0.9736
Validation Iteration #1150:: 0.027800586074590683 Acc: 1.0
Epoch Validation:: Loss: 0.2723 Acc: 0.9518
Epoch 21/49
----------
Train Iteration #10650:: 0.07245682179927826 Acc: 1.0
Train Iteration #10700:: 0.2949942648410797 Acc: 0.9166666666666666
Train Iteration #10750:: 0.23788467049598694 Acc: 0.9583333333333334
Train Iteration #10800:: 0.0069788722321391106 Acc: 1.0
Train Iteration #10850:: 0.002553334692493081 Acc: 1.0
Train Iteration #10900:: 0.011180928908288479 Acc: 1.0
Train Iteration #10950:: 0.12070846557617188 Acc: 0.9583333333333334
Train Iteration #11000:: 0.2385927438735962 Acc: 0.9166666666666666
Train Iteration #11050:: 0.03217751532793045 Acc: 1.0
Train Iteration #11100:: 0.03777123987674713 Acc: 1.0
Epoch Training:: Loss: 0.0831 Acc: 0.9771
Validation Iteration #1200:: 0.35780760645866394 Acc: 0.9583333333333334
Validation Iteration #1250:: 0.004214247688651085 Acc: 1.0
Epoch Validation:: Loss: 0.3337 Acc: 0.9466
Epoch 22/49
----------
Train Iteration #11150:: 0.0800447165966034 Acc: 0.9583333333333334
Train Iteration #11200:: 0.04336816817522049 Acc: 1.0
Train Iteration #11250:: 0.006148712243884802 Acc: 1.0
Train Iteration #11300:: 0.1923193335533142 Acc: 0.9583333333333334
Train Iteration #11350:: 0.03749062120914459 Acc: 1.0
Train Iteration #11400:: 0.0267399400472641 Acc: 1.0
Train Iteration #11450:: 0.024449121206998825 Acc: 1.0
Train Iteration #11500:: 0.1708187311887741 Acc: 0.9583333333333334
Train Iteration #11550:: 0.010691152885556221 Acc: 1.0
Train Iteration #11600:: 0.11231239885091782 Acc: 0.9583333333333334
Epoch Training:: Loss: 0.0917 Acc: 0.9764
Validation Iteration #1300:: 0.155775249004364 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.2625 Acc: 0.9511
Epoch 23/49
----------
Train Iteration #11650:: 0.048573072999715805 Acc: 0.9583333333333334
Train Iteration #11700:: 0.012744634412229061 Acc: 1.0
Train Iteration #11750:: 0.016565974801778793 Acc: 1.0
Train Iteration #11800:: 0.08104784786701202 Acc: 0.9583333333333334
Train Iteration #11850:: 0.12097783386707306 Acc: 0.9583333333333334
Train Iteration #11900:: 0.03591669723391533 Acc: 1.0
Train Iteration #11950:: 0.015616342425346375 Acc: 1.0
Train Iteration #12000:: 0.030855907127261162 Acc: 1.0
Train Iteration #12050:: 0.15925928950309753 Acc: 0.9166666666666666
Train Iteration #12100:: 0.10050924122333527 Acc: 0.9583333333333334
Epoch Training:: Loss: 0.0847 Acc: 0.9769
Validation Iteration #1350:: 0.05405965447425842 Acc: 1.0
Epoch Validation:: Loss: 0.3323 Acc: 0.9407
Epoch 24/49
----------
Train Iteration #12150:: 0.041313838213682175 Acc: 1.0
Train Iteration #12200:: 0.21410731971263885 Acc: 0.9583333333333334
Train Iteration #12250:: 0.02081872709095478 Acc: 1.0
Train Iteration #12300:: 0.010126851499080658 Acc: 1.0
Train Iteration #12350:: 0.005557382013648748 Acc: 1.0
Train Iteration #12400:: 0.04047390818595886 Acc: 1.0
Train Iteration #12450:: 0.02620955929160118 Acc: 1.0
Train Iteration #12500:: 0.18533316254615784 Acc: 0.9583333333333334
Train Iteration #12550:: 0.011405154131352901 Acc: 1.0
Train Iteration #12600:: 0.000899003236554563 Acc: 1.0
Epoch Training:: Loss: 0.0681 Acc: 0.9820
Validation Iteration #1400:: 1.0872774124145508 Acc: 0.875
Epoch Validation:: Loss: 0.3068 Acc: 0.9355
Epoch 25/49
----------
Train Iteration #12650:: 0.2566186189651489 Acc: 0.9166666666666666
Train Iteration #12700:: 0.13360828161239624 Acc: 0.9583333333333334
Train Iteration #12750:: 0.020591242238879204 Acc: 1.0
Train Iteration #12800:: 0.038801684975624084 Acc: 1.0
Train Iteration #12850:: 0.03474007174372673 Acc: 1.0
Train Iteration #12900:: 0.3485657572746277 Acc: 0.9166666666666666
Train Iteration #12950:: 0.00882447324693203 Acc: 1.0
Train Iteration #13000:: 0.2197086066007614 Acc: 0.875
Train Iteration #13050:: 0.1271018236875534 Acc: 0.9583333333333334
Train Iteration #13100:: 0.0006798831163905561 Acc: 1.0
Train Iteration #13150:: 0.009029034525156021 Acc: 1.0
Epoch Training:: Loss: 0.0719 Acc: 0.9817
Validation Iteration #1450:: 0.5968479514122009 Acc: 0.9166666666666666
Epoch Validation:: Loss: 0.3064 Acc: 0.9392
Epoch 26/49
----------
Train Iteration #13200:: 0.0276481993496418 Acc: 1.0
Train Iteration #13250:: 0.0037868234794586897 Acc: 1.0
Train Iteration #13300:: 0.022700538858771324 Acc: 1.0
Train Iteration #13350:: 0.09707248210906982 Acc: 0.9583333333333334
Train Iteration #13400:: 0.2861602306365967 Acc: 0.9166666666666666
Train Iteration #13450:: 0.01513562723994255 Acc: 1.0
Train Iteration #13500:: 0.2722271680831909 Acc: 0.9166666666666666
Train Iteration #13550:: 0.020045245066285133 Acc: 1.0
Train Iteration #13600:: 0.002696756972000003 Acc: 1.0
Train Iteration #13650:: 0.05964529514312744 Acc: 1.0
Epoch Training:: Loss: 0.0766 Acc: 0.9790
Validation Iteration #1500:: 0.32717618346214294 Acc: 0.9166666666666666
Epoch Validation:: Loss: 0.2496 Acc: 0.9400
Epoch 27/49
----------
Train Iteration #13700:: 0.18304871022701263 Acc: 0.9583333333333334
Train Iteration #13750:: 0.01986960507929325 Acc: 1.0
Train Iteration #13800:: 0.004978743847459555 Acc: 1.0
Train Iteration #13850:: 0.01313853170722723 Acc: 1.0
Train Iteration #13900:: 0.018650420010089874 Acc: 1.0
Train Iteration #13950:: 0.012068495154380798 Acc: 1.0
Train Iteration #14000:: 0.07059076428413391 Acc: 1.0
Train Iteration #14050:: 0.004896454978734255 Acc: 1.0
Train Iteration #14100:: 0.054444462060928345 Acc: 0.9583333333333334
Train Iteration #14150:: 0.003767784219235182 Acc: 1.0
Epoch Training:: Loss: 0.0669 Acc: 0.9825
Validation Iteration #1550:: 0.3646491765975952 Acc: 0.9166666666666666
Epoch Validation:: Loss: 0.3299 Acc: 0.9451
Epoch 28/49
----------
Train Iteration #14200:: 0.00330898049287498 Acc: 1.0
Train Iteration #14250:: 0.04693683236837387 Acc: 0.9583333333333334
Train Iteration #14300:: 0.01010306179523468 Acc: 1.0
Train Iteration #14350:: 0.007613047957420349 Acc: 1.0
Train Iteration #14400:: 0.11118350923061371 Acc: 0.9583333333333334
Train Iteration #14450:: 0.009266413748264313 Acc: 1.0
Train Iteration #14500:: 0.06854228675365448 Acc: 1.0
Train Iteration #14550:: 0.013125618919730186 Acc: 1.0
Train Iteration #14600:: 0.00416509248316288 Acc: 1.0
Train Iteration #14650:: 0.005434268619865179 Acc: 1.0
Epoch Training:: Loss: 0.0672 Acc: 0.9826
Validation Iteration #1600:: 0.004518723580986261 Acc: 1.0
Validation Iteration #1650:: 0.09964209049940109 Acc: 0.9583333333333334
Epoch Validation:: Loss: 0.2952 Acc: 0.9385
Epoch 29/49
----------
Train Iteration #14700:: 0.047610558569431305 Acc: 0.9583333333333334
Train Iteration #14750:: 0.013956593349575996 Acc: 1.0
Train Iteration #14800:: 0.0030134725384414196 Acc: 1.0
Train Iteration #14850:: 0.22692930698394775 Acc: 0.9166666666666666
Train Iteration #14900:: 0.16474765539169312 Acc: 0.9166666666666666
Train Iteration #14950:: 0.20127396285533905 Acc: 0.9583333333333334
Train Iteration #15000:: 0.01868879608809948 Acc: 1.0
Train Iteration #15050:: 0.003021612763404846 Acc: 1.0
Train Iteration #15100:: 0.09218089282512665 Acc: 0.9583333333333334
Train Iteration #15150:: 0.1267572045326233 Acc: 0.9166666666666666
Epoch Training:: Loss: 0.0467 Acc: 0.9880
Validation Iteration #1700:: 0.6939117312431335 Acc: 0.875
Epoch Validation:: Loss: 0.3158 Acc: 0.9466
Epoch 30/49
----------
