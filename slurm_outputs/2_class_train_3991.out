cuda
Params to learn, when feature extract = True:
	 fc.weight
	 fc.bias
Program started
Epoch 0/9
----------
Train Iteration #0:: 1.0630370378494263 Acc: 0.4166666666666667
Train Iteration #50:: 0.6383724808692932 Acc: 0.7083333333333334
Train Iteration #100:: 1.1528887748718262 Acc: 0.5
Train Iteration #150:: 0.8977739214897156 Acc: 0.5416666666666666
Train Iteration #200:: 0.5231468677520752 Acc: 0.8333333333333334
Train Iteration #250:: 0.6525250673294067 Acc: 0.7083333333333334
Train Iteration #300:: 0.38421630859375 Acc: 0.9166666666666666
Train Iteration #350:: 0.591924786567688 Acc: 0.7916666666666666
Train Iteration #400:: 0.7860585451126099 Acc: 0.7083333333333334
Train Iteration #450:: 1.8840103149414062 Acc: 0.5833333333333334
Train Iteration #500:: 0.8000925779342651 Acc: 0.7083333333333334
Epoch Training:: Loss: 0.9993 Acc: 0.6807
Validation Iteration #0:: 0.3577086329460144 Acc: 0.9166666666666666
Validation Iteration #50:: 0.7456981539726257 Acc: 0.75
Epoch Validation:: Loss: 0.5134 Acc: 0.8273
Epoch 1/9
----------
Params to learn, when feature extract = False:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer2.3.conv1.weight
	 layer2.3.bn1.weight
	 layer2.3.bn1.bias
	 layer2.3.conv2.weight
	 layer2.3.bn2.weight
	 layer2.3.bn2.bias
	 layer2.3.conv3.weight
	 layer2.3.bn3.weight
	 layer2.3.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer3.3.conv1.weight
	 layer3.3.bn1.weight
	 layer3.3.bn1.bias
	 layer3.3.conv2.weight
	 layer3.3.bn2.weight
	 layer3.3.bn2.bias
	 layer3.3.conv3.weight
	 layer3.3.bn3.weight
	 layer3.3.bn3.bias
	 layer3.4.conv1.weight
	 layer3.4.bn1.weight
	 layer3.4.bn1.bias
	 layer3.4.conv2.weight
	 layer3.4.bn2.weight
	 layer3.4.bn2.bias
	 layer3.4.conv3.weight
	 layer3.4.bn3.weight
	 layer3.4.bn3.bias
	 layer3.5.conv1.weight
	 layer3.5.bn1.weight
	 layer3.5.bn1.bias
	 layer3.5.conv2.weight
	 layer3.5.bn2.weight
	 layer3.5.bn2.bias
	 layer3.5.conv3.weight
	 layer3.5.bn3.weight
	 layer3.5.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fc.weight
	 fc.bias
Train Iteration #550:: 0.6053608655929565 Acc: 0.7083333333333334
Train Iteration #600:: 1.0753638744354248 Acc: 0.5416666666666666
Train Iteration #650:: 0.6160452961921692 Acc: 0.8333333333333334
Train Iteration #700:: 0.7170697450637817 Acc: 0.625
Train Iteration #750:: 0.5167869925498962 Acc: 0.8333333333333334
Train Iteration #800:: 0.6034755706787109 Acc: 0.7083333333333334
Train Iteration #850:: 0.16102413833141327 Acc: 0.9583333333333334
Train Iteration #900:: 0.4553871154785156 Acc: 0.875
Train Iteration #950:: 0.428422212600708 Acc: 0.875
Train Iteration #1000:: 0.3775501847267151 Acc: 0.875
Epoch Training:: Loss: 0.6944 Acc: 0.7537
Validation Iteration #100:: 0.4802830219268799 Acc: 0.75
Epoch Validation:: Loss: 0.5621 Acc: 0.7450
Epoch 2/9
----------
Train Iteration #1050:: 0.42119747400283813 Acc: 0.7916666666666666
Train Iteration #1100:: 0.5411520600318909 Acc: 0.75
Train Iteration #1150:: 0.3653874397277832 Acc: 0.9583333333333334
Train Iteration #1200:: 0.2901839017868042 Acc: 0.875
Train Iteration #1250:: 0.3966681957244873 Acc: 0.9166666666666666
Train Iteration #1300:: 0.5019552111625671 Acc: 0.8333333333333334
Train Iteration #1350:: 0.4856109023094177 Acc: 0.875
Train Iteration #1400:: 0.3557809591293335 Acc: 0.875
Train Iteration #1450:: 0.5866787433624268 Acc: 0.6666666666666666
Train Iteration #1500:: 0.7615982294082642 Acc: 0.75
Epoch Training:: Loss: 0.5125 Acc: 0.8233
Validation Iteration #150:: 0.4655436873435974 Acc: 0.7916666666666666
Epoch Validation:: Loss: 0.4940 Acc: 0.8547
Epoch 3/9
----------
Train Iteration #1550:: 0.24600836634635925 Acc: 0.875
Train Iteration #1600:: 0.3328874707221985 Acc: 0.9166666666666666
Train Iteration #1650:: 0.3059462010860443 Acc: 0.8333333333333334
Train Iteration #1700:: 0.22079269587993622 Acc: 0.9166666666666666
Train Iteration #1750:: 0.5244239568710327 Acc: 0.875
Train Iteration #1800:: 0.4541032314300537 Acc: 0.7916666666666666
Train Iteration #1850:: 0.5172616839408875 Acc: 0.75
Train Iteration #1900:: 0.9958869814872742 Acc: 0.5833333333333334
Train Iteration #1950:: 0.561884880065918 Acc: 0.7916666666666666
Train Iteration #2000:: 0.9128901958465576 Acc: 0.5833333333333334
Epoch Training:: Loss: 0.4684 Acc: 0.8427
Validation Iteration #200:: 1.0297645330429077 Acc: 0.6666666666666666
Epoch Validation:: Loss: 0.8374 Acc: 0.6976
Epoch 4/9
----------
Train Iteration #2050:: 0.5728247165679932 Acc: 0.7916666666666666
Train Iteration #2100:: 0.412136048078537 Acc: 0.7916666666666666
Train Iteration #2150:: 0.733070969581604 Acc: 0.8333333333333334
Train Iteration #2200:: 0.7483951449394226 Acc: 0.8333333333333334
Train Iteration #2250:: 0.4313739240169525 Acc: 0.9166666666666666
Train Iteration #2300:: 0.28008848428726196 Acc: 0.9166666666666666
Train Iteration #2350:: 0.25180312991142273 Acc: 0.9166666666666666
Train Iteration #2400:: 0.49005231261253357 Acc: 0.9166666666666666
Train Iteration #2450:: 0.32711076736450195 Acc: 0.9166666666666666
Train Iteration #2500:: 0.4525447487831116 Acc: 0.9166666666666666
Epoch Training:: Loss: 0.4491 Acc: 0.8496
Validation Iteration #250:: 0.8812107443809509 Acc: 0.6666666666666666
Epoch Validation:: Loss: 0.7385 Acc: 0.6590
Epoch 5/9
----------
Train Iteration #2550:: 0.5344469547271729 Acc: 0.9166666666666666
Train Iteration #2600:: 0.7050475478172302 Acc: 0.9166666666666666
Train Iteration #2650:: 0.47957152128219604 Acc: 0.875
Train Iteration #2700:: 0.2170722335577011 Acc: 0.875
Train Iteration #2750:: 0.4962718188762665 Acc: 0.8333333333333334
Train Iteration #2800:: 0.4258445203304291 Acc: 0.875
Train Iteration #2850:: 0.312646746635437 Acc: 0.9166666666666666
Train Iteration #2900:: 0.4260863661766052 Acc: 0.7916666666666666
Train Iteration #2950:: 0.6061590909957886 Acc: 0.7916666666666666
Train Iteration #3000:: 0.4828205108642578 Acc: 0.875
Epoch Training:: Loss: 0.4370 Acc: 0.8545
Validation Iteration #300:: 0.8577815890312195 Acc: 0.5833333333333334
Epoch Validation:: Loss: 0.8277 Acc: 0.6857
Epoch 6/9
----------
Train Iteration #3050:: 0.7408478260040283 Acc: 0.6666666666666666
Train Iteration #3100:: 0.5668576955795288 Acc: 0.7916666666666666
Train Iteration #3150:: 0.33651846647262573 Acc: 0.875
Train Iteration #3200:: 0.4354425370693207 Acc: 0.8333333333333334
Train Iteration #3250:: 0.26980650424957275 Acc: 0.9166666666666666
Train Iteration #3300:: 0.7058147192001343 Acc: 0.875
Train Iteration #3350:: 0.26930367946624756 Acc: 0.9166666666666666
Train Iteration #3400:: 0.23434220254421234 Acc: 0.9166666666666666
Train Iteration #3450:: 0.2300197333097458 Acc: 0.875
Train Iteration #3500:: 0.4852595925331116 Acc: 0.875
Epoch Training:: Loss: 0.4088 Acc: 0.8671
Validation Iteration #350:: 0.26905590295791626 Acc: 0.9166666666666666
Epoch Validation:: Loss: 0.3621 Acc: 0.8695
Epoch 7/9
----------
Train Iteration #3550:: 0.3699813485145569 Acc: 0.875
Train Iteration #3600:: 0.486525297164917 Acc: 0.8333333333333334
Train Iteration #3650:: 0.452724426984787 Acc: 0.9166666666666666
Train Iteration #3700:: 0.29615259170532227 Acc: 0.9583333333333334
Train Iteration #3750:: 0.40756067633628845 Acc: 0.7916666666666666
Train Iteration #3800:: 0.20850889384746552 Acc: 0.9583333333333334
Train Iteration #3850:: 0.49578484892845154 Acc: 0.7916666666666666
Train Iteration #3900:: 0.12054161727428436 Acc: 1.0
Train Iteration #3950:: 0.35723254084587097 Acc: 0.8333333333333334
Train Iteration #4000:: 0.13033998012542725 Acc: 1.0
Epoch Training:: Loss: 0.3972 Acc: 0.8702
Validation Iteration #400:: 0.6394616961479187 Acc: 0.7916666666666666
Validation Iteration #450:: 0.5314685106277466 Acc: 0.75
Epoch Validation:: Loss: 0.4112 Acc: 0.8258
Epoch 8/9
----------
Train Iteration #4050:: 0.4459099769592285 Acc: 0.9166666666666666
Train Iteration #4100:: 0.1485753208398819 Acc: 0.9583333333333334
Train Iteration #4150:: 0.40726518630981445 Acc: 0.7916666666666666
Train Iteration #4200:: 0.5520788431167603 Acc: 0.75
Train Iteration #4250:: 0.4176377058029175 Acc: 0.7916666666666666
Train Iteration #4300:: 0.5625297427177429 Acc: 0.8333333333333334
Train Iteration #4350:: 0.31117531657218933 Acc: 0.9166666666666666
Train Iteration #4400:: 0.3476296365261078 Acc: 0.875
Train Iteration #4450:: 0.6201432943344116 Acc: 0.7916666666666666
Train Iteration #4500:: 0.5002367496490479 Acc: 0.7916666666666666
Train Iteration #4550:: 0.38704192638397217 Acc: 0.875
Epoch Training:: Loss: 0.3890 Acc: 0.8695
Validation Iteration #500:: 0.3556569516658783 Acc: 0.875
Epoch Validation:: Loss: 0.3531 Acc: 0.8851
Epoch 9/9
----------
Train Iteration #4600:: 0.4411787986755371 Acc: 0.875
Train Iteration #4650:: 0.6844557523727417 Acc: 0.75
Train Iteration #4700:: 0.35950157046318054 Acc: 0.9166666666666666
Train Iteration #4750:: 0.4414207339286804 Acc: 0.8333333333333334
Train Iteration #4800:: 0.3402104079723358 Acc: 0.875
Train Iteration #4850:: 0.20166265964508057 Acc: 0.875
Train Iteration #4900:: 0.4249688386917114 Acc: 0.875
Train Iteration #4950:: 0.15860983729362488 Acc: 0.9583333333333334
Train Iteration #5000:: 0.5752556324005127 Acc: 0.75
Train Iteration #5050:: 0.36456558108329773 Acc: 0.7083333333333334
Epoch Training:: Loss: 0.3851 Acc: 0.8751
Validation Iteration #550:: 0.28929513692855835 Acc: 0.875
Epoch Validation:: Loss: 0.4250 Acc: 0.8236
Lowest Validation Acc: 0.353056 at epoch:8
End time:1:07:05.531689
Program Complete
Average Train Loss:0.5140661279570329
Average Validation Loss:0.5524551354374203
Average Train Accuracy:0.828646734206408
Average Validation Accuracy:0.7873239436619718
