cuda
Params to learn, when feature extract = True:
	 classifier.6.weight
	 classifier.6.bias
Program started
Epoch 0/29
----------
Train Iteration #0:: 1.1323933601379395 Acc: 0.625
Train Iteration #50:: 0.5436251163482666 Acc: 0.875
Train Iteration #100:: 1.5384265184402466 Acc: 0.625
Train Iteration #150:: 1.2593096494674683 Acc: 0.625
Train Iteration #200:: 3.4825527667999268 Acc: 0.375
Train Iteration #250:: 1.8247554302215576 Acc: 0.625
Train Iteration #300:: 0.6772469282150269 Acc: 0.875
Train Iteration #350:: 0.40513360500335693 Acc: 0.875
Train Iteration #400:: 1.816262125968933 Acc: 0.875
Train Iteration #450:: 0.6832113862037659 Acc: 0.625
Train Iteration #500:: 2.1143579483032227 Acc: 0.625
Train Iteration #550:: 3.7079591751098633 Acc: 0.25
Train Iteration #600:: 3.204742908477783 Acc: 0.375
Train Iteration #650:: 1.1299278736114502 Acc: 0.875
Train Iteration #700:: 2.409818172454834 Acc: 0.5
Train Iteration #750:: 2.5974278450012207 Acc: 0.25
Train Iteration #800:: 0.35136088728904724 Acc: 0.875
Train Iteration #850:: 0.4506002366542816 Acc: 0.75
Train Iteration #900:: 1.7327256202697754 Acc: 0.5
Train Iteration #950:: 2.6261844635009766 Acc: 0.625
Train Iteration #1000:: 0.39989322423934937 Acc: 0.875
Train Iteration #1050:: 1.2570362091064453 Acc: 0.75
Train Iteration #1100:: 0.5132652521133423 Acc: 0.875
Train Iteration #1150:: 1.5735299587249756 Acc: 0.625
Train Iteration #1200:: 1.3271487951278687 Acc: 0.75
Train Iteration #1250:: 0.5111539363861084 Acc: 0.875
Train Iteration #1300:: 1.1358155012130737 Acc: 0.875
Train Iteration #1350:: 0.3481132984161377 Acc: 0.875
Train Iteration #1400:: 1.1002509593963623 Acc: 0.875
Train Iteration #1450:: 0.5142781734466553 Acc: 0.875
Train Iteration #1500:: 3.253970146179199 Acc: 0.625
Epoch Training:: Loss: 1.6674 Acc: 0.6658
Validation Iteration #0:: 0.6888406276702881 Acc: 0.875
Validation Iteration #50:: 2.7494266033172607 Acc: 0.625
Validation Iteration #100:: 0.5229944586753845 Acc: 0.75
Validation Iteration #150:: 1.0230399370193481 Acc: 0.875
Epoch Validation:: Loss: 0.9166 Acc: 0.7583
Epoch 1/29
----------
Train Iteration #1550:: 2.863687515258789 Acc: 0.5
Train Iteration #1600:: 1.7798172235488892 Acc: 0.75
Train Iteration #1650:: 2.7826802730560303 Acc: 0.5
Train Iteration #1700:: 1.8246421813964844 Acc: 0.625
Train Iteration #1750:: 0.6854089498519897 Acc: 0.75
Train Iteration #1800:: 0.2884289622306824 Acc: 0.875
Train Iteration #1850:: 0.24486297369003296 Acc: 0.75
Train Iteration #1900:: 3.4522080421447754 Acc: 0.5
Train Iteration #1950:: 0.28460416197776794 Acc: 0.875
Train Iteration #2000:: 1.9679173231124878 Acc: 0.875
Train Iteration #2050:: 1.925147294998169 Acc: 0.75
Train Iteration #2100:: 1.8374379873275757 Acc: 0.5
Train Iteration #2150:: 0.8639368414878845 Acc: 0.625
Train Iteration #2200:: 2.6943302154541016 Acc: 0.625
Train Iteration #2250:: 3.186452865600586 Acc: 0.75
Train Iteration #2300:: 8.354721069335938 Acc: 0.25
Train Iteration #2350:: 4.288535118103027 Acc: 0.5
Train Iteration #2400:: 0.07743681222200394 Acc: 1.0
Train Iteration #2450:: 5.958914756774902 Acc: 0.5
Train Iteration #2500:: 3.6757149696350098 Acc: 0.5
Train Iteration #2550:: 4.311539649963379 Acc: 0.375
Train Iteration #2600:: 3.5437817573547363 Acc: 0.5
Train Iteration #2650:: 1.6959710121154785 Acc: 0.5
Train Iteration #2700:: 1.3730179071426392 Acc: 0.75
Train Iteration #2750:: 0.2369057834148407 Acc: 0.875
Train Iteration #2800:: 4.243290901184082 Acc: 0.5
Train Iteration #2850:: 1.946537733078003 Acc: 0.875
Train Iteration #2900:: 3.2997827529907227 Acc: 0.625
Train Iteration #2950:: 1.1336103677749634 Acc: 0.75
Train Iteration #3000:: 2.371783494949341 Acc: 0.75
Epoch Training:: Loss: 1.8231 Acc: 0.7004
Validation Iteration #200:: 2.872903823852539 Acc: 0.5
Validation Iteration #250:: 0.5757145881652832 Acc: 0.875
Validation Iteration #300:: 0.34523895382881165 Acc: 0.875
Epoch Validation:: Loss: 1.2356 Acc: 0.7242
Epoch 2/29
----------
Train Iteration #3050:: 2.310166358947754 Acc: 0.75
Train Iteration #3100:: 2.421945095062256 Acc: 0.625
Train Iteration #3150:: 1.4387253522872925 Acc: 0.625
Train Iteration #3200:: 3.9555153846740723 Acc: 0.5
Train Iteration #3250:: 2.584784746170044 Acc: 0.625
Train Iteration #3300:: 1.1833546161651611 Acc: 0.75
Train Iteration #3350:: 5.725269794464111 Acc: 0.375
Train Iteration #3400:: 3.6189255714416504 Acc: 0.5
Train Iteration #3450:: 0.5930303335189819 Acc: 0.875
Train Iteration #3500:: 2.079118251800537 Acc: 0.75
Train Iteration #3550:: 0.8620314598083496 Acc: 0.75
Train Iteration #3600:: 0.3960418105125427 Acc: 0.75
Train Iteration #3650:: 1.7836629152297974 Acc: 0.625
Train Iteration #3700:: 0.3790803849697113 Acc: 0.875
Train Iteration #3750:: 1.092138409614563 Acc: 0.75
Train Iteration #3800:: 1.698474407196045 Acc: 0.75
Train Iteration #3850:: 2.4535984992980957 Acc: 0.75
Train Iteration #3900:: 0.8297112584114075 Acc: 0.75
Train Iteration #3950:: 4.048771858215332 Acc: 0.625
Train Iteration #4000:: 1.455838918685913 Acc: 0.75
Train Iteration #4050:: 1.3927173614501953 Acc: 0.875
Train Iteration #4100:: 1.0621577501296997 Acc: 0.75
Train Iteration #4150:: 1.5453804731369019 Acc: 0.625
Train Iteration #4200:: 0.8906693458557129 Acc: 0.625
Train Iteration #4250:: 2.910308837890625 Acc: 0.5
Train Iteration #4300:: 8.535686492919922 Acc: 0.375
Train Iteration #4350:: 0.6966832876205444 Acc: 0.875
Train Iteration #4400:: 0.5164799690246582 Acc: 0.75
Train Iteration #4450:: 0.7377521395683289 Acc: 0.75
Train Iteration #4500:: 0.7934560775756836 Acc: 0.875
Train Iteration #4550:: 0.37959903478622437 Acc: 0.875
Epoch Training:: Loss: 1.9048 Acc: 0.6954
Validation Iteration #350:: 4.082428932189941 Acc: 0.375
Validation Iteration #400:: 0.7713925838470459 Acc: 0.75
Validation Iteration #450:: 2.8380298614501953 Acc: 0.5
Validation Iteration #500:: 1.9280402660369873 Acc: 0.75
Epoch Validation:: Loss: 1.6109 Acc: 0.6709
Epoch 3/29
----------
Train Iteration #4600:: 4.220386505126953 Acc: 0.625
Train Iteration #4650:: 3.3214645385742188 Acc: 0.75
Train Iteration #4700:: 0.0632440373301506 Acc: 1.0
Train Iteration #4750:: 1.5804286003112793 Acc: 0.75
Train Iteration #4800:: 2.3256125450134277 Acc: 0.75
Train Iteration #4850:: 3.755424976348877 Acc: 0.125
Train Iteration #4900:: 0.22628341615200043 Acc: 1.0
Train Iteration #4950:: 4.991764068603516 Acc: 0.5
Train Iteration #5000:: 1.4970686435699463 Acc: 0.75
Train Iteration #5050:: 0.6780121326446533 Acc: 0.875
Train Iteration #5100:: 1.0631605386734009 Acc: 0.75
Train Iteration #5150:: 2.3689022064208984 Acc: 0.5
Train Iteration #5200:: 2.0487403869628906 Acc: 0.75
Train Iteration #5250:: 2.580894947052002 Acc: 0.5
Train Iteration #5300:: 2.5277233123779297 Acc: 0.875
Train Iteration #5350:: 0.8090776801109314 Acc: 0.75
Train Iteration #5400:: 0.11974811553955078 Acc: 1.0
Train Iteration #5450:: 3.2459192276000977 Acc: 0.875
Train Iteration #5500:: 1.9890344142913818 Acc: 0.75
Train Iteration #5550:: 2.0058786869049072 Acc: 0.75
Train Iteration #5600:: 1.6414848566055298 Acc: 0.625
Train Iteration #5650:: 5.85454797744751 Acc: 0.25
Train Iteration #5700:: 1.8030651807785034 Acc: 0.75
Train Iteration #5750:: 1.2685434818267822 Acc: 0.625
Train Iteration #5800:: 3.141547203063965 Acc: 0.5
Train Iteration #5850:: 0.45984527468681335 Acc: 0.875
Train Iteration #5900:: 0.13778811693191528 Acc: 1.0
Train Iteration #5950:: 1.6747677326202393 Acc: 0.75
Train Iteration #6000:: 1.7979836463928223 Acc: 0.625
Train Iteration #6050:: 0.30673685669898987 Acc: 0.875
Epoch Training:: Loss: 2.0173 Acc: 0.6930
Validation Iteration #550:: 1.039730429649353 Acc: 0.75
Validation Iteration #600:: 0.0601433701813221 Acc: 1.0
Validation Iteration #650:: 0.5564517974853516 Acc: 0.875
Epoch Validation:: Loss: 0.8822 Acc: 0.7991
Epoch 4/29
----------
Train Iteration #6100:: 1.7154619693756104 Acc: 0.75
Train Iteration #6150:: 2.350029945373535 Acc: 0.625
Train Iteration #6200:: 0.9093114137649536 Acc: 0.75
Train Iteration #6250:: 1.7845124006271362 Acc: 0.875
Train Iteration #6300:: 4.637321472167969 Acc: 0.5
Train Iteration #6350:: 0.5823184847831726 Acc: 0.875
Train Iteration #6400:: 0.7098811268806458 Acc: 0.75
Train Iteration #6450:: 1.1643049716949463 Acc: 0.625
Train Iteration #6500:: 1.2234928607940674 Acc: 0.75
Train Iteration #6550:: 3.548675775527954 Acc: 0.375
Train Iteration #6600:: 3.3960914611816406 Acc: 0.5
Train Iteration #6650:: 0.7307717800140381 Acc: 0.75
Train Iteration #6700:: 1.1448323726654053 Acc: 0.75
Train Iteration #6750:: 0.050757698714733124 Acc: 1.0
Train Iteration #6800:: 2.3710873126983643 Acc: 0.5
Train Iteration #6850:: 2.4688234329223633 Acc: 0.625
Train Iteration #6900:: 1.7828967571258545 Acc: 0.875
Train Iteration #6950:: 1.4526524543762207 Acc: 0.625
Train Iteration #7000:: 1.655137538909912 Acc: 0.75
Train Iteration #7050:: 0.17097046971321106 Acc: 1.0
Train Iteration #7100:: 0.012038764543831348 Acc: 1.0
Train Iteration #7150:: 1.632866382598877 Acc: 0.625
Train Iteration #7200:: 2.0408360958099365 Acc: 0.75
Train Iteration #7250:: 3.0342674255371094 Acc: 0.375
Train Iteration #7300:: 2.0463452339172363 Acc: 0.5
Train Iteration #7350:: 1.5282213687896729 Acc: 0.875
Train Iteration #7400:: 1.076690912246704 Acc: 0.75
Train Iteration #7450:: 0.7884725332260132 Acc: 0.75
Train Iteration #7500:: 1.3440220355987549 Acc: 0.75
Train Iteration #7550:: 1.5160728693008423 Acc: 0.625
Epoch Training:: Loss: 2.0337 Acc: 0.6918
Validation Iteration #700:: 0.6710906624794006 Acc: 0.875
Validation Iteration #750:: 0.23990242183208466 Acc: 0.875
Validation Iteration #800:: 2.27315616607666 Acc: 0.5
Epoch Validation:: Loss: 1.3404 Acc: 0.7079
Epoch 5/29
----------
Params to learn, when feature extract = False:
	 features.0.weight
	 features.0.bias
	 features.1.weight
	 features.1.bias
	 features.3.weight
	 features.3.bias
	 features.4.weight
	 features.4.bias
	 features.7.weight
	 features.7.bias
	 features.8.weight
	 features.8.bias
	 features.10.weight
	 features.10.bias
	 features.11.weight
	 features.11.bias
	 features.14.weight
	 features.14.bias
	 features.15.weight
	 features.15.bias
	 features.17.weight
	 features.17.bias
	 features.18.weight
	 features.18.bias
	 features.20.weight
	 features.20.bias
	 features.21.weight
	 features.21.bias
	 features.23.weight
	 features.23.bias
	 features.24.weight
	 features.24.bias
	 features.27.weight
	 features.27.bias
	 features.28.weight
	 features.28.bias
	 features.30.weight
	 features.30.bias
	 features.31.weight
	 features.31.bias
	 features.33.weight
	 features.33.bias
	 features.34.weight
	 features.34.bias
	 features.36.weight
	 features.36.bias
	 features.37.weight
	 features.37.bias
	 features.40.weight
	 features.40.bias
	 features.41.weight
	 features.41.bias
	 features.43.weight
	 features.43.bias
	 features.44.weight
	 features.44.bias
	 features.46.weight
	 features.46.bias
	 features.47.weight
	 features.47.bias
	 features.49.weight
	 features.49.bias
	 features.50.weight
	 features.50.bias
	 classifier.0.weight
	 classifier.0.bias
	 classifier.3.weight
	 classifier.3.bias
	 classifier.6.weight
	 classifier.6.bias
Train Iteration #7600:: 3.5287880897521973 Acc: 0.5
Train Iteration #7650:: 0.7120825052261353 Acc: 0.875
Train Iteration #7700:: 0.4288364350795746 Acc: 0.875
Train Iteration #7750:: 0.41417574882507324 Acc: 0.75
Train Iteration #7800:: 0.039307232946157455 Acc: 1.0
Train Iteration #7850:: 1.852473497390747 Acc: 0.375
Train Iteration #7900:: 0.9537402391433716 Acc: 0.75
Train Iteration #7950:: 0.35411354899406433 Acc: 1.0
Train Iteration #8000:: 0.13010428845882416 Acc: 1.0
Train Iteration #8050:: 1.0740923881530762 Acc: 0.75
Train Iteration #8100:: 0.7411821484565735 Acc: 0.5
Train Iteration #8150:: 1.136460542678833 Acc: 0.625
Train Iteration #8200:: 0.6346773505210876 Acc: 0.75
Train Iteration #8250:: 0.35570037364959717 Acc: 0.875
Train Iteration #8300:: 0.19657637178897858 Acc: 1.0
Train Iteration #8350:: 0.5352993607521057 Acc: 0.75
Train Iteration #8400:: 0.05725100263953209 Acc: 1.0
Train Iteration #8450:: 0.18197990953922272 Acc: 1.0
Train Iteration #8500:: 0.20972290635108948 Acc: 1.0
Train Iteration #8550:: 0.34958478808403015 Acc: 0.75
Train Iteration #8600:: 0.036712028086185455 Acc: 1.0
Train Iteration #8650:: 0.2629554569721222 Acc: 0.875
Train Iteration #8700:: 0.1678314059972763 Acc: 0.875
Train Iteration #8750:: 0.1713520884513855 Acc: 1.0
Train Iteration #8800:: 0.9239252805709839 Acc: 0.75
Train Iteration #8850:: 0.48780232667922974 Acc: 0.875
Train Iteration #8900:: 1.0022306442260742 Acc: 0.625
Train Iteration #8950:: 0.19775262475013733 Acc: 1.0
Train Iteration #9000:: 0.4838969111442566 Acc: 0.875
Train Iteration #9050:: 0.0434286892414093 Acc: 1.0
Train Iteration #9100:: 0.4251360595226288 Acc: 0.625
Epoch Training:: Loss: 0.6497 Acc: 0.8117
Validation Iteration #850:: 1.0669485330581665 Acc: 0.75
Validation Iteration #900:: 0.2025051712989807 Acc: 1.0
Validation Iteration #950:: 0.0070341406390070915 Acc: 1.0
Validation Iteration #1000:: 0.02385573275387287 Acc: 1.0
Epoch Validation:: Loss: 0.5289 Acc: 0.8999
Epoch 6/29
----------
Train Iteration #9150:: 0.8973658084869385 Acc: 0.625
Train Iteration #9200:: 0.8442863821983337 Acc: 0.75
Train Iteration #9250:: 0.4097328782081604 Acc: 0.75
Train Iteration #9300:: 0.05670039355754852 Acc: 1.0
Train Iteration #9350:: 0.12099703401327133 Acc: 0.875
Train Iteration #9400:: 0.19671082496643066 Acc: 0.875
Train Iteration #9450:: 0.500845193862915 Acc: 0.875
Train Iteration #9500:: 0.43673866987228394 Acc: 0.875
Train Iteration #9550:: 0.32891201972961426 Acc: 0.875
Train Iteration #9600:: 0.38517946004867554 Acc: 0.75
Train Iteration #9650:: 0.13599298894405365 Acc: 0.875
Train Iteration #9700:: 0.21190939843654633 Acc: 0.875
Train Iteration #9750:: 0.04542259871959686 Acc: 1.0
Train Iteration #9800:: 0.07045446336269379 Acc: 1.0
Train Iteration #9850:: 0.5094825625419617 Acc: 0.875
Train Iteration #9900:: 0.08488146215677261 Acc: 1.0
Train Iteration #9950:: 0.1000991091132164 Acc: 0.875
Train Iteration #10000:: 1.2931957244873047 Acc: 0.75
Train Iteration #10050:: 2.1142613887786865 Acc: 0.75
Train Iteration #10100:: 0.0434262789785862 Acc: 1.0
Train Iteration #10150:: 0.4546230137348175 Acc: 0.875
Train Iteration #10200:: 1.1228736639022827 Acc: 0.875
Train Iteration #10250:: 0.09285173565149307 Acc: 1.0
Train Iteration #10300:: 1.0876648426055908 Acc: 0.625
Train Iteration #10350:: 0.10206694900989532 Acc: 1.0
Train Iteration #10400:: 0.10107730329036713 Acc: 1.0
Train Iteration #10450:: 0.030343055725097656 Acc: 1.0
Train Iteration #10500:: 0.5862430930137634 Acc: 0.875
Train Iteration #10550:: 0.5213387608528137 Acc: 0.625
Train Iteration #10600:: 0.3494910001754761 Acc: 0.75
Epoch Training:: Loss: 0.4315 Acc: 0.8681
Validation Iteration #1050:: 0.5656055808067322 Acc: 0.625
Validation Iteration #1100:: 0.0064496854320168495 Acc: 1.0
Validation Iteration #1150:: 0.15344806015491486 Acc: 0.875
Epoch Validation:: Loss: 0.4294 Acc: 0.8065
Epoch 7/29
----------
Train Iteration #10650:: 0.1672017127275467 Acc: 1.0
Train Iteration #10700:: 0.5787703990936279 Acc: 0.75
Train Iteration #10750:: 0.427867591381073 Acc: 0.875
Train Iteration #10800:: 0.7409212589263916 Acc: 0.875
Train Iteration #10850:: 0.20762966573238373 Acc: 0.875
Train Iteration #10900:: 0.24901363253593445 Acc: 0.875
Train Iteration #10950:: 0.0836053341627121 Acc: 1.0
Train Iteration #11000:: 0.23151639103889465 Acc: 0.875
Train Iteration #11050:: 0.16428813338279724 Acc: 1.0
Train Iteration #11100:: 0.19259966909885406 Acc: 1.0
Train Iteration #11150:: 0.63004070520401 Acc: 0.75
Train Iteration #11200:: 0.3789220154285431 Acc: 0.875
Train Iteration #11250:: 0.38385826349258423 Acc: 0.875
Train Iteration #11300:: 0.06538783013820648 Acc: 1.0
Train Iteration #11350:: 0.1413590908050537 Acc: 0.875
Train Iteration #11400:: 0.6064998507499695 Acc: 0.75
Train Iteration #11450:: 0.3652525246143341 Acc: 0.875
Train Iteration #11500:: 0.24003031849861145 Acc: 0.875
Train Iteration #11550:: 0.4143725037574768 Acc: 0.75
Train Iteration #11600:: 0.0016141622327268124 Acc: 1.0
Train Iteration #11650:: 0.7729628086090088 Acc: 0.75
Train Iteration #11700:: 0.1583426296710968 Acc: 1.0
Train Iteration #11750:: 0.15045227110385895 Acc: 1.0
Train Iteration #11800:: 0.9597064852714539 Acc: 0.375
Train Iteration #11850:: 0.36685219407081604 Acc: 0.875
Train Iteration #11900:: 0.08242911100387573 Acc: 1.0
Train Iteration #11950:: 0.5821879506111145 Acc: 0.875
Train Iteration #12000:: 0.0981065109372139 Acc: 1.0
Train Iteration #12050:: 0.13456761837005615 Acc: 1.0
Train Iteration #12100:: 0.580612301826477 Acc: 0.75
Epoch Training:: Loss: 0.3546 Acc: 0.8906
Validation Iteration #1200:: 0.001150633324868977 Acc: 1.0
Validation Iteration #1250:: 0.8336723446846008 Acc: 0.75
Validation Iteration #1300:: 0.1933227777481079 Acc: 1.0
Validation Iteration #1350:: 0.085861936211586 Acc: 1.0
Epoch Validation:: Loss: 0.2728 Acc: 0.9340
Epoch 8/29
----------
Train Iteration #12150:: 1.1783467531204224 Acc: 0.875
Train Iteration #12200:: 0.15356291830539703 Acc: 1.0
Train Iteration #12250:: 0.18782252073287964 Acc: 0.875
Train Iteration #12300:: 0.2211999148130417 Acc: 0.875
Train Iteration #12350:: 0.011782441288232803 Acc: 1.0
Train Iteration #12400:: 0.046005845069885254 Acc: 1.0
Train Iteration #12450:: 0.4720219671726227 Acc: 0.75
Train Iteration #12500:: 0.040986448526382446 Acc: 1.0
Train Iteration #12550:: 0.02764565497636795 Acc: 1.0
Train Iteration #12600:: 0.12754331529140472 Acc: 1.0
Train Iteration #12650:: 0.22508852183818817 Acc: 0.875
Train Iteration #12700:: 0.7252100110054016 Acc: 0.75
Train Iteration #12750:: 0.08845587819814682 Acc: 1.0
Train Iteration #12800:: 0.057953834533691406 Acc: 1.0
Train Iteration #12850:: 0.26784855127334595 Acc: 0.875
Train Iteration #12900:: 0.27919238805770874 Acc: 1.0
Train Iteration #12950:: 0.0210689939558506 Acc: 1.0
Train Iteration #13000:: 0.6162306666374207 Acc: 0.875
Train Iteration #13050:: 0.3537483513355255 Acc: 0.625
Train Iteration #13100:: 0.320310115814209 Acc: 0.875
Train Iteration #13150:: 1.2970443964004517 Acc: 0.875
Train Iteration #13200:: 0.4261937737464905 Acc: 0.75
Train Iteration #13250:: 0.1682748645544052 Acc: 0.875
Train Iteration #13300:: 0.5052261352539062 Acc: 0.75
Train Iteration #13350:: 0.05989973992109299 Acc: 1.0
Train Iteration #13400:: 0.13805481791496277 Acc: 1.0
Train Iteration #13450:: 0.23227077722549438 Acc: 0.875
Train Iteration #13500:: 0.5775618553161621 Acc: 0.875
Train Iteration #13550:: 0.21312327682971954 Acc: 0.875
Train Iteration #13600:: 0.22133751213550568 Acc: 0.875
Train Iteration #13650:: 0.08106142282485962 Acc: 1.0
Epoch Training:: Loss: 0.3527 Acc: 0.8946
Validation Iteration #1400:: 0.5029635429382324 Acc: 0.75
Validation Iteration #1450:: 0.45366305112838745 Acc: 0.875
Validation Iteration #1500:: 0.9446171522140503 Acc: 0.625
Epoch Validation:: Loss: 0.6115 Acc: 0.7376
Epoch 9/29
----------
Train Iteration #13700:: 0.05060066655278206 Acc: 1.0
Train Iteration #13750:: 0.35103103518486023 Acc: 0.875
Train Iteration #13800:: 0.054442718625068665 Acc: 1.0
Train Iteration #13850:: 1.249056100845337 Acc: 0.75
Train Iteration #13900:: 0.318057119846344 Acc: 0.875
Train Iteration #13950:: 0.018284937366843224 Acc: 1.0
Train Iteration #14000:: 0.14492398500442505 Acc: 0.875
Train Iteration #14050:: 0.0388813279569149 Acc: 1.0
Train Iteration #14100:: 0.008825846947729588 Acc: 1.0
Train Iteration #14150:: 0.005232720635831356 Acc: 1.0
Train Iteration #14200:: 1.425695776939392 Acc: 0.875
Train Iteration #14250:: 0.3854941427707672 Acc: 0.75
Train Iteration #14300:: 0.3583087921142578 Acc: 0.875
Train Iteration #14350:: 0.12368910014629364 Acc: 1.0
Train Iteration #14400:: 0.11811605095863342 Acc: 1.0
Train Iteration #14450:: 0.6249407529830933 Acc: 0.75
Train Iteration #14500:: 0.5533367395401001 Acc: 0.875
Train Iteration #14550:: 0.9881787300109863 Acc: 0.75
Train Iteration #14600:: 0.48925450444221497 Acc: 0.75
Train Iteration #14650:: 1.6362305879592896 Acc: 0.5
Train Iteration #14700:: 1.215491771697998 Acc: 0.75
Train Iteration #14750:: 0.08905322104692459 Acc: 1.0
Train Iteration #14800:: 0.05335240811109543 Acc: 1.0
Train Iteration #14850:: 0.1346273124217987 Acc: 1.0
Train Iteration #14900:: 0.4610786437988281 Acc: 1.0
Train Iteration #14950:: 0.384779155254364 Acc: 0.875
Train Iteration #15000:: 0.5709183216094971 Acc: 0.875
Train Iteration #15050:: 0.031946901232004166 Acc: 1.0
Train Iteration #15100:: 0.03126290813088417 Acc: 1.0
Train Iteration #15150:: 0.10669948905706406 Acc: 1.0
Epoch Training:: Loss: 0.3066 Acc: 0.9087
Validation Iteration #1550:: 0.07786721736192703 Acc: 1.0
Validation Iteration #1600:: 0.02126261591911316 Acc: 1.0
Validation Iteration #1650:: 0.1681845635175705 Acc: 1.0
Epoch Validation:: Loss: 0.3325 Acc: 0.9155
Epoch 10/29
----------
Train Iteration #15200:: 0.054763857275247574 Acc: 1.0
Train Iteration #15250:: 0.16178037226200104 Acc: 0.875
Train Iteration #15300:: 0.042300447821617126 Acc: 1.0
Train Iteration #15350:: 0.23042413592338562 Acc: 0.875
Train Iteration #15400:: 0.31463658809661865 Acc: 0.75
Train Iteration #15450:: 0.0772956907749176 Acc: 1.0
Train Iteration #15500:: 0.19399292767047882 Acc: 1.0
Train Iteration #15550:: 0.06602533906698227 Acc: 1.0
Train Iteration #15600:: 0.4024708867073059 Acc: 0.875
Train Iteration #15650:: 0.3175241947174072 Acc: 0.75
Train Iteration #15700:: 0.09960758686065674 Acc: 0.875
Train Iteration #15750:: 0.46159207820892334 Acc: 0.75
Train Iteration #15800:: 1.1721577644348145 Acc: 0.75
Train Iteration #15850:: 0.2312181442975998 Acc: 0.875
Train Iteration #15900:: 0.13651449978351593 Acc: 0.875
Train Iteration #15950:: 0.6017836928367615 Acc: 0.75
Train Iteration #16000:: 0.2084340900182724 Acc: 0.875
Train Iteration #16050:: 0.12078923732042313 Acc: 1.0
Train Iteration #16100:: 0.09385725855827332 Acc: 1.0
Train Iteration #16150:: 0.3316008448600769 Acc: 0.875
Train Iteration #16200:: 1.2142070531845093 Acc: 0.625
Train Iteration #16250:: 0.4157784879207611 Acc: 0.875
Train Iteration #16300:: 0.3671165704727173 Acc: 0.875
Train Iteration #16350:: 0.19730713963508606 Acc: 1.0
Train Iteration #16400:: 0.07642873376607895 Acc: 1.0
Train Iteration #16450:: 0.0675533339381218 Acc: 1.0
Train Iteration #16500:: 0.9177995324134827 Acc: 0.875
Train Iteration #16550:: 0.6354418396949768 Acc: 0.625
Train Iteration #16600:: 0.02388501539826393 Acc: 1.0
Train Iteration #16650:: 0.04934888333082199 Acc: 1.0
Epoch Training:: Loss: 0.3019 Acc: 0.9117
Validation Iteration #1700:: 0.5562780499458313 Acc: 0.75
Validation Iteration #1750:: 0.5586974024772644 Acc: 0.875
Validation Iteration #1800:: 0.1502922922372818 Acc: 1.0
Validation Iteration #1850:: 0.5758358836174011 Acc: 0.875
Epoch Validation:: Loss: 0.3010 Acc: 0.9088
Epoch 11/29
----------
Train Iteration #16700:: 0.11892303079366684 Acc: 1.0
Train Iteration #16750:: 0.5905475616455078 Acc: 0.75
Train Iteration #16800:: 0.12875549495220184 Acc: 1.0
Train Iteration #16850:: 0.1958354115486145 Acc: 1.0
Train Iteration #16900:: 0.04215516895055771 Acc: 1.0
Train Iteration #16950:: 0.17886342108249664 Acc: 0.875
Train Iteration #17000:: 0.49453166127204895 Acc: 0.75
Train Iteration #17050:: 0.17139363288879395 Acc: 0.875
Train Iteration #17100:: 0.5722053647041321 Acc: 0.875
Train Iteration #17150:: 0.012830089777708054 Acc: 1.0
Train Iteration #17200:: 0.004108401946723461 Acc: 1.0
Train Iteration #17250:: 0.018285391852259636 Acc: 1.0
Train Iteration #17300:: 0.8115293383598328 Acc: 0.875
Train Iteration #17350:: 0.6012616157531738 Acc: 0.875
Train Iteration #17400:: 0.1929619163274765 Acc: 1.0
Train Iteration #17450:: 0.12802696228027344 Acc: 0.875
Train Iteration #17500:: 0.04614098742604256 Acc: 1.0
Train Iteration #17550:: 0.42994925379753113 Acc: 0.875
Train Iteration #17600:: 0.15177403390407562 Acc: 1.0
Train Iteration #17650:: 0.009089929983019829 Acc: 1.0
Train Iteration #17700:: 0.1608818769454956 Acc: 0.875
Train Iteration #17750:: 0.07237314432859421 Acc: 1.0
Train Iteration #17800:: 0.05064348876476288 Acc: 1.0
Train Iteration #17850:: 0.3795699179172516 Acc: 0.875
Train Iteration #17900:: 0.05471894517540932 Acc: 1.0
Train Iteration #17950:: 0.1483244001865387 Acc: 0.875
Train Iteration #18000:: 0.4997377395629883 Acc: 0.875
Train Iteration #18050:: 0.35255640745162964 Acc: 0.875
Train Iteration #18100:: 0.06125864014029503 Acc: 1.0
Train Iteration #18150:: 0.182150736451149 Acc: 1.0
Train Iteration #18200:: 0.3555522859096527 Acc: 0.875
Epoch Training:: Loss: 0.2685 Acc: 0.9217
Validation Iteration #1900:: 0.7721424102783203 Acc: 0.875
Validation Iteration #1950:: 0.0007587075233459473 Acc: 1.0
Validation Iteration #2000:: 0.024581756442785263 Acc: 1.0
Epoch Validation:: Loss: 0.4163 Acc: 0.9192
Epoch 12/29
----------
Train Iteration #18250:: 0.6237552165985107 Acc: 0.875
Train Iteration #18300:: 0.06186360865831375 Acc: 1.0
Train Iteration #18350:: 0.3560139238834381 Acc: 0.875
Train Iteration #18400:: 0.017792243510484695 Acc: 1.0
Train Iteration #18450:: 0.020960733294487 Acc: 1.0
Train Iteration #18500:: 0.013617786578834057 Acc: 1.0
Train Iteration #18550:: 0.20989757776260376 Acc: 0.875
Train Iteration #18600:: 0.6876989603042603 Acc: 0.75
Train Iteration #18650:: 0.022152278572320938 Acc: 1.0
Train Iteration #18700:: 0.01658649742603302 Acc: 1.0
Train Iteration #18750:: 0.07227977365255356 Acc: 1.0
Train Iteration #18800:: 0.04978606477379799 Acc: 1.0
Train Iteration #18850:: 0.032941754907369614 Acc: 1.0
Train Iteration #18900:: 0.013334996066987514 Acc: 1.0
Train Iteration #18950:: 0.45191237330436707 Acc: 0.625
Train Iteration #19000:: 0.36310335993766785 Acc: 1.0
Train Iteration #19050:: 0.3659361004829407 Acc: 1.0
Train Iteration #19100:: 0.9069399833679199 Acc: 0.75
Train Iteration #19150:: 0.1192021295428276 Acc: 1.0
Train Iteration #19200:: 0.005752256140112877 Acc: 1.0
Train Iteration #19250:: 0.002242372138425708 Acc: 1.0
Train Iteration #19300:: 1.011109709739685 Acc: 0.75
Train Iteration #19350:: 0.12142857164144516 Acc: 1.0
Train Iteration #19400:: 0.007660167291760445 Acc: 1.0
Train Iteration #19450:: 0.06880081444978714 Acc: 1.0
Train Iteration #19500:: 0.008022777736186981 Acc: 1.0
Train Iteration #19550:: 0.3923170268535614 Acc: 0.875
Train Iteration #19600:: 0.11252741515636444 Acc: 1.0
Train Iteration #19650:: 0.10251019895076752 Acc: 1.0
Train Iteration #19700:: 0.21481451392173767 Acc: 1.0
Epoch Training:: Loss: 0.2436 Acc: 0.9324
Validation Iteration #2050:: 0.12279261648654938 Acc: 0.875
Validation Iteration #2100:: 0.03930022567510605 Acc: 1.0
Validation Iteration #2150:: 0.21444112062454224 Acc: 0.875
Epoch Validation:: Loss: 0.2987 Acc: 0.8940
Epoch 13/29
----------
Train Iteration #19750:: 0.1868746280670166 Acc: 0.875
Train Iteration #19800:: 0.3339821696281433 Acc: 0.875
Train Iteration #19850:: 0.2871388792991638 Acc: 0.875
Train Iteration #19900:: 0.04571592062711716 Acc: 1.0
Train Iteration #19950:: 0.08477956056594849 Acc: 1.0
Train Iteration #20000:: 0.30251947045326233 Acc: 0.875
Train Iteration #20050:: 1.2210302352905273 Acc: 0.75
Train Iteration #20100:: 0.0012148490641266108 Acc: 1.0
Train Iteration #20150:: 0.24570389091968536 Acc: 0.875
Train Iteration #20200:: 0.02382800355553627 Acc: 1.0
Train Iteration #20250:: 0.25380510091781616 Acc: 0.875
Train Iteration #20300:: 0.40540817379951477 Acc: 0.75
Train Iteration #20350:: 0.13093988597393036 Acc: 0.875
Train Iteration #20400:: 0.14603503048419952 Acc: 0.875
Train Iteration #20450:: 0.42207765579223633 Acc: 0.875
Train Iteration #20500:: 0.04224792867898941 Acc: 1.0
Train Iteration #20550:: 0.22109505534172058 Acc: 1.0
Train Iteration #20600:: 0.2265932857990265 Acc: 1.0
Train Iteration #20650:: 0.10900554805994034 Acc: 0.875
Train Iteration #20700:: 0.5389854907989502 Acc: 0.875
Train Iteration #20750:: 1.0317773818969727 Acc: 0.625
Train Iteration #20800:: 0.09372207522392273 Acc: 1.0
Train Iteration #20850:: 0.13847461342811584 Acc: 1.0
Train Iteration #20900:: 0.7970961928367615 Acc: 0.75
Train Iteration #20950:: 0.5552667379379272 Acc: 0.875
Train Iteration #21000:: 0.39188069105148315 Acc: 0.75
Train Iteration #21050:: 0.07770875841379166 Acc: 1.0
Train Iteration #21100:: 0.08627700805664062 Acc: 1.0
Train Iteration #21150:: 0.7845785021781921 Acc: 0.875
Train Iteration #21200:: 0.06931403279304504 Acc: 1.0
Train Iteration #21250:: 0.10774815827608109 Acc: 1.0
Epoch Training:: Loss: 0.2645 Acc: 0.9264
Validation Iteration #2200:: 0.01071686577051878 Acc: 1.0
Validation Iteration #2250:: 0.009551671333611012 Acc: 1.0
Validation Iteration #2300:: 0.7745317220687866 Acc: 0.75
Validation Iteration #2350:: 0.01046398188918829 Acc: 1.0
Epoch Validation:: Loss: 0.2652 Acc: 0.9318
Epoch 14/29
----------
Train Iteration #21300:: 0.14271104335784912 Acc: 1.0
Train Iteration #21350:: 0.09382501989603043 Acc: 1.0
Train Iteration #21400:: 0.3612500727176666 Acc: 0.75
Train Iteration #21450:: 0.08371467143297195 Acc: 1.0
Train Iteration #21500:: 0.0825778916478157 Acc: 1.0
Train Iteration #21550:: 0.05013527721166611 Acc: 1.0
Train Iteration #21600:: 0.23325368762016296 Acc: 1.0
Train Iteration #21650:: 0.21174030005931854 Acc: 0.875
Train Iteration #21700:: 0.09477561712265015 Acc: 1.0
Train Iteration #21750:: 0.10535983741283417 Acc: 1.0
Train Iteration #21800:: 0.18454575538635254 Acc: 0.875
Train Iteration #21850:: 0.11002294719219208 Acc: 0.875
Train Iteration #21900:: 0.16391856968402863 Acc: 0.875
Train Iteration #21950:: 0.16806566715240479 Acc: 1.0
Train Iteration #22000:: 0.05131283402442932 Acc: 1.0
Train Iteration #22050:: 0.011594484560191631 Acc: 1.0
Train Iteration #22100:: 0.5023544430732727 Acc: 0.875
Train Iteration #22150:: 0.49462413787841797 Acc: 0.875
Train Iteration #22200:: 0.6308354735374451 Acc: 0.75
Train Iteration #22250:: 0.06310838460922241 Acc: 1.0
Train Iteration #22300:: 0.13928836584091187 Acc: 0.875
Train Iteration #22350:: 0.17773887515068054 Acc: 0.875
Train Iteration #22400:: 0.21750423312187195 Acc: 1.0
Train Iteration #22450:: 0.08352700620889664 Acc: 1.0
Train Iteration #22500:: 0.010428482666611671 Acc: 1.0
Train Iteration #22550:: 0.08878396451473236 Acc: 1.0
Train Iteration #22600:: 0.435403436422348 Acc: 0.875
Train Iteration #22650:: 0.001753110671415925 Acc: 1.0
Train Iteration #22700:: 0.287180095911026 Acc: 0.875
Train Iteration #22750:: 0.10068379342556 Acc: 1.0
Epoch Training:: Loss: 0.2160 Acc: 0.9355
Validation Iteration #2400:: 0.015816299244761467 Acc: 1.0
Validation Iteration #2450:: 0.1118556559085846 Acc: 1.0
Validation Iteration #2500:: 0.11399254947900772 Acc: 1.0
Epoch Validation:: Loss: 0.2370 Acc: 0.9362
Epoch 15/29
----------
Train Iteration #22800:: 0.23782941699028015 Acc: 0.875
Train Iteration #22850:: 0.016773290932178497 Acc: 1.0
Train Iteration #22900:: 0.20213738083839417 Acc: 1.0
Train Iteration #22950:: 0.16552111506462097 Acc: 0.875
Train Iteration #23000:: 0.007614572532474995 Acc: 1.0
Train Iteration #23050:: 0.24777668714523315 Acc: 0.875
Train Iteration #23100:: 0.08333761245012283 Acc: 1.0
Train Iteration #23150:: 0.009198182262480259 Acc: 1.0
Train Iteration #23200:: 0.01600474864244461 Acc: 1.0
Train Iteration #23250:: 0.027960002422332764 Acc: 1.0
Train Iteration #23300:: 0.7880876660346985 Acc: 0.875
Train Iteration #23350:: 0.008437572047114372 Acc: 1.0
Train Iteration #23400:: 0.005787624046206474 Acc: 1.0
Train Iteration #23450:: 0.10436101257801056 Acc: 1.0
Train Iteration #23500:: 0.006137510295957327 Acc: 1.0
Train Iteration #23550:: 0.03737326338887215 Acc: 1.0
Train Iteration #23600:: 0.057434991002082825 Acc: 1.0
Train Iteration #23650:: 0.05403823405504227 Acc: 1.0
Train Iteration #23700:: 0.04134289547801018 Acc: 1.0
Train Iteration #23750:: 0.023638872429728508 Acc: 1.0
Train Iteration #23800:: 0.01597633771598339 Acc: 1.0
Train Iteration #23850:: 0.5753360390663147 Acc: 0.75
Train Iteration #23900:: 0.054756615310907364 Acc: 1.0
Train Iteration #23950:: 0.23330092430114746 Acc: 0.875
Train Iteration #24000:: 0.05783631652593613 Acc: 1.0
Train Iteration #24050:: 0.09619902819395065 Acc: 0.875
Train Iteration #24100:: 0.026071734726428986 Acc: 1.0
Train Iteration #24150:: 0.05132193863391876 Acc: 1.0
Train Iteration #24200:: 0.03008308820426464 Acc: 1.0
Train Iteration #24250:: 0.10226737707853317 Acc: 1.0
Epoch Training:: Loss: 0.2036 Acc: 0.9426
Validation Iteration #2550:: 0.5804045796394348 Acc: 0.875
Validation Iteration #2600:: 0.07497121393680573 Acc: 1.0
Validation Iteration #2650:: 0.0167197547852993 Acc: 1.0
Validation Iteration #2700:: 0.46466389298439026 Acc: 0.875
Epoch Validation:: Loss: 0.2896 Acc: 0.9370
Epoch 16/29
----------
Train Iteration #24300:: 0.18870438635349274 Acc: 0.875
Train Iteration #24350:: 0.010678578168153763 Acc: 1.0
Train Iteration #24400:: 0.08017134666442871 Acc: 1.0
Train Iteration #24450:: 0.2211286872625351 Acc: 1.0
Train Iteration #24500:: 0.0019295436795800924 Acc: 1.0
Train Iteration #24550:: 0.2862294316291809 Acc: 0.875
Train Iteration #24600:: 0.009800261817872524 Acc: 1.0
Train Iteration #24650:: 0.8272312879562378 Acc: 0.875
Train Iteration #24700:: 0.061916325241327286 Acc: 1.0
Train Iteration #24750:: 0.014568943530321121 Acc: 1.0
Train Iteration #24800:: 0.023421719670295715 Acc: 1.0
Train Iteration #24850:: 0.129169762134552 Acc: 1.0
Train Iteration #24900:: 0.08986496180295944 Acc: 0.875
Train Iteration #24950:: 0.005140276625752449 Acc: 1.0
Train Iteration #25000:: 0.9136523604393005 Acc: 0.75
Train Iteration #25050:: 0.11434217542409897 Acc: 1.0
Train Iteration #25100:: 0.1390235424041748 Acc: 1.0
Train Iteration #25150:: 0.056150492280721664 Acc: 1.0
Train Iteration #25200:: 0.2965020537376404 Acc: 0.875
Train Iteration #25250:: 0.3388906717300415 Acc: 1.0
Train Iteration #25300:: 0.2794268727302551 Acc: 0.875
Train Iteration #25350:: 0.05921490490436554 Acc: 1.0
Train Iteration #25400:: 0.17218443751335144 Acc: 0.875
Train Iteration #25450:: 0.017420757561922073 Acc: 1.0
Train Iteration #25500:: 0.05219196900725365 Acc: 1.0
Train Iteration #25550:: 0.025825757533311844 Acc: 1.0
Train Iteration #25600:: 0.2027200311422348 Acc: 1.0
Train Iteration #25650:: 0.0008468321757391095 Acc: 1.0
Train Iteration #25700:: 0.3022630214691162 Acc: 0.875
Train Iteration #25750:: 0.010110707953572273 Acc: 1.0
Train Iteration #25800:: 0.021921444684267044 Acc: 1.0
Epoch Training:: Loss: 0.2177 Acc: 0.9426
Validation Iteration #2750:: 1.0035284757614136 Acc: 0.75
Validation Iteration #2800:: 0.00481021823361516 Acc: 1.0
Validation Iteration #2850:: 0.28683027625083923 Acc: 0.875
Epoch Validation:: Loss: 0.3485 Acc: 0.8977
Epoch 17/29
----------
Train Iteration #25850:: 0.08329777419567108 Acc: 1.0
Train Iteration #25900:: 0.3538421094417572 Acc: 0.875
Train Iteration #25950:: 0.7616804242134094 Acc: 0.75
Train Iteration #26000:: 0.055669307708740234 Acc: 1.0
Train Iteration #26050:: 0.28550663590431213 Acc: 0.875
Train Iteration #26100:: 0.03942912817001343 Acc: 1.0
Train Iteration #26150:: 0.3606973886489868 Acc: 0.75
Train Iteration #26200:: 0.07808886468410492 Acc: 1.0
Train Iteration #26250:: 0.2017548829317093 Acc: 1.0
Train Iteration #26300:: 0.002033670200034976 Acc: 1.0
Train Iteration #26350:: 0.02068915218114853 Acc: 1.0
Train Iteration #26400:: 0.022499816492199898 Acc: 1.0
Train Iteration #26450:: 0.11436593532562256 Acc: 1.0
Train Iteration #26500:: 0.6958893537521362 Acc: 0.875
Train Iteration #26550:: 0.5105774402618408 Acc: 0.875
Train Iteration #26600:: 0.15296225249767303 Acc: 1.0
Train Iteration #26650:: 0.015369508415460587 Acc: 1.0
Train Iteration #26700:: 0.025110037997364998 Acc: 1.0
Train Iteration #26750:: 0.10542194545269012 Acc: 1.0
Train Iteration #26800:: 0.36645710468292236 Acc: 0.875
Train Iteration #26850:: 0.020857956260442734 Acc: 1.0
Train Iteration #26900:: 0.20718275010585785 Acc: 0.875
Train Iteration #26950:: 0.012236104346811771 Acc: 1.0
Train Iteration #27000:: 0.05260474607348442 Acc: 1.0
Train Iteration #27050:: 0.2920875549316406 Acc: 0.875
Train Iteration #27100:: 0.14283400774002075 Acc: 1.0
Train Iteration #27150:: 0.04547211527824402 Acc: 1.0
Train Iteration #27200:: 0.023954171687364578 Acc: 1.0
Train Iteration #27250:: 1.1313124895095825 Acc: 0.75
Train Iteration #27300:: 0.0005921832052990794 Acc: 1.0
Epoch Training:: Loss: 0.1699 Acc: 0.9520
Validation Iteration #2900:: 0.0062265703454613686 Acc: 1.0
Validation Iteration #2950:: 0.000272217090241611 Acc: 1.0
Validation Iteration #3000:: 1.3569672107696533 Acc: 0.875
Epoch Validation:: Loss: 0.3972 Acc: 0.9451
Epoch 18/29
----------
Train Iteration #27350:: 0.0002878594968933612 Acc: 1.0
Train Iteration #27400:: 0.07545885443687439 Acc: 1.0
Train Iteration #27450:: 0.004609256982803345 Acc: 1.0
Train Iteration #27500:: 0.0029987923335283995 Acc: 1.0
Train Iteration #27550:: 0.045160677284002304 Acc: 1.0
Train Iteration #27600:: 0.4497159719467163 Acc: 0.875
Train Iteration #27650:: 0.9971227645874023 Acc: 0.75
Train Iteration #27700:: 0.12492921948432922 Acc: 1.0
Train Iteration #27750:: 0.15699689090251923 Acc: 0.875
Train Iteration #27800:: 0.014422436244785786 Acc: 1.0
Train Iteration #27850:: 0.07792738080024719 Acc: 1.0
Train Iteration #27900:: 0.4549786448478699 Acc: 0.875
Train Iteration #27950:: 0.0004160475800745189 Acc: 1.0
Train Iteration #28000:: 1.0871930122375488 Acc: 0.75
Train Iteration #28050:: 0.34267324209213257 Acc: 0.875
Train Iteration #28100:: 0.029148273169994354 Acc: 1.0
Train Iteration #28150:: 0.1611722707748413 Acc: 1.0
Train Iteration #28200:: 0.02718433365225792 Acc: 1.0
Train Iteration #28250:: 0.0019169533625245094 Acc: 1.0
Train Iteration #28300:: 0.20003299415111542 Acc: 0.875
Train Iteration #28350:: 0.07639548182487488 Acc: 1.0
Train Iteration #28400:: 0.0009270894224755466 Acc: 1.0
Train Iteration #28450:: 0.05212199687957764 Acc: 1.0
Train Iteration #28500:: 0.05184432491660118 Acc: 1.0
Train Iteration #28550:: 0.029845669865608215 Acc: 1.0
Train Iteration #28600:: 0.2208642065525055 Acc: 0.875
Train Iteration #28650:: 0.14068631827831268 Acc: 1.0
Train Iteration #28700:: 0.04263385012745857 Acc: 1.0
Train Iteration #28750:: 0.12275214493274689 Acc: 0.875
Train Iteration #28800:: 0.011818303726613522 Acc: 1.0
Epoch Training:: Loss: 0.1589 Acc: 0.9561
Validation Iteration #3050:: 0.2319783717393875 Acc: 0.875
Validation Iteration #3100:: 0.3136567771434784 Acc: 0.75
Validation Iteration #3150:: 0.008947769179940224 Acc: 1.0
Validation Iteration #3200:: 0.06073436513543129 Acc: 1.0
Epoch Validation:: Loss: 0.4010 Acc: 0.8910
Epoch 19/29
----------
Train Iteration #28850:: 0.14725449681282043 Acc: 0.875
Train Iteration #28900:: 0.364176481962204 Acc: 0.75
Train Iteration #28950:: 1.5952041149139404 Acc: 0.75
Train Iteration #29000:: 0.0030296382028609514 Acc: 1.0
Train Iteration #29050:: 0.09964796155691147 Acc: 1.0
Train Iteration #29100:: 0.021372083574533463 Acc: 1.0
Train Iteration #29150:: 0.66300368309021 Acc: 0.875
Train Iteration #29200:: 0.3392450511455536 Acc: 0.875
Train Iteration #29250:: 0.05566318705677986 Acc: 1.0
Train Iteration #29300:: 0.6344341039657593 Acc: 0.875
Train Iteration #29350:: 0.041566237807273865 Acc: 1.0
Train Iteration #29400:: 0.10020904242992401 Acc: 1.0
Train Iteration #29450:: 0.17957764863967896 Acc: 0.875
Train Iteration #29500:: 0.5303996801376343 Acc: 0.875
Train Iteration #29550:: 0.2698310315608978 Acc: 0.875
Train Iteration #29600:: 0.08210659772157669 Acc: 1.0
Train Iteration #29650:: 0.001975095132365823 Acc: 1.0
Train Iteration #29700:: 0.18453945219516754 Acc: 0.875
Train Iteration #29750:: 0.0070987301878631115 Acc: 1.0
Train Iteration #29800:: 0.030497606843709946 Acc: 1.0
Train Iteration #29850:: 0.037801213562488556 Acc: 1.0
Train Iteration #29900:: 0.0689997673034668 Acc: 1.0
Train Iteration #29950:: 0.028758537024259567 Acc: 1.0
Train Iteration #30000:: 0.19621634483337402 Acc: 0.875
Train Iteration #30050:: 0.6971719264984131 Acc: 0.875
Train Iteration #30100:: 0.384357750415802 Acc: 0.875
Train Iteration #30150:: 0.02516034059226513 Acc: 1.0
Train Iteration #30200:: 0.3374629020690918 Acc: 0.875
Train Iteration #30250:: 0.05823666602373123 Acc: 1.0
Train Iteration #30300:: 0.11711936444044113 Acc: 0.875
Train Iteration #30350:: 0.025597596541047096 Acc: 1.0
Epoch Training:: Loss: 0.1635 Acc: 0.9553
Validation Iteration #3250:: 0.014316615648567677 Acc: 1.0
Validation Iteration #3300:: 0.016532987356185913 Acc: 1.0
Validation Iteration #3350:: 0.001489870948716998 Acc: 1.0
Epoch Validation:: Loss: 0.3598 Acc: 0.9311
Epoch 20/29
----------
Train Iteration #30400:: 0.06737038493156433 Acc: 1.0
Train Iteration #30450:: 0.9297072291374207 Acc: 0.875
Train Iteration #30500:: 0.023629305884242058 Acc: 1.0
Train Iteration #30550:: 0.015210377052426338 Acc: 1.0
Train Iteration #30600:: 0.009230545721948147 Acc: 1.0
Train Iteration #30650:: 0.018303999677300453 Acc: 1.0
Train Iteration #30700:: 0.12458141148090363 Acc: 1.0
Train Iteration #30750:: 0.041266560554504395 Acc: 1.0
Train Iteration #30800:: 0.11941159516572952 Acc: 1.0
Train Iteration #30850:: 0.12321122735738754 Acc: 0.875
Train Iteration #30900:: 0.293020099401474 Acc: 0.875
Train Iteration #30950:: 0.12545309960842133 Acc: 1.0
Train Iteration #31000:: 0.018354790285229683 Acc: 1.0
Train Iteration #31050:: 0.11337556689977646 Acc: 0.875
Train Iteration #31100:: 0.18411393463611603 Acc: 0.875
Train Iteration #31150:: 1.1707470417022705 Acc: 0.75
Train Iteration #31200:: 0.14939340949058533 Acc: 0.875
Train Iteration #31250:: 0.05010221153497696 Acc: 1.0
Train Iteration #31300:: 0.34781932830810547 Acc: 0.875
Train Iteration #31350:: 0.003244941122829914 Acc: 1.0
Train Iteration #31400:: 0.026685480028390884 Acc: 1.0
Train Iteration #31450:: 0.756557285785675 Acc: 0.625
Train Iteration #31500:: 0.0005944838048890233 Acc: 1.0
Train Iteration #31550:: 1.201471209526062 Acc: 0.75
Train Iteration #31600:: 0.056291498243808746 Acc: 1.0
Train Iteration #31650:: 0.9717986583709717 Acc: 0.75
Train Iteration #31700:: 0.31963393092155457 Acc: 0.875
Train Iteration #31750:: 0.4391392767429352 Acc: 0.875
Train Iteration #31800:: 0.1043906882405281 Acc: 1.0
Train Iteration #31850:: 0.21937119960784912 Acc: 0.875
Epoch Training:: Loss: 0.1587 Acc: 0.9554
Validation Iteration #3400:: 0.021693648770451546 Acc: 1.0
Validation Iteration #3450:: 0.007773443590849638 Acc: 1.0
Validation Iteration #3500:: 0.03303872048854828 Acc: 1.0
Epoch Validation:: Loss: 0.3110 Acc: 0.9370
Epoch 21/29
----------
Train Iteration #31900:: 0.0022692482452839613 Acc: 1.0
Train Iteration #31950:: 0.023211292922496796 Acc: 1.0
Train Iteration #32000:: 0.04286948963999748 Acc: 1.0
Train Iteration #32050:: 0.2897728383541107 Acc: 0.875
Train Iteration #32100:: 0.04359222203493118 Acc: 1.0
Train Iteration #32150:: 8.1178019172512e-05 Acc: 1.0
Train Iteration #32200:: 0.08489783853292465 Acc: 1.0
Train Iteration #32250:: 0.020155273377895355 Acc: 1.0
Train Iteration #32300:: 0.058702271431684494 Acc: 1.0
Train Iteration #32350:: 0.08975271135568619 Acc: 1.0
Train Iteration #32400:: 0.13695259392261505 Acc: 1.0
Train Iteration #32450:: 0.04442480206489563 Acc: 1.0
Train Iteration #32500:: 0.13088777661323547 Acc: 0.875
Train Iteration #32550:: 0.0017760065384209156 Acc: 1.0
Train Iteration #32600:: 0.0083494633436203 Acc: 1.0
Train Iteration #32650:: 0.3163580894470215 Acc: 0.875
Train Iteration #32700:: 0.008971080183982849 Acc: 1.0
Train Iteration #32750:: 0.02680002711713314 Acc: 1.0
Train Iteration #32800:: 0.3539154827594757 Acc: 0.875
Train Iteration #32850:: 0.006237731780856848 Acc: 1.0
Train Iteration #32900:: 0.2106555700302124 Acc: 0.875
Train Iteration #32950:: 0.07536368817090988 Acc: 1.0
Train Iteration #33000:: 0.0038578296080231667 Acc: 1.0
Train Iteration #33050:: 0.09121312201023102 Acc: 0.875
Train Iteration #33100:: 0.00043070458923466504 Acc: 1.0
Train Iteration #33150:: 0.1212446540594101 Acc: 1.0
Train Iteration #33200:: 0.0005274778231978416 Acc: 1.0
Train Iteration #33250:: 0.00509620038792491 Acc: 1.0
Train Iteration #33300:: 0.0036762624513357878 Acc: 1.0
Train Iteration #33350:: 0.03941688314080238 Acc: 1.0
Epoch Training:: Loss: 0.1157 Acc: 0.9677
Validation Iteration #3550:: 0.5129379034042358 Acc: 0.75
Validation Iteration #3600:: 0.21556787192821503 Acc: 0.875
Validation Iteration #3650:: 0.4133244454860687 Acc: 0.875
Validation Iteration #3700:: 0.30646851658821106 Acc: 0.875
Epoch Validation:: Loss: 0.5351 Acc: 0.8873
Epoch 22/29
----------
Train Iteration #33400:: 0.034848954528570175 Acc: 1.0
Train Iteration #33450:: 0.0008243033662438393 Acc: 1.0
Train Iteration #33500:: 0.4513130784034729 Acc: 0.875
Train Iteration #33550:: 0.7060121893882751 Acc: 0.875
Train Iteration #33600:: 0.05899031460285187 Acc: 1.0
Train Iteration #33650:: 0.00606634421274066 Acc: 1.0
Train Iteration #33700:: 0.0007988152210600674 Acc: 1.0
Train Iteration #33750:: 0.023267343640327454 Acc: 1.0
Train Iteration #33800:: 0.4109184741973877 Acc: 0.875
Train Iteration #33850:: 0.0008441011887043715 Acc: 1.0
Train Iteration #33900:: 0.010212937369942665 Acc: 1.0
Train Iteration #33950:: 0.005074646323919296 Acc: 1.0
Train Iteration #34000:: 0.14320918917655945 Acc: 0.875
Train Iteration #34050:: 0.08344842493534088 Acc: 1.0
Train Iteration #34100:: 0.02015293389558792 Acc: 1.0
Train Iteration #34150:: 1.5334558486938477 Acc: 0.875
Train Iteration #34200:: 0.10158486664295197 Acc: 1.0
Train Iteration #34250:: 3.7895446439506486e-05 Acc: 1.0
Train Iteration #34300:: 0.009637737646698952 Acc: 1.0
Train Iteration #34350:: 0.10037146508693695 Acc: 1.0
Train Iteration #34400:: 0.005591273307800293 Acc: 1.0
Train Iteration #34450:: 0.004499367438256741 Acc: 1.0
Train Iteration #34500:: 0.0030957709532231092 Acc: 1.0
Train Iteration #34550:: 0.02817704528570175 Acc: 1.0
Train Iteration #34600:: 0.09346244484186172 Acc: 1.0
Train Iteration #34650:: 0.08107932657003403 Acc: 1.0
Train Iteration #34700:: 0.6103523373603821 Acc: 0.75
Train Iteration #34750:: 0.09794744104146957 Acc: 1.0
Train Iteration #34800:: 0.6177657842636108 Acc: 0.875
Train Iteration #34850:: 0.005063735879957676 Acc: 1.0
Train Iteration #34900:: 0.003068199846893549 Acc: 1.0
Epoch Training:: Loss: 0.1609 Acc: 0.9574
Validation Iteration #3750:: 1.8774962425231934 Acc: 0.625
Validation Iteration #3800:: 0.5903721451759338 Acc: 0.75
Validation Iteration #3850:: 0.23758786916732788 Acc: 0.875
Epoch Validation:: Loss: 0.4165 Acc: 0.8851
Epoch 23/29
----------
Train Iteration #34950:: 0.0381261371076107 Acc: 1.0
Train Iteration #35000:: 0.04166975989937782 Acc: 1.0
Train Iteration #35050:: 0.551931619644165 Acc: 0.875
Train Iteration #35100:: 0.0060942284762859344 Acc: 1.0
Train Iteration #35150:: 0.1457292139530182 Acc: 1.0
Train Iteration #35200:: 0.258548766374588 Acc: 0.875
Train Iteration #35250:: 0.18669761717319489 Acc: 0.875
Train Iteration #35300:: 0.15303030610084534 Acc: 1.0
Train Iteration #35350:: 0.7002348303794861 Acc: 0.875
Train Iteration #35400:: 0.19941756129264832 Acc: 0.875
Train Iteration #35450:: 0.16799123585224152 Acc: 0.875
Train Iteration #35500:: 0.2514405846595764 Acc: 0.875
Train Iteration #35550:: 0.03817281499505043 Acc: 1.0
Train Iteration #35600:: 0.1163453757762909 Acc: 1.0
Train Iteration #35650:: 0.33746337890625 Acc: 0.875
Train Iteration #35700:: 0.06912586092948914 Acc: 1.0
Train Iteration #35750:: 0.042949073016643524 Acc: 1.0
Train Iteration #35800:: 0.023374440148472786 Acc: 1.0
Train Iteration #35850:: 0.39097365736961365 Acc: 0.875
Train Iteration #35900:: 0.705884575843811 Acc: 0.875
Train Iteration #35950:: 0.04534408450126648 Acc: 1.0
Train Iteration #36000:: 0.398732453584671 Acc: 0.875
Train Iteration #36050:: 5.361856892704964e-06 Acc: 1.0
Train Iteration #36100:: 0.32992956042289734 Acc: 0.875
Train Iteration #36150:: 0.0027274824678897858 Acc: 1.0
Train Iteration #36200:: 0.02214713953435421 Acc: 1.0
Train Iteration #36250:: 0.2566976249217987 Acc: 0.75
Train Iteration #36300:: 0.019687756896018982 Acc: 1.0
Train Iteration #36350:: 0.04288411885499954 Acc: 1.0
Train Iteration #36400:: 0.09365660697221756 Acc: 0.875
Epoch Training:: Loss: 0.1346 Acc: 0.9645
Validation Iteration #3900:: 0.003220571903511882 Acc: 1.0
Validation Iteration #3950:: 0.01782798580825329 Acc: 1.0
Validation Iteration #4000:: 1.7189749479293823 Acc: 0.875
Validation Iteration #4050:: 0.3019571900367737 Acc: 0.875
Epoch Validation:: Loss: 0.3829 Acc: 0.9244
Epoch 24/29
----------
Train Iteration #36450:: 0.005492397118359804 Acc: 1.0
Train Iteration #36500:: 0.031201334670186043 Acc: 1.0
Train Iteration #36550:: 0.03024262934923172 Acc: 1.0
Train Iteration #36600:: 0.07808411121368408 Acc: 1.0
Train Iteration #36650:: 0.04048137366771698 Acc: 1.0
Train Iteration #36700:: 0.35721561312675476 Acc: 0.875
Train Iteration #36750:: 0.03177385404706001 Acc: 1.0
Train Iteration #36800:: 0.14945194125175476 Acc: 1.0
Train Iteration #36850:: 0.0004598110681399703 Acc: 1.0
Train Iteration #36900:: 0.08934841305017471 Acc: 1.0
Train Iteration #36950:: 0.992566704750061 Acc: 0.75
Train Iteration #37000:: 1.7682901670923457e-05 Acc: 1.0
Train Iteration #37050:: 0.11929166316986084 Acc: 0.875
Train Iteration #37100:: 0.022023621946573257 Acc: 1.0
Train Iteration #37150:: 0.0017781020142138004 Acc: 1.0
Train Iteration #37200:: 0.0019585471600294113 Acc: 1.0
Train Iteration #37250:: 0.0318436361849308 Acc: 1.0
Train Iteration #37300:: 0.659140944480896 Acc: 0.875
Train Iteration #37350:: 0.0002430962340440601 Acc: 1.0
Train Iteration #37400:: 0.005035840440541506 Acc: 1.0
Train Iteration #37450:: 0.1298130452632904 Acc: 1.0
Train Iteration #37500:: 0.005176364444196224 Acc: 1.0
Train Iteration #37550:: 0.03111637756228447 Acc: 1.0
Train Iteration #37600:: 0.0704960823059082 Acc: 1.0
Train Iteration #37650:: 0.151332288980484 Acc: 1.0
Train Iteration #37700:: 0.4428991675376892 Acc: 0.875
Train Iteration #37750:: 0.11372198909521103 Acc: 1.0
Train Iteration #37800:: 0.13399524986743927 Acc: 1.0
Train Iteration #37850:: 0.13995206356048584 Acc: 0.875
Train Iteration #37900:: 8.225440979003906e-06 Acc: 1.0
Epoch Training:: Loss: 0.1265 Acc: 0.9669
Validation Iteration #4100:: 0.026289766654372215 Acc: 1.0
Validation Iteration #4150:: 0.0007691601640544832 Acc: 1.0
Validation Iteration #4200:: 0.003460679668933153 Acc: 1.0
Epoch Validation:: Loss: 0.4250 Acc: 0.9340
Epoch 25/29
----------
Train Iteration #37950:: 0.014257811941206455 Acc: 1.0
Train Iteration #38000:: 0.005387615412473679 Acc: 1.0
Train Iteration #38050:: 0.030573632568120956 Acc: 1.0
Train Iteration #38100:: 0.013997320085763931 Acc: 1.0
Train Iteration #38150:: 0.0059989457949995995 Acc: 1.0
Train Iteration #38200:: 0.02254621684551239 Acc: 1.0
Train Iteration #38250:: 0.1197534054517746 Acc: 0.875
Train Iteration #38300:: 0.0002929996117018163 Acc: 1.0
Train Iteration #38350:: 0.038466926664114 Acc: 1.0
Train Iteration #38400:: 0.025176215916872025 Acc: 1.0
Train Iteration #38450:: 0.0009028047788888216 Acc: 1.0
Train Iteration #38500:: 0.010287027806043625 Acc: 1.0
Train Iteration #38550:: 0.000649058842100203 Acc: 1.0
Train Iteration #38600:: 0.0011870781891047955 Acc: 1.0
Train Iteration #38650:: 0.09119457751512527 Acc: 1.0
Train Iteration #38700:: 0.00022041340707801282 Acc: 1.0
Train Iteration #38750:: 0.0408954992890358 Acc: 1.0
Train Iteration #38800:: 0.5172370076179504 Acc: 0.875
Train Iteration #38850:: 0.02205505035817623 Acc: 1.0
Train Iteration #38900:: 7.429593824781477e-05 Acc: 1.0
Train Iteration #38950:: 0.1371125876903534 Acc: 0.875
Train Iteration #39000:: 0.16992920637130737 Acc: 1.0
Train Iteration #39050:: 0.005792644806206226 Acc: 1.0
Train Iteration #39100:: 7.824948261259124e-05 Acc: 1.0
Train Iteration #39150:: 0.32567840814590454 Acc: 0.875
Train Iteration #39200:: 0.0027384513523429632 Acc: 1.0
Train Iteration #39250:: 0.1029546782374382 Acc: 0.875
Train Iteration #39300:: 0.11697003245353699 Acc: 0.875
Train Iteration #39350:: 0.08784309029579163 Acc: 0.875
Train Iteration #39400:: 0.38867422938346863 Acc: 0.875
Train Iteration #39450:: 0.3267734944820404 Acc: 0.875
Epoch Training:: Loss: 0.1226 Acc: 0.9664
Validation Iteration #4250:: 0.0007583308615721762 Acc: 1.0
Validation Iteration #4300:: 0.19009047746658325 Acc: 1.0
Validation Iteration #4350:: 0.017565911635756493 Acc: 1.0
Epoch Validation:: Loss: 0.3297 Acc: 0.9392
Epoch 26/29
----------
Train Iteration #39500:: 0.2655634582042694 Acc: 0.875
Train Iteration #39550:: 2.097831020364538e-05 Acc: 1.0
Train Iteration #39600:: 1.670604251557961e-05 Acc: 1.0
Train Iteration #39650:: 0.00029225923935882747 Acc: 1.0
Train Iteration #39700:: 0.019754750654101372 Acc: 1.0
Train Iteration #39750:: 0.0036970439832657576 Acc: 1.0
Train Iteration #39800:: 0.10979903489351273 Acc: 0.875
Train Iteration #39850:: 0.06302780658006668 Acc: 1.0
Train Iteration #39900:: 0.04864243417978287 Acc: 1.0
Train Iteration #39950:: 0.019622333347797394 Acc: 1.0
Train Iteration #40000:: 0.0031361295841634274 Acc: 1.0
Train Iteration #40050:: 0.004377762787044048 Acc: 1.0
Train Iteration #40100:: 0.10836610198020935 Acc: 1.0
Train Iteration #40150:: 0.3567115366458893 Acc: 0.875
Train Iteration #40200:: 0.0021143555641174316 Acc: 1.0
Train Iteration #40250:: 0.07379202544689178 Acc: 1.0
Train Iteration #40300:: 0.127195805311203 Acc: 1.0
Train Iteration #40350:: 0.155008003115654 Acc: 0.875
Train Iteration #40400:: 0.0004933476448059082 Acc: 1.0
Train Iteration #40450:: 0.09053385257720947 Acc: 1.0
Train Iteration #40500:: 0.17842994630336761 Acc: 0.875
Train Iteration #40550:: 0.08478325605392456 Acc: 1.0
Train Iteration #40600:: 0.2594414949417114 Acc: 0.875
Train Iteration #40650:: 0.15255814790725708 Acc: 1.0
Train Iteration #40700:: 0.0032708344515413046 Acc: 1.0
Train Iteration #40750:: 0.01437803078442812 Acc: 1.0
Train Iteration #40800:: 0.0021988011430948973 Acc: 1.0
Train Iteration #40850:: 0.013487396761775017 Acc: 1.0
Train Iteration #40900:: 0.012725207954645157 Acc: 1.0
Train Iteration #40950:: 0.002083598403260112 Acc: 1.0
Epoch Training:: Loss: 0.1177 Acc: 0.9697
Validation Iteration #4400:: 0.05070989578962326 Acc: 1.0
Validation Iteration #4450:: 0.855505108833313 Acc: 0.875
Validation Iteration #4500:: 0.024085694923996925 Acc: 1.0
Validation Iteration #4550:: 0.006645741406828165 Acc: 1.0
Epoch Validation:: Loss: 0.5821 Acc: 0.9429
Epoch 27/29
----------
Train Iteration #41000:: 0.008656219579279423 Acc: 1.0
Train Iteration #41050:: 0.027774866670370102 Acc: 1.0
Train Iteration #41100:: 0.054709143936634064 Acc: 1.0
Train Iteration #41150:: 0.029451431706547737 Acc: 1.0
Train Iteration #41200:: 0.15700648725032806 Acc: 1.0
Train Iteration #41250:: 0.153795525431633 Acc: 1.0
Train Iteration #41300:: 0.07727916538715363 Acc: 1.0
Train Iteration #41350:: 0.007899157702922821 Acc: 1.0
Train Iteration #41400:: 0.003697064472362399 Acc: 1.0
Train Iteration #41450:: 6.247718556551263e-05 Acc: 1.0
Train Iteration #41500:: 0.005371220875531435 Acc: 1.0
Train Iteration #41550:: 0.0011833809548988938 Acc: 1.0
Train Iteration #41600:: 0.012944549322128296 Acc: 1.0
Train Iteration #41650:: 0.04272754490375519 Acc: 1.0
Train Iteration #41700:: 0.0014383314410224557 Acc: 1.0
Train Iteration #41750:: 0.002199433045461774 Acc: 1.0
Train Iteration #41800:: 0.014524209313094616 Acc: 1.0
Train Iteration #41850:: 0.00037589232670143247 Acc: 1.0
Train Iteration #41900:: 0.04409310221672058 Acc: 1.0
Train Iteration #41950:: 0.015348399989306927 Acc: 1.0
Train Iteration #42000:: 0.0011887657456099987 Acc: 1.0
Train Iteration #42050:: 0.14848080277442932 Acc: 1.0
Train Iteration #42100:: 0.004985725041478872 Acc: 1.0
Train Iteration #42150:: 0.00023161394346971065 Acc: 1.0
Train Iteration #42200:: 0.00358566758222878 Acc: 1.0
Train Iteration #42250:: 0.003540732664987445 Acc: 1.0
Train Iteration #42300:: 0.00661197816953063 Acc: 1.0
Train Iteration #42350:: 0.2448563426733017 Acc: 0.875
Train Iteration #42400:: 0.0006713856128044426 Acc: 1.0
Train Iteration #42450:: 0.09486009180545807 Acc: 0.875
Train Iteration #42500:: 0.005894768051803112 Acc: 1.0
Epoch Training:: Loss: 0.1128 Acc: 0.9690
Validation Iteration #4600:: 0.1907811164855957 Acc: 0.875
Validation Iteration #4650:: 0.0010918909683823586 Acc: 1.0
Validation Iteration #4700:: 0.7551610469818115 Acc: 0.875
Epoch Validation:: Loss: 0.3117 Acc: 0.9333
Epoch 28/29
----------
Train Iteration #42550:: 0.04013948515057564 Acc: 1.0
Train Iteration #42600:: 0.08907310664653778 Acc: 1.0
Train Iteration #42650:: 0.6526322960853577 Acc: 0.875
Train Iteration #42700:: 0.020591601729393005 Acc: 1.0
Train Iteration #42750:: 0.226183220744133 Acc: 1.0
Train Iteration #42800:: 0.019168507307767868 Acc: 1.0
Train Iteration #42850:: 0.0017996462993323803 Acc: 1.0
Train Iteration #42900:: 0.08413580060005188 Acc: 1.0
Train Iteration #42950:: 0.006828601472079754 Acc: 1.0
Train Iteration #43000:: 0.3777480721473694 Acc: 0.75
Train Iteration #43050:: 0.20400145649909973 Acc: 1.0
Train Iteration #43100:: 0.03737340494990349 Acc: 1.0
Train Iteration #43150:: 0.27229583263397217 Acc: 0.875
Train Iteration #43200:: 0.083203986287117 Acc: 1.0
Train Iteration #43250:: 0.00366412615403533 Acc: 1.0
Train Iteration #43300:: 0.0008577978005632758 Acc: 1.0
Train Iteration #43350:: 0.016358962282538414 Acc: 1.0
Train Iteration #43400:: 0.9249311685562134 Acc: 0.875
Train Iteration #43450:: 0.051124803721904755 Acc: 1.0
Train Iteration #43500:: 0.07123643904924393 Acc: 1.0
Train Iteration #43550:: 0.02127823419868946 Acc: 1.0
Train Iteration #43600:: 0.031721457839012146 Acc: 1.0
Train Iteration #43650:: 0.017407938838005066 Acc: 1.0
Train Iteration #43700:: 0.007090118248015642 Acc: 1.0
Train Iteration #43750:: 0.0009719381923787296 Acc: 1.0
Train Iteration #43800:: 0.021851833909749985 Acc: 1.0
Train Iteration #43850:: 0.21194276213645935 Acc: 0.875
Train Iteration #43900:: 0.10194975137710571 Acc: 1.0
Train Iteration #43950:: 0.009306659922003746 Acc: 1.0
Train Iteration #44000:: 0.006637575104832649 Acc: 1.0
Epoch Training:: Loss: 0.1046 Acc: 0.9724
Validation Iteration #4750:: 0.00011554085358511657 Acc: 1.0
Validation Iteration #4800:: 0.0008156010881066322 Acc: 1.0
Validation Iteration #4850:: 0.13464944064617157 Acc: 0.875
Validation Iteration #4900:: 0.012423218227922916 Acc: 1.0
Epoch Validation:: Loss: 0.3738 Acc: 0.9266
Epoch 29/29
----------
Train Iteration #44050:: 0.0006239862996153533 Acc: 1.0
Train Iteration #44100:: 0.02052851766347885 Acc: 1.0
Train Iteration #44150:: 0.1486547440290451 Acc: 0.875
Train Iteration #44200:: 0.023321211338043213 Acc: 1.0
Train Iteration #44250:: 0.01520236860960722 Acc: 1.0
Train Iteration #44300:: 0.0004992247559130192 Acc: 1.0
Train Iteration #44350:: 0.02637897990643978 Acc: 1.0
Train Iteration #44400:: 0.00020797704928554595 Acc: 1.0
Train Iteration #44450:: 0.046738557517528534 Acc: 1.0
Train Iteration #44500:: 0.11934497207403183 Acc: 0.875
Train Iteration #44550:: 0.08724693208932877 Acc: 1.0
Train Iteration #44600:: 0.02871902845799923 Acc: 1.0
Train Iteration #44650:: 0.12667536735534668 Acc: 0.875
Train Iteration #44700:: 0.006536772474646568 Acc: 1.0
Train Iteration #44750:: 0.20770174264907837 Acc: 0.875
Train Iteration #44800:: 0.03869720920920372 Acc: 1.0
Train Iteration #44850:: 0.007794040255248547 Acc: 1.0
Train Iteration #44900:: 0.0017032771138474345 Acc: 1.0
Train Iteration #44950:: 0.0014432461466640234 Acc: 1.0
Train Iteration #45000:: 0.19673007726669312 Acc: 0.875
Train Iteration #45050:: 0.08104032278060913 Acc: 1.0
Train Iteration #45100:: 0.01760040409862995 Acc: 1.0
Train Iteration #45150:: 1.537799835205078e-05 Acc: 1.0
Train Iteration #45200:: 0.009716934524476528 Acc: 1.0
Train Iteration #45250:: 0.027300642803311348 Acc: 1.0
Train Iteration #45300:: 0.0025637317448854446 Acc: 1.0
Train Iteration #45350:: 0.023098088800907135 Acc: 1.0
Train Iteration #45400:: 0.0008714419673196971 Acc: 1.0
Train Iteration #45450:: 0.1278703212738037 Acc: 1.0
Train Iteration #45500:: 0.07164342701435089 Acc: 1.0
Epoch Training:: Loss: 0.1354 Acc: 0.9640
Validation Iteration #4950:: 0.06688807159662247 Acc: 1.0
Validation Iteration #5000:: 0.12535415589809418 Acc: 1.0
Validation Iteration #5050:: 0.27248385548591614 Acc: 0.875
Epoch Validation:: Loss: 0.3484 Acc: 0.9429
Lowest Validation Acc: 0.236980 at epoch:14
End time:6:57:25.302277
Program Complete
Average Train Loss:0.5012957065038595
Average Validation Loss:0.5163812898031923
Average Train Accuracy:0.8949921752738654
Average Validation Accuracy:0.8799604645416358
