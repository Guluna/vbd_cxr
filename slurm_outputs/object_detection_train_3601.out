cuda
Program started
Epoch 0/49
----------
Training Total Loss for Iteration 0 :: 3.247730255126953
Training Total Loss for Iteration 50 :: 0.5056360363960266
Training Total Loss for Iteration 100 :: 0.5279307961463928
Training Total Loss for Iteration 150 :: 0.859862208366394
Training Total Loss for Iteration 200 :: 0.5055071115493774
Training Total Loss for Iteration 250 :: 0.8416582345962524
Training Total Loss for Iteration 300 :: 0.5409516096115112
Training Total Loss for Iteration 350 :: 0.46926674246788025
Train mAP Epoch 0: 0.025857379660010338
Training Epoch #0 Sum Loss:0.0854897464893281
Validation Total Loss for Iteration 0 :: 0.9169683456420898
Validation Total Loss for Iteration 50 :: 0.6605182886123657
Validation mAP Epoch 0: 0.02616943046450615
Validation Epoch #0 Sum Loss:0.07583876086088519
Epoch 1/49
----------
Training Total Loss for Iteration 400 :: 0.5442593097686768
Training Total Loss for Iteration 450 :: 0.7814592123031616
Training Total Loss for Iteration 500 :: 0.6338909268379211
Training Total Loss for Iteration 550 :: 0.523777186870575
Training Total Loss for Iteration 600 :: 0.5626027584075928
Training Total Loss for Iteration 650 :: 0.5648509860038757
Training Total Loss for Iteration 700 :: 0.448038250207901
Train mAP Epoch 1: 0.10177841037511826
Training Epoch #1 Sum Loss:0.07205565377235093
Validation Total Loss for Iteration 100 :: 0.6186100244522095
Validation Total Loss for Iteration 150 :: 0.5018909573554993
Validation mAP Epoch 1: 0.08417125791311264
Validation Epoch #1 Sum Loss:0.06783419311977923
Epoch 2/49
----------
Training Total Loss for Iteration 750 :: 0.7164645195007324
Training Total Loss for Iteration 800 :: 0.38608047366142273
Training Total Loss for Iteration 850 :: 0.6970961689949036
Training Total Loss for Iteration 900 :: 0.4898449182510376
Training Total Loss for Iteration 950 :: 0.5151657462120056
Training Total Loss for Iteration 1000 :: 0.9032728672027588
Training Total Loss for Iteration 1050 :: 0.5130025744438171
Training Total Loss for Iteration 1100 :: 0.7970861792564392
Train mAP Epoch 2: 0.14857126772403717
Training Epoch #2 Sum Loss:0.0720433464329421
Validation Total Loss for Iteration 200 :: 0.3252007067203522
Validation Total Loss for Iteration 250 :: 0.8190764784812927
Validation mAP Epoch 2: 0.11468242108821869
Validation Epoch #2 Sum Loss:0.07310544070787728
Epoch 3/49
----------
Training Total Loss for Iteration 1150 :: 0.5584990978240967
Training Total Loss for Iteration 1200 :: 0.7074480056762695
Training Total Loss for Iteration 1250 :: 0.3520280420780182
Training Total Loss for Iteration 1300 :: 0.7139612436294556
Training Total Loss for Iteration 1350 :: 0.4500276744365692
Training Total Loss for Iteration 1400 :: 0.7946070432662964
Training Total Loss for Iteration 1450 :: 0.7021058797836304
Train mAP Epoch 3: 0.2045595943927765
Training Epoch #3 Sum Loss:0.07290137716884862
Validation Total Loss for Iteration 300 :: 0.5051500797271729
Validation Total Loss for Iteration 350 :: 0.42916128039360046
Validation mAP Epoch 3: 0.16447637975215912
Validation Epoch #3 Sum Loss:0.06934386410284787
Epoch 4/49
----------
Training Total Loss for Iteration 1500 :: 0.6092488169670105
Training Total Loss for Iteration 1550 :: 0.5000657439231873
Training Total Loss for Iteration 1600 :: 0.3815530240535736
Training Total Loss for Iteration 1650 :: 0.4086630344390869
Training Total Loss for Iteration 1700 :: 0.6272975206375122
Training Total Loss for Iteration 1750 :: 0.5101654529571533
Training Total Loss for Iteration 1800 :: 0.7615467309951782
Training Total Loss for Iteration 1850 :: 0.46334338188171387
Train mAP Epoch 4: 0.23296399414539337
Training Epoch #4 Sum Loss:0.07192529836923874
Validation Total Loss for Iteration 400 :: 0.5741134285926819
Validation Total Loss for Iteration 450 :: 0.4636070728302002
Validation mAP Epoch 4: 0.18352314829826355
Validation Epoch #4 Sum Loss:0.06940014959157755
Epoch 5/49
----------
Training Total Loss for Iteration 1900 :: 0.7114870548248291
Training Total Loss for Iteration 1950 :: 0.3429642617702484
Training Total Loss for Iteration 2000 :: 0.5121211409568787
Training Total Loss for Iteration 2050 :: 0.7156861424446106
Training Total Loss for Iteration 2100 :: 0.47855329513549805
Training Total Loss for Iteration 2150 :: 0.5573858022689819
Training Total Loss for Iteration 2200 :: 0.8995639681816101
Train mAP Epoch 5: 0.27664074301719666
Training Epoch #5 Sum Loss:0.0714218745252423
Validation Total Loss for Iteration 500 :: 0.4007898271083832
Validation Total Loss for Iteration 550 :: 0.5545130968093872
Validation mAP Epoch 5: 0.23472800850868225
Validation Epoch #5 Sum Loss:0.07261306319075327
Epoch 6/49
----------
Training Total Loss for Iteration 2250 :: 0.6357877254486084
Training Total Loss for Iteration 2300 :: 0.47055813670158386
Training Total Loss for Iteration 2350 :: 0.7954808473587036
Training Total Loss for Iteration 2400 :: 0.5646679401397705
Training Total Loss for Iteration 2450 :: 0.5053451657295227
Training Total Loss for Iteration 2500 :: 0.5659295320510864
Training Total Loss for Iteration 2550 :: 0.9342713952064514
Training Total Loss for Iteration 2600 :: 0.6130937337875366
Train mAP Epoch 6: 0.3036808371543884
Training Epoch #6 Sum Loss:0.0721106608115519
Validation Total Loss for Iteration 600 :: 0.6759661436080933
Validation Total Loss for Iteration 650 :: 0.6603464484214783
Validation mAP Epoch 6: 0.2092118263244629
Validation Epoch #6 Sum Loss:0.07075964837955932
Epoch 7/49
----------
Training Total Loss for Iteration 2650 :: 0.5602907538414001
Training Total Loss for Iteration 2700 :: 0.6797810792922974
Training Total Loss for Iteration 2750 :: 0.3567934036254883
Training Total Loss for Iteration 2800 :: 0.5928896069526672
Training Total Loss for Iteration 2850 :: 0.5512863993644714
Training Total Loss for Iteration 2900 :: 0.396633118391037
Training Total Loss for Iteration 2950 :: 0.426129549741745
Train mAP Epoch 7: 0.31818196177482605
Training Epoch #7 Sum Loss:0.07063055146424332
Validation Total Loss for Iteration 700 :: 0.6225084662437439
Validation Total Loss for Iteration 750 :: 0.9379701018333435
Validation mAP Epoch 7: 0.26460590958595276
Validation Epoch #7 Sum Loss:0.07324933086056262
Epoch 8/49
----------
Training Total Loss for Iteration 3000 :: 0.4901309609413147
Training Total Loss for Iteration 3050 :: 0.6169530153274536
Training Total Loss for Iteration 3100 :: 0.43839502334594727
Training Total Loss for Iteration 3150 :: 0.5507144927978516
Training Total Loss for Iteration 3200 :: 0.4932519495487213
Training Total Loss for Iteration 3250 :: 0.6278348565101624
Training Total Loss for Iteration 3300 :: 0.48467332124710083
Training Total Loss for Iteration 3350 :: 0.6009535789489746
Train mAP Epoch 8: 0.33781224489212036
Training Epoch #8 Sum Loss:0.06990148201559822
Validation Total Loss for Iteration 800 :: 0.8745296001434326
Validation Total Loss for Iteration 850 :: 0.7416188716888428
Validation mAP Epoch 8: 0.2650671899318695
Validation Epoch #8 Sum Loss:0.07203856389969587
Epoch 9/49
----------
Training Total Loss for Iteration 3400 :: 0.8091222643852234
Training Total Loss for Iteration 3450 :: 0.47555428743362427
Training Total Loss for Iteration 3500 :: 0.7679566740989685
Training Total Loss for Iteration 3550 :: 0.5836249589920044
Training Total Loss for Iteration 3600 :: 0.5044751763343811
Training Total Loss for Iteration 3650 :: 0.8303267955780029
Training Total Loss for Iteration 3700 :: 0.567979097366333
Train mAP Epoch 9: 0.3593687415122986
Training Epoch #9 Sum Loss:0.06961499472485817
Validation Total Loss for Iteration 900 :: 0.39951956272125244
Validation Total Loss for Iteration 950 :: 0.6583492755889893
Validation mAP Epoch 9: 0.2801629900932312
Validation Epoch #9 Sum Loss:0.06723453407175839
Epoch 10/49
----------
Training Total Loss for Iteration 3750 :: 0.7462387084960938
Training Total Loss for Iteration 3800 :: 0.5412724018096924
Training Total Loss for Iteration 3850 :: 0.6121214032173157
Training Total Loss for Iteration 3900 :: 0.9569182395935059
Training Total Loss for Iteration 3950 :: 0.5784788131713867
Training Total Loss for Iteration 4000 :: 0.5258020758628845
Training Total Loss for Iteration 4050 :: 0.7711567282676697
Training Total Loss for Iteration 4100 :: 0.8240190744400024
Train mAP Epoch 10: 0.38633236289024353
Training Epoch #10 Sum Loss:0.06830718286562909
Validation Total Loss for Iteration 1000 :: 0.5796678066253662
Validation Total Loss for Iteration 1050 :: 0.5038950443267822
Validation mAP Epoch 10: 0.28712525963783264
Validation Epoch #10 Sum Loss:0.07502049778122455
Epoch 11/49
----------
Training Total Loss for Iteration 4150 :: 0.32241055369377136
Training Total Loss for Iteration 4200 :: 0.44490760564804077
Training Total Loss for Iteration 4250 :: 0.5621093511581421
Training Total Loss for Iteration 4300 :: 0.4449964165687561
Training Total Loss for Iteration 4350 :: 0.5779936909675598
Training Total Loss for Iteration 4400 :: 0.6344163417816162
Training Total Loss for Iteration 4450 :: 0.5812429189682007
Train mAP Epoch 11: 0.38724786043167114
Training Epoch #11 Sum Loss:0.06791360973212111
Validation Total Loss for Iteration 1100 :: 0.37097734212875366
Validation Total Loss for Iteration 1150 :: 0.5251641273498535
Validation mAP Epoch 11: 0.3021479547023773
Validation Epoch #11 Sum Loss:0.07090270406721781
Epoch 12/49
----------
Training Total Loss for Iteration 4500 :: 0.4383421540260315
Training Total Loss for Iteration 4550 :: 0.598206639289856
Training Total Loss for Iteration 4600 :: 0.48681336641311646
Training Total Loss for Iteration 4650 :: 0.48095032572746277
Training Total Loss for Iteration 4700 :: 0.6499331593513489
Training Total Loss for Iteration 4750 :: 0.5028954148292542
Training Total Loss for Iteration 4800 :: 0.5215557217597961
Train mAP Epoch 12: 0.4134218990802765
Training Epoch #12 Sum Loss:0.06713203535468656
Validation Total Loss for Iteration 1200 :: 0.5654209852218628
Validation mAP Epoch 12: 0.3393671214580536
Validation Epoch #12 Sum Loss:0.07523283511788274
Epoch 13/49
----------
Training Total Loss for Iteration 4850 :: 0.4074353575706482
Training Total Loss for Iteration 4900 :: 0.3642937242984772
Training Total Loss for Iteration 4950 :: 0.5804508924484253
Training Total Loss for Iteration 5000 :: 0.5552173852920532
Training Total Loss for Iteration 5050 :: 0.46099358797073364
Training Total Loss for Iteration 5100 :: 0.5762609243392944
Training Total Loss for Iteration 5150 :: 0.5174347162246704
Training Total Loss for Iteration 5200 :: 0.5713440775871277
Train mAP Epoch 13: 0.419450044631958
Training Epoch #13 Sum Loss:0.0669082154796972
Validation Total Loss for Iteration 1250 :: 0.5688990354537964
Validation Total Loss for Iteration 1300 :: 0.5931009650230408
Validation mAP Epoch 13: 0.2965010106563568
Validation Epoch #13 Sum Loss:0.06985887807483475
Epoch 14/49
----------
Training Total Loss for Iteration 5250 :: 0.4145781695842743
Training Total Loss for Iteration 5300 :: 0.5942263603210449
Training Total Loss for Iteration 5350 :: 0.5743797421455383
Training Total Loss for Iteration 5400 :: 0.6407020688056946
Training Total Loss for Iteration 5450 :: 0.4199441075325012
Training Total Loss for Iteration 5500 :: 0.37159955501556396
Training Total Loss for Iteration 5550 :: 0.6655421853065491
Train mAP Epoch 14: 0.43003979325294495
Training Epoch #14 Sum Loss:0.06601411380131186
Validation Total Loss for Iteration 1350 :: 0.41388264298439026
Validation Total Loss for Iteration 1400 :: 0.44206199049949646
Validation mAP Epoch 14: 0.30683088302612305
Validation Epoch #14 Sum Loss:0.07821246290889879
Epoch 15/49
----------
Training Total Loss for Iteration 5600 :: 0.44470730423927307
Training Total Loss for Iteration 5650 :: 0.41340768337249756
Training Total Loss for Iteration 5700 :: 0.5958662629127502
Training Total Loss for Iteration 5750 :: 0.3081910312175751
Training Total Loss for Iteration 5800 :: 0.5907989144325256
Training Total Loss for Iteration 5850 :: 0.5362492203712463
Training Total Loss for Iteration 5900 :: 0.49396708607673645
Training Total Loss for Iteration 5950 :: 0.3601841628551483
Train mAP Epoch 15: 0.43445873260498047
Training Epoch #15 Sum Loss:0.06534566983945496
Validation Total Loss for Iteration 1450 :: 0.576205849647522
Validation Total Loss for Iteration 1500 :: 0.5998290777206421
Validation mAP Epoch 15: 0.33307841420173645
Validation Epoch #15 Sum Loss:0.071837466074309
Epoch 16/49
----------
Training Total Loss for Iteration 6000 :: 0.35049161314964294
Training Total Loss for Iteration 6050 :: 0.46698814630508423
Training Total Loss for Iteration 6100 :: 0.47818583250045776
Training Total Loss for Iteration 6150 :: 0.5363067984580994
Training Total Loss for Iteration 6200 :: 0.4528079330921173
Training Total Loss for Iteration 6250 :: 0.718507707118988
Training Total Loss for Iteration 6300 :: 0.5752755999565125
Train mAP Epoch 16: 0.44855281710624695
Training Epoch #16 Sum Loss:0.06477998904396487
Validation Total Loss for Iteration 1550 :: 0.6642026305198669
Validation Total Loss for Iteration 1600 :: 0.7337653040885925
Validation mAP Epoch 16: 0.34844711422920227
Validation Epoch #16 Sum Loss:0.07323958148481324
Epoch 17/49
----------
Training Total Loss for Iteration 6350 :: 0.4763200879096985
Training Total Loss for Iteration 6400 :: 0.8689719438552856
Training Total Loss for Iteration 6450 :: 0.4932660758495331
Training Total Loss for Iteration 6500 :: 0.35066038370132446
Training Total Loss for Iteration 6550 :: 0.4532393217086792
Training Total Loss for Iteration 6600 :: 0.42637109756469727
Training Total Loss for Iteration 6650 :: 0.32611072063446045
Training Total Loss for Iteration 6700 :: 0.40913915634155273
Train mAP Epoch 17: 0.44221869111061096
Training Epoch #17 Sum Loss:0.06502169651079003
Validation Total Loss for Iteration 1650 :: 0.6107435822486877
Validation Total Loss for Iteration 1700 :: 0.42753636837005615
Validation mAP Epoch 17: 0.3330574929714203
Validation Epoch #17 Sum Loss:0.07170824873416375
Epoch 18/49
----------
Training Total Loss for Iteration 6750 :: 0.546597957611084
Training Total Loss for Iteration 6800 :: 0.5081206560134888
Training Total Loss for Iteration 6850 :: 0.430466890335083
Training Total Loss for Iteration 6900 :: 0.5084575414657593
Training Total Loss for Iteration 6950 :: 0.36323121190071106
Training Total Loss for Iteration 7000 :: 0.4742810130119324
Training Total Loss for Iteration 7050 :: 0.5113433599472046
Train mAP Epoch 18: 0.45771345496177673
Training Epoch #18 Sum Loss:0.06444664486939777
Validation Total Loss for Iteration 1750 :: 0.5723606944084167
Validation Total Loss for Iteration 1800 :: 0.7505877017974854
Validation mAP Epoch 18: 0.32129213213920593
Validation Epoch #18 Sum Loss:0.07492236071266234
Epoch 19/49
----------
Training Total Loss for Iteration 7100 :: 0.4281928837299347
Training Total Loss for Iteration 7150 :: 0.7780861258506775
Training Total Loss for Iteration 7200 :: 0.5613292455673218
Training Total Loss for Iteration 7250 :: 0.34436240792274475
Training Total Loss for Iteration 7300 :: 0.5686690211296082
Training Total Loss for Iteration 7350 :: 0.3746787905693054
Training Total Loss for Iteration 7400 :: 0.4726905822753906
Training Total Loss for Iteration 7450 :: 0.4694935083389282
Train mAP Epoch 19: 0.4507335126399994
Training Epoch #19 Sum Loss:0.06390355827323704
Validation Total Loss for Iteration 1850 :: 0.49994662404060364
Validation Total Loss for Iteration 1900 :: 0.383714884519577
Validation mAP Epoch 19: 0.3505205512046814
Validation Epoch #19 Sum Loss:0.07404580211732537
Epoch 20/49
----------
Training Total Loss for Iteration 7500 :: 0.7063264846801758
Training Total Loss for Iteration 7550 :: 0.6428171396255493
Training Total Loss for Iteration 7600 :: 0.6201184391975403
Training Total Loss for Iteration 7650 :: 0.7716452479362488
Training Total Loss for Iteration 7700 :: 0.4558843970298767
Training Total Loss for Iteration 7750 :: 0.46823617815971375
Training Total Loss for Iteration 7800 :: 0.5878282189369202
Train mAP Epoch 20: 0.46337857842445374
Training Epoch #20 Sum Loss:0.06297600663216152
Validation Total Loss for Iteration 1950 :: 0.5486125946044922
Validation Total Loss for Iteration 2000 :: 0.5787330269813538
Validation mAP Epoch 20: 0.34968894720077515
Validation Epoch #20 Sum Loss:0.07688151684124023
Epoch 21/49
----------
Training Total Loss for Iteration 7850 :: 0.45857134461402893
Training Total Loss for Iteration 7900 :: 0.3101624548435211
Training Total Loss for Iteration 7950 :: 0.3895694613456726
Training Total Loss for Iteration 8000 :: 0.3559946119785309
Training Total Loss for Iteration 8050 :: 0.5801588892936707
Training Total Loss for Iteration 8100 :: 0.5283545255661011
Training Total Loss for Iteration 8150 :: 0.5508261919021606
Training Total Loss for Iteration 8200 :: 0.5317227840423584
Train mAP Epoch 21: 0.47117337584495544
Training Epoch #21 Sum Loss:0.06245494767128722
Validation Total Loss for Iteration 2050 :: 0.4659397602081299
Validation Total Loss for Iteration 2100 :: 0.976089358329773
Validation mAP Epoch 21: 0.33410006761550903
Validation Epoch #21 Sum Loss:0.07397571764886379
Epoch 22/49
----------
Training Total Loss for Iteration 8250 :: 0.453329473733902
Training Total Loss for Iteration 8300 :: 0.33011966943740845
Training Total Loss for Iteration 8350 :: 0.37648311257362366
Training Total Loss for Iteration 8400 :: 0.8079243898391724
Training Total Loss for Iteration 8450 :: 0.5013014674186707
Training Total Loss for Iteration 8500 :: 0.5713183879852295
Training Total Loss for Iteration 8550 :: 0.5435671210289001
Train mAP Epoch 22: 0.4661164879798889
Training Epoch #22 Sum Loss:0.06284942927075808
Validation Total Loss for Iteration 2150 :: 0.36873859167099
Validation Total Loss for Iteration 2200 :: 0.5964826345443726
Validation mAP Epoch 22: 0.3389136493206024
Validation Epoch #22 Sum Loss:0.07676386703193809
Epoch 23/49
----------
Training Total Loss for Iteration 8600 :: 0.42842531204223633
Training Total Loss for Iteration 8650 :: 0.588782012462616
Training Total Loss for Iteration 8700 :: 0.4019526541233063
Training Total Loss for Iteration 8750 :: 0.5218837261199951
Training Total Loss for Iteration 8800 :: 0.55802983045578
Training Total Loss for Iteration 8850 :: 0.5391650199890137
Training Total Loss for Iteration 8900 :: 0.5805628299713135
Training Total Loss for Iteration 8950 :: 0.45243531465530396
Train mAP Epoch 23: 0.47714418172836304
Training Epoch #23 Sum Loss:0.06263690289636059
Validation Total Loss for Iteration 2250 :: 0.5909547805786133
Validation Total Loss for Iteration 2300 :: 0.611977219581604
Validation mAP Epoch 23: 0.3650577664375305
Validation Epoch #23 Sum Loss:0.07404539241300274
Epoch 24/49
----------
Training Total Loss for Iteration 9000 :: 0.5986636877059937
Training Total Loss for Iteration 9050 :: 0.7642724514007568
Training Total Loss for Iteration 9100 :: 0.7323459982872009
Training Total Loss for Iteration 9150 :: 0.4398755133152008
Training Total Loss for Iteration 9200 :: 0.3106580972671509
Training Total Loss for Iteration 9250 :: 0.3925653100013733
Training Total Loss for Iteration 9300 :: 0.3993833363056183
Train mAP Epoch 24: 0.46793898940086365
Training Epoch #24 Sum Loss:0.06231434763237679
Validation Total Loss for Iteration 2350 :: 0.5872405767440796
Validation mAP Epoch 24: 0.3426555097103119
Validation Epoch #24 Sum Loss:0.07405263503702979
Epoch 25/49
----------
Training Total Loss for Iteration 9350 :: 0.5056326389312744
Training Total Loss for Iteration 9400 :: 0.3932286500930786
Training Total Loss for Iteration 9450 :: 0.4101601541042328
Training Total Loss for Iteration 9500 :: 0.3616926372051239
Training Total Loss for Iteration 9550 :: 0.7111194729804993
Training Total Loss for Iteration 9600 :: 0.4531008005142212
Training Total Loss for Iteration 9650 :: 0.7963937520980835
Train mAP Epoch 25: 0.4772813022136688
Training Epoch #25 Sum Loss:0.062122126776913755
Validation Total Loss for Iteration 2400 :: 1.006514549255371
Validation Total Loss for Iteration 2450 :: 0.7951357364654541
Validation mAP Epoch 25: 0.3593924641609192
Validation Epoch #25 Sum Loss:0.07542844233103096
Epoch 26/49
----------
Training Total Loss for Iteration 9700 :: 0.589354932308197
Training Total Loss for Iteration 9750 :: 0.6067342758178711
Training Total Loss for Iteration 9800 :: 0.35722875595092773
Training Total Loss for Iteration 9850 :: 0.7570144534111023
Training Total Loss for Iteration 9900 :: 0.6348432898521423
Training Total Loss for Iteration 9950 :: 0.7487934231758118
Training Total Loss for Iteration 10000 :: 0.561194896697998
Training Total Loss for Iteration 10050 :: 0.4308971166610718
Train mAP Epoch 26: 0.47814372181892395
Training Epoch #26 Sum Loss:0.06168562620228387
Validation Total Loss for Iteration 2500 :: 0.7109530568122864
Validation Total Loss for Iteration 2550 :: 0.6223416328430176
Validation mAP Epoch 26: 0.3616519272327423
Validation Epoch #26 Sum Loss:0.07903968488487105
Epoch 27/49
----------
Training Total Loss for Iteration 10100 :: 0.4950351119041443
Training Total Loss for Iteration 10150 :: 0.5532390475273132
Training Total Loss for Iteration 10200 :: 0.5757596492767334
Training Total Loss for Iteration 10250 :: 0.5669691562652588
Training Total Loss for Iteration 10300 :: 0.6142391562461853
Training Total Loss for Iteration 10350 :: 0.35037946701049805
Training Total Loss for Iteration 10400 :: 0.4865493178367615
Train mAP Epoch 27: 0.4894081950187683
Training Epoch #27 Sum Loss:0.06153149949308611
Validation Total Loss for Iteration 2600 :: 0.34112000465393066
Validation Total Loss for Iteration 2650 :: 0.7638962864875793
Validation mAP Epoch 27: 0.35221561789512634
Validation Epoch #27 Sum Loss:0.07286636255836736
Epoch 28/49
----------
Training Total Loss for Iteration 10450 :: 0.480192095041275
Training Total Loss for Iteration 10500 :: 0.27591314911842346
Training Total Loss for Iteration 10550 :: 0.7943789958953857
Training Total Loss for Iteration 10600 :: 1.0703963041305542
Training Total Loss for Iteration 10650 :: 0.6832931041717529
Training Total Loss for Iteration 10700 :: 2.705538272857666
Training Total Loss for Iteration 10750 :: 2.302341938018799
Training Total Loss for Iteration 10800 :: 1.9715793132781982
Train mAP Epoch 28: 0.36590883135795593
Training Epoch #28 Sum Loss:2913.0122413198264
Validation Total Loss for Iteration 2700 :: 1.9736323356628418
Validation Total Loss for Iteration 2750 :: 1.8296865224838257
Validation mAP Epoch 28: 0.0
Validation Epoch #28 Sum Loss:0.23690691062559685
Epoch 29/49
----------
Training Total Loss for Iteration 10850 :: 1.760183334350586
Training Total Loss for Iteration 10900 :: 1.525674819946289
Training Total Loss for Iteration 10950 :: 1.38849937915802
Training Total Loss for Iteration 11000 :: 1.2691997289657593
Training Total Loss for Iteration 11050 :: 1.245720624923706
Training Total Loss for Iteration 11100 :: 1.3325411081314087
Training Total Loss for Iteration 11150 :: 1.0544891357421875
Train mAP Epoch 29: 0.0
Training Epoch #29 Sum Loss:41717.22315451286
Validation Total Loss for Iteration 2800 :: 1.1131234169006348
Validation Total Loss for Iteration 2850 :: 0.9967122077941895
Validation mAP Epoch 29: 0.0
Validation Epoch #29 Sum Loss:0.13616960231835643
Epoch 30/49
----------
Training Total Loss for Iteration 11200 :: 1.267607569694519
Training Total Loss for Iteration 11250 :: 0.9797372817993164
Training Total Loss for Iteration 11300 :: 0.953004002571106
Training Total Loss for Iteration 11350 :: 1.1781338453292847
Training Total Loss for Iteration 11400 :: 0.9056552052497864
Training Total Loss for Iteration 11450 :: 0.8863334655761719
Training Total Loss for Iteration 11500 :: 0.8287826776504517
Training Total Loss for Iteration 11550 :: 0.9888379573822021
Train mAP Epoch 30: 0.0
Training Epoch #30 Sum Loss:0.1184448652201873
Validation Total Loss for Iteration 2900 :: 0.7475631237030029
Validation Total Loss for Iteration 2950 :: 0.8385676741600037
Validation mAP Epoch 30: 0.0
Validation Epoch #30 Sum Loss:0.10302968866502245
Epoch 31/49
----------
Training Total Loss for Iteration 11600 :: 0.6434450745582581
Training Total Loss for Iteration 11650 :: 0.6881154179573059
Training Total Loss for Iteration 11700 :: 1.1525993347167969
Training Total Loss for Iteration 11750 :: 0.6034977436065674
Training Total Loss for Iteration 11800 :: 0.7326319217681885
Training Total Loss for Iteration 11850 :: 0.6867765188217163
Training Total Loss for Iteration 11900 :: 0.6734589338302612
Train mAP Epoch 31: 0.0
Training Epoch #31 Sum Loss:0.09600436337559626
Validation Total Loss for Iteration 3000 :: 0.7372117042541504
Validation Total Loss for Iteration 3050 :: 0.8884446620941162
Validation mAP Epoch 31: 0.0
Validation Epoch #31 Sum Loss:0.08852279411318402
Epoch 32/49
----------
